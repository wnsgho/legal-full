#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JSON ì§ˆë¬¸ íŒŒì¼ì„ ì‚¬ìš©í•œ ìë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
API ì„œë²„ì˜ /chat ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ í•˜ë‚˜ì”© ì „ì†¡í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import requests
import argparse
import io
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import logging

# UTF-8 ë¡œê¹… ì„¤ì •
try:
    from atlas_rag.utils.utf8_logging import setup_utf8_logging
    setup_utf8_logging()
except ImportError:
    # atlas_rag ëª¨ë“ˆì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    question_id: int
    original_question_id: int
    question: str
    expected_answer: str
    actual_answer: str
    success: bool
    processing_time: float
    category: str
    difficulty: str
    points: int
    similarity_score: Optional[float] = None
    context_count: int = 0
    api_processing_time: float = 0.0
    error_message: Optional[str] = None

class AutoEvaluator:
    """ìë™ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, api_base_url: str = "http://localhost:8000", timeout: int = 30):
        self.api_base_url = api_base_url.rstrip('/')
        self.timeout = timeout
        self.results: List[EvaluationResult] = []
        
    def test_api_connection(self) -> bool:
        """API ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.api_base_url}/health", timeout=5)
            if response.status_code == 200:
                logger.info("âœ… API ì„œë²„ ì—°ê²° ì„±ê³µ")
                return True
            else:
                logger.error(f"âŒ API ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def send_question(self, question: str, max_tokens: int = 2048, temperature: float = 0.5) -> Dict[str, Any]:
        """ë‹¨ì¼ ì§ˆë¬¸ì„ APIë¡œ ì „ì†¡"""
        url = f"{self.api_base_url}/chat"
        
        payload = {
            "question": question,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        logger.debug(f"ğŸ” API ìš”ì²­ ì‹œì‘ - íƒ€ì„ì•„ì›ƒ: {self.timeout}ì´ˆ")
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return {
                    "success": False,
                    "answer": f"API ì˜¤ë¥˜: {response.status_code}",
                    "context_count": 0,
                    "processing_time": 0
                }
                
        except requests.exceptions.Timeout:
            logger.error(f"âŒ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ: {self.timeout}ì´ˆ")
            return {
                "success": False,
                "answer": "API ìš”ì²­ íƒ€ì„ì•„ì›ƒ",
                "context_count": 0,
                "processing_time": self.timeout
            }
        except Exception as e:
            logger.error(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "answer": f"API ìš”ì²­ ì˜¤ë¥˜: {str(e)}",
                "context_count": 0,
                "processing_time": 0
            }
    
    def calculate_similarity(self, expected: str, actual: str) -> float:
        """ë‹µë³€ ìœ ì‚¬ë„ ê³„ì‚° (ê°œì„ ëœ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        if not expected or not actual:
            return 0.0
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        expected_clean = self._clean_text(expected)
        actual_clean = self._clean_text(actual)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)
        expected_keywords = set(expected_clean.split())
        actual_keywords = set(actual_clean.split())
        
        if not expected_keywords:
            return 0.0
        
        # êµì§‘í•© ê³„ì‚°
        intersection = expected_keywords.intersection(actual_keywords)
        
        # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        union = expected_keywords.union(actual_keywords)
        jaccard_similarity = len(intersection) / len(union) if union else 0
        
        # í¬í•¨ë„ ê³„ì‚° (ì˜ˆìƒ ë‹µë³€ì˜ í‚¤ì›Œë“œê°€ ì‹¤ì œ ë‹µë³€ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€)
        inclusion_similarity = len(intersection) / len(expected_keywords)
        
        # ê°€ì¤‘ í‰ê·  (Jaccard 70%, í¬í•¨ë„ 30%)
        similarity = (jaccard_similarity * 0.7) + (inclusion_similarity * 0.3)
        
        return min(similarity, 1.0)
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬ (ë¶ˆìš©ì–´ ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
        import re
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€',
            'ì—ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë”', 'ë˜', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        }
        
        words = text.lower().split()
        cleaned_words = [word for word in words if word not in stopwords and len(word) > 1]
        
        return ' '.join(cleaned_words)
    
    def evaluate_single_question(self, question_data: Dict[str, Any]) -> EvaluationResult:
        """ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€"""
        question_id = question_data.get("question_id", 0)
        original_question_id = question_data.get("original_question_id", 0)
        question = question_data.get("question", "")
        expected_answer = question_data.get("answer", "")
        category = question_data.get("category", "ê¸°íƒ€")
        difficulty = question_data.get("difficulty", "low")
        points = question_data.get("points", 3)
        
        logger.info(f"ğŸ“ ì§ˆë¬¸ {question_id} í‰ê°€ ì¤‘: {question[:50]}...")
        
        # APIë¡œ ì§ˆë¬¸ ì „ì†¡
        start_time = time.time()
        api_response = self.send_question(question)
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        success = api_response.get("success", False)
        actual_answer = api_response.get("answer", "")
        api_processing_time = api_response.get("processing_time", 0)
        context_count = api_response.get("context_count", 0)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity_score = self.calculate_similarity(expected_answer, actual_answer)
        
        # ê²°ê³¼ ìƒì„±
        result = EvaluationResult(
            question_id=question_id,
            original_question_id=original_question_id,
            question=question,
            expected_answer=expected_answer,
            actual_answer=actual_answer,
            success=success,
            processing_time=processing_time,
            category=category,
            difficulty=difficulty,
            points=points,
            similarity_score=similarity_score,
            context_count=context_count,
            api_processing_time=api_processing_time,
            error_message=None if success else "API í˜¸ì¶œ ì‹¤íŒ¨"
        )
        
        logger.info(f"âœ… ì§ˆë¬¸ {question_id} ì™„ë£Œ - ì„±ê³µ: {success}, ìœ ì‚¬ë„: {similarity_score:.3f}")
        
        return result
    
    def load_questions(self, json_file_path: str) -> List[Dict[str, Any]]:
        """JSON ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ (ê¸°ì¡´ í˜•ì‹ê³¼ ìƒˆë¡œìš´ í˜•ì‹ ëª¨ë‘ ì§€ì›)"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            questions = []
            
            # ìƒˆë¡œìš´ í˜•ì‹ (questions ë°°ì—´ì´ ì§ì ‘ ìˆëŠ” ê²½ìš°)
            if "questions" in data:
                questions = data["questions"]
                logger.info(f"ğŸ“š ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ {len(questions)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
            # ê¸°ì¡´ í˜•ì‹ (sections ë°°ì—´ ì•ˆì— questionsê°€ ìˆëŠ” ê²½ìš°)
            elif "sections" in data:
                for section in data.get("sections", []):
                    questions.extend(section.get("questions", []))
                logger.info(f"ğŸ“š ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ {len(questions)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.error("âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” JSON í˜•ì‹ì…ë‹ˆë‹¤.")
                return []
            
            return questions
            
        except Exception as e:
            logger.error(f"âŒ ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def run_evaluation(self, json_file_path: str, max_questions: Optional[int] = None, 
                      start_from: int = 0, delay_between_requests: float = 1.0) -> List[EvaluationResult]:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        logger.info("ğŸš€ ìë™ í‰ê°€ ì‹œì‘")
        
        # API ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.test_api_connection():
            logger.error("âŒ API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return []
        
        # ì§ˆë¬¸ ë¡œë“œ
        questions = self.load_questions(json_file_path)
        if not questions:
            logger.error("âŒ ì§ˆë¬¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # í‰ê°€í•  ì§ˆë¬¸ ì„ íƒ
        if max_questions:
            questions = questions[start_from:start_from + max_questions]
        else:
            questions = questions[start_from:]
        
        logger.info(f"ğŸ“Š {len(questions)}ê°œ ì§ˆë¬¸ í‰ê°€ ì˜ˆì • (ì‹œì‘: {start_from})")
        
        # ì§ˆë¬¸ë³„ í‰ê°€ ì‹¤í–‰
        for i, question_data in enumerate(questions, 1):
            try:
                result = self.evaluate_single_question(question_data)
                self.results.append(result)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = (i / len(questions)) * 100
                logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(questions)})")
                
                # ìš”ì²­ ê°„ ì§€ì—°
                if delay_between_requests > 0 and i < len(questions):
                    time.sleep(delay_between_requests)
                    
            except Exception as e:
                logger.error(f"âŒ ì§ˆë¬¸ {i} í‰ê°€ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ì§ˆë¬¸ë„ ê²°ê³¼ì— ì¶”ê°€
                error_result = EvaluationResult(
                    question_id=question_data.get("question_id", i),
                    original_question_id=question_data.get("original_question_id", i),
                    question=question_data.get("question", ""),
                    expected_answer=question_data.get("answer", ""),
                    actual_answer="",
                    success=False,
                    processing_time=0,
                    category=question_data.get("category", "ê¸°íƒ€"),
                    difficulty=question_data.get("difficulty", "low"),
                    points=question_data.get("points", 3),
                    error_message=str(e)
                )
                self.results.append(error_result)
        
        logger.info("âœ… ìë™ í‰ê°€ ì™„ë£Œ")
        return self.results
    
    def analyze_results(self) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ë¶„ì„"""
        if not self.results:
            return {}
        
        total_questions = len(self.results)
        successful_questions = sum(1 for r in self.results if r.success)
        failed_questions = total_questions - successful_questions
        
        # ìœ ì‚¬ë„ í†µê³„
        similarities = [r.similarity_score for r in self.results if r.similarity_score is not None]
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for result in self.results:
            category = result.category
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "successful": 0,
                    "avg_similarity": 0,
                    "total_points": 0,
                    "earned_points": 0
                }
            
            category_stats[category]["total"] += 1
            if result.success:
                category_stats[category]["successful"] += 1
            category_stats[category]["total_points"] += result.points
            if result.similarity_score and result.similarity_score > 0.5:  # 50% ì´ìƒ ìœ ì‚¬ë„
                category_stats[category]["earned_points"] += result.points
        
        # ë‚œì´ë„ë³„ í†µê³„
        difficulty_stats = {}
        for result in self.results:
            difficulty = result.difficulty
            if difficulty not in difficulty_stats:
                difficulty_stats[difficulty] = {
                    "total": 0,
                    "successful": 0,
                    "avg_similarity": 0
                }
            
            difficulty_stats[difficulty]["total"] += 1
            if result.success:
                difficulty_stats[difficulty]["successful"] += 1
        
        # ì²˜ë¦¬ ì‹œê°„ í†µê³„
        processing_times = [r.processing_time for r in self.results if r.processing_time > 0]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        # API ì²˜ë¦¬ ì‹œê°„ í†µê³„
        api_processing_times = [r.api_processing_time for r in self.results if r.api_processing_time > 0]
        avg_api_processing_time = sum(api_processing_times) / len(api_processing_times) if api_processing_times else 0
        
        # ì»¨í…ìŠ¤íŠ¸ í†µê³„
        context_counts = [r.context_count for r in self.results if r.context_count > 0]
        avg_context_count = sum(context_counts) / len(context_counts) if context_counts else 0
        total_contexts = sum(context_counts)
        
        analysis = {
            "summary": {
                "total_questions": total_questions,
                "successful_questions": successful_questions,
                "failed_questions": failed_questions,
                "success_rate": (successful_questions / total_questions) * 100 if total_questions > 0 else 0,
                "average_similarity": avg_similarity,
                "average_processing_time": avg_processing_time,
                "average_api_processing_time": avg_api_processing_time,
                "average_context_count": avg_context_count,
                "total_contexts_retrieved": total_contexts
            },
            "category_stats": category_stats,
            "difficulty_stats": difficulty_stats
        }
        
        return analysis
    
    def save_results(self, output_file: str):
        """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
            results_data = {
                "evaluation_info": {
                    "timestamp": datetime.now().isoformat(),
                    "api_base_url": self.api_base_url,
                    "total_questions": len(self.results)
                },
                "analysis": self.analyze_results(),
                "detailed_results": []
            }
            
            # ìƒì„¸ ê²°ê³¼ ì¶”ê°€
            for result in self.results:
                result_dict = {
                    "question_id": result.question_id,
                    "original_question_id": result.original_question_id,
                    "question": result.question,
                    "expected_answer": result.expected_answer,
                    "actual_answer": result.actual_answer,
                    "success": result.success,
                    "processing_time": result.processing_time,
                    "category": result.category,
                    "difficulty": result.difficulty,
                    "points": result.points,
                    "similarity_score": result.similarity_score,
                    "context_count": result.context_count,
                    "api_processing_time": result.api_processing_time,
                    "error_message": result.error_message
                }
                results_data["detailed_results"].append(result_dict)
            
            # íŒŒì¼ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def print_summary(self):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        analysis = self.analyze_results()
        
        if not analysis:
            print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        summary = analysis["summary"]
        
        print("\n" + "="*60)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ì´ ì§ˆë¬¸ ìˆ˜: {summary['total_questions']}")
        print(f"ì„±ê³µí•œ ì§ˆë¬¸: {summary['successful_questions']}")
        print(f"ì‹¤íŒ¨í•œ ì§ˆë¬¸: {summary['failed_questions']}")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        print(f"í‰ê·  ìœ ì‚¬ë„: {summary['average_similarity']:.3f}")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary['average_processing_time']:.2f}ì´ˆ")
        print(f"í‰ê·  API ì²˜ë¦¬ ì‹œê°„: {summary['average_api_processing_time']:.2f}ì´ˆ")
        print(f"í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {summary['average_context_count']:.1f}ê°œ")
        print(f"ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸: {summary['total_contexts_retrieved']}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
        for category, stats in analysis["category_stats"].items():
            success_rate = (stats["successful"] / stats["total"]) * 100 if stats["total"] > 0 else 0
            point_rate = (stats["earned_points"] / stats["total_points"]) * 100 if stats["total_points"] > 0 else 0
            print(f"  {category}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%) - ì ìˆ˜: {stats['earned_points']}/{stats['total_points']} ({point_rate:.1f}%)")
        
        # ë‚œì´ë„ë³„ í†µê³„
        print("\nğŸ¯ ë‚œì´ë„ë³„ í†µê³„:")
        for difficulty, stats in analysis["difficulty_stats"].items():
            success_rate = (stats["successful"] / stats["total"]) * 100 if stats["total"] > 0 else 0
            print(f"  {difficulty}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)")
        
        print("="*60)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="JSON ì§ˆë¬¸ íŒŒì¼ì„ ì‚¬ìš©í•œ ìë™ í‰ê°€")
    parser.add_argument("--input", "-i", required=True, help="ì…ë ¥ JSON ì§ˆë¬¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", "-o", help="ì¶œë ¥ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: evaluation_results.json)")
    parser.add_argument("--api-url", default="http://localhost:8000", help="API ì„œë²„ URL (ê¸°ë³¸: http://localhost:8000)")
    parser.add_argument("--max-questions", "-m", type=int, help="í‰ê°€í•  ìµœëŒ€ ì§ˆë¬¸ ìˆ˜")
    parser.add_argument("--start-from", "-s", type=int, default=0, help="ì‹œì‘ ì§ˆë¬¸ ë²ˆí˜¸ (0ë¶€í„°)")
    parser.add_argument("--delay", "-d", type=float, default=1.0, help="ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--timeout", "-t", type=int, default=30, help="API ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if not args.output:
        input_path = Path(args.input)
        args.output = input_path.parent / f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # í‰ê°€ì ìƒì„±
    evaluator = AutoEvaluator(api_base_url=args.api_url, timeout=args.timeout)
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_evaluation(
        json_file_path=args.input,
        max_questions=args.max_questions,
        start_from=args.start_from,
        delay_between_requests=args.delay
    )
    
    if results:
        # ê²°ê³¼ ì €ì¥
        evaluator.save_results(str(args.output))
        
        # ìš”ì•½ ì¶œë ¥
        evaluator.print_summary()
        
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {args.output}")
    else:
        print("âŒ í‰ê°€ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
