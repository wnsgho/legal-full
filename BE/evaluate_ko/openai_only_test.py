#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenAI GPT ì „ìš© ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë³¸ë¬¸ì„ OpenAIì— ë„£ê³  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import openai
from dotenv import load_dotenv
import re

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('openai_only_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OpenAITester:
    def __init__(self, model: str = "gpt-4.1"):
        """
        OpenAI í…ŒìŠ¤í„° ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  OpenAI ëª¨ë¸
        """
        self.model = model
        self.client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.test_results = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model": model,
                "total_questions": 0
            },
            "test_results": [],
            "analysis": {}
        }
    
    def load_questions(self, questions_file: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            questions_file: ì§ˆë¬¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì§ˆë¬¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions_data = json.load(f)
            logger.info(f"ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {questions_file}")
            return questions_data
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def load_document(self, document_path: str) -> str:
        """
        ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            document_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¬¸ì„œ ë‚´ìš© ë¬¸ìì—´
        """
        try:
            with open(document_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"ë¬¸ì„œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {document_path}")
            return content
        except Exception as e:
            logger.error(f"ë¬¸ì„œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_openai_answer(self, question: str, document_content: str) -> Dict[str, Any]:
        """
        OpenAI APIë¥¼ í†µí•´ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            question: ì§ˆë¬¸
            document_content: ë¬¸ì„œ ë‚´ìš©
            
        Returns:
            OpenAI ë‹µë³€ ê²°ê³¼
        """
        try:
            start_time = time.time()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ê³„ì•½ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ê³„ì•½ì„œ ë‚´ìš©:
{document_content}

ì§ˆë¬¸: {question}

"""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,
                temperature=0.1
            )
            
            api_time = time.time() - start_time
            
            answer = response.choices[0].message.content.strip()
            
            return {
                "success": True,
                "answer": answer,
                "processing_time": api_time,
                "model": self.model,
                "tokens_used": response.usage.total_tokens if response.usage else 0,
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0
            }
            
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": 0,
                "answer": "",
                "model": self.model,
                "tokens_used": 0
            }
    
    def run_test(self, questions_file: str, document_path: str, 
                max_questions: Optional[int] = None) -> Dict[str, Any]:
        """
        OpenAI í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            questions_file: ì§ˆë¬¸ íŒŒì¼ ê²½ë¡œ
            document_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            max_questions: ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ (Noneì´ë©´ ëª¨ë“  ì§ˆë¬¸)
            
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        logger.info("OpenAI í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì§ˆë¬¸ ë¡œë“œ
        questions_data = self.load_questions(questions_file)
        questions = questions_data.get("questions", [])
        
        if max_questions:
            questions = questions[:max_questions]
        
        # ë¬¸ì„œ ë¡œë“œ
        document_content = self.load_document(document_path)
        
        self.test_results["metadata"]["total_questions"] = len(questions)
        
        successful_tests = 0
        total_time = 0
        total_tokens = 0
        
        for i, question_data in enumerate(questions, 1):
            question_id = question_data.get("question_id", i)
            question = question_data.get("question", "")
            expected_answer = question_data.get("answer", "")
            
            logger.info(f"ì§ˆë¬¸ {i}/{len(questions)} ì²˜ë¦¬ ì¤‘: {question[:50]}...")
            
            # OpenAI ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            openai_result = self.get_openai_answer(question, document_content)
            
            # ê²°ê³¼ ì €ì¥
            test_result = {
                "question_id": question_id,
                "question": question,
                "expected_answer": expected_answer,
                "openai_result": openai_result,
                "processing_time": openai_result.get("processing_time", 0)
            }
            
            self.test_results["test_results"].append(test_result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if openai_result["success"]:
                successful_tests += 1
                total_time += openai_result.get("processing_time", 0)
                total_tokens += openai_result.get("tokens_used", 0)
            
            logger.info(f"ì§ˆë¬¸ {i} ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {openai_result.get('processing_time', 0):.2f}ì´ˆ")
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        self.test_results["analysis"] = {
            "summary": {
                "total_questions": len(questions),
                "successful_tests": successful_tests,
                "success_rate": (successful_tests / len(questions)) * 100 if questions else 0,
                "average_processing_time": total_time / successful_tests if successful_tests > 0 else 0,
                "total_tokens_used": total_tokens,
                "average_tokens_per_question": total_tokens / successful_tests if successful_tests > 0 else 0
            }
        }
        
        logger.info("OpenAI í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return self.test_results
    
    def save_results(self, output_file: str):
        """
        ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def print_summary(self):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
        """
        analysis = self.test_results.get("analysis", {})
        summary = analysis.get("summary", {})
        
        print("\n" + "="*60)
        print("OpenAI í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ëª¨ë¸: {self.model}")
        print(f"ì´ ì§ˆë¬¸ ìˆ˜: {summary.get('total_questions', 0)}")
        print(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {summary.get('successful_tests', 0)}")
        print(f"ì„±ê³µë¥ : {summary.get('success_rate', 0):.1f}%")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary.get('average_processing_time', 0):.2f}ì´ˆ")
        print(f"ì´ í† í° ì‚¬ìš©ëŸ‰: {summary.get('total_tokens_used', 0):,}")
        print(f"ì§ˆë¬¸ë‹¹ í‰ê·  í† í°: {summary.get('average_tokens_per_question', 0):.0f}")
        print("="*60)
    
    def print_detailed_results(self, max_results: int = 5):
        """
        ìƒì„¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        
        Args:
            max_results: ì¶œë ¥í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        """
        results = self.test_results.get("test_results", [])
        
        print(f"\nğŸ“‹ ìƒì„¸ ê²°ê³¼ (ìµœëŒ€ {max_results}ê°œ)")
        print("-" * 80)
        
        for i, result in enumerate(results[:max_results], 1):
            question = result.get("question", "")
            openai_result = result.get("openai_result", {})
            answer = openai_result.get("answer", "")
            
            print(f"\n{i}. ì§ˆë¬¸: {question}")
            print(f"   ì²˜ë¦¬ì‹œê°„: {openai_result.get('processing_time', 0):.2f}ì´ˆ")
            print(f"   í† í° ì‚¬ìš©ëŸ‰: {openai_result.get('tokens_used', 0):,}")
            print(f"   ë‹µë³€: {answer[:200]}...")
            print("-" * 80)

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    parser = argparse.ArgumentParser(description="OpenAI GPT ì „ìš© ì§ˆë¬¸ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--questions", required=True, help="ì§ˆë¬¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--document", required=True, help="ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", default="openai_only_results.json", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ(ë¯¸ì§€ì • ì‹œ ë¬¸ì„œ ë²ˆí˜¸ë¡œ ìë™ ëª…ëª…)")
    parser.add_argument("--max-questions", type=int, help="ìµœëŒ€ ì§ˆë¬¸ ìˆ˜")
    parser.add_argument("--model", default="gpt-4.1", help="OpenAI ëª¨ë¸ (gpt-4.1, gpt-4, gpt-3.5-turbo)")
    parser.add_argument("--detailed", action="store_true", help="ìƒì„¸ ê²°ê³¼ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        logger.error("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.questions).exists():
        logger.error(f"ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.questions}")
        sys.exit(1)
    
    if not Path(args.document).exists():
        logger.error(f"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.document}")
        sys.exit(1)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = OpenAITester(model=args.model)
    
    try:
        results = tester.run_test(
            questions_file=args.questions,
            document_path=args.document,
            max_questions=args.max_questions
        )
        
        # ê²°ê³¼ íŒŒì¼ëª… ìë™ ê²°ì •: contract ë²ˆí˜¸ ê¸°ë°˜
        def derive_output_path(document_path: str, user_output: Optional[str]) -> str:
            if user_output and user_output != "openai_only_results.json":
                return user_output
            base = Path(document_path).stem  # e.g., contract_5
            match = re.search(r"contract[_\-\s]?(\d+)", base, re.IGNORECASE)
            if match:
                return f"openai_only_results_contract_{match.group(1)}.json"
            # ìˆ«ì íŒ¨í„´ì´ ì—†ì„ ê²½ìš° ì „ì²´ ë² ì´ìŠ¤ëª… ì‚¬ìš©
            return f"openai_only_results_{base}.json"

        output_path = derive_output_path(args.document, args.output)

        # ê²°ê³¼ ì €ì¥
        tester.save_results(output_path)
        
        # ìš”ì•½ ì¶œë ¥
        tester.print_summary()
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        if args.detailed:
            tester.print_detailed_results()
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
