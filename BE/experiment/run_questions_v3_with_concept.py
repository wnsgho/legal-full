#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Concept í™œìš© LKG ìš°ì„  ê²€ìƒ‰ + HiPPO-RAG2 + LKG ì—¬ëŸ¬ ì¡°í•­ ê²€ìƒ‰ í•˜ì´ë¸Œë¦¬ë“œ RAG ì§ˆë¬¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Conceptì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

=============================================================================
ì„œë²„ ë°°í¬ ì‹œ ìˆ˜ì • í•„ìš” ì‚¬í•­:
=============================================================================
1. ìƒëŒ€ import â†’ ì ˆëŒ€ import ë³€ê²½ (ì™„ë£Œ)
   - from ..atlas_rag... â†’ from atlas_rag...

2. ì‹¤í–‰ ê²½ë¡œ ì„¤ì •
   - í˜„ì¬: BE/experiment/ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
   - ì„œë²„: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ python -m BE.experiment.run_questions_v3_with_concept.py

3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸
   - .env íŒŒì¼ ê²½ë¡œ í™•ì¸
   - Neo4j ì—°ê²° ì„¤ì • í™•ì¸
   - OpenRouter API ì„¤ì • í™•ì¸
=============================================================================
"""

import os
import sys
import time
import json
import re
import warnings
import logging
from datetime import datetime
from pathlib import Path
from collections import Counter

# ì„œë²„ ë°°í¬ ì‹œ Python ê²½ë¡œ ì„¤ì •
if __name__ == "__main__":
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ì—ì„œ BE ë””ë ‰í† ë¦¬ ì°¾ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    be_dir = os.path.join(current_dir, '..')
    if be_dir not in sys.path:
        sys.path.insert(0, be_dir)
import numpy as np
from dotenv import load_dotenv

# ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì–µì œ
warnings.filterwarnings("ignore")
logging.getLogger("faiss.loader").setLevel(logging.WARNING)
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
logging.getLogger("nltk").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# .env íŒŒì¼ ë¡œë“œ (ì„œë²„ ì‹¤í–‰ ì‹œ ê²½ë¡œ ìˆ˜ì •)
def load_env_file():
    """ì„œë²„ì™€ ì§ì ‘ ì‹¤í–‰ ëª¨ë‘ì—ì„œ .env íŒŒì¼ì„ ì°¾ì•„ì„œ ë¡œë“œ"""
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ì—ì„œ BE ë””ë ‰í† ë¦¬ ì°¾ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    be_dir = os.path.join(current_dir, '..')
    
    # .env íŒŒì¼ ê²½ë¡œë“¤ ì‹œë„
    env_paths = [
        os.path.join(be_dir, '.env'),  # BE/.env
        '.env',                        # í˜„ì¬ ë””ë ‰í† ë¦¬
        '../.env'                      # ìƒìœ„ ë””ë ‰í† ë¦¬
    ]
    
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            print(f"âœ… .env íŒŒì¼ ë¡œë“œ ì„±ê³µ: {env_path}")
            return True
    
    print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return False

# .env íŒŒì¼ ë¡œë“œ
load_env_file()

def load_enhanced_rag_system():
    """í–¥ìƒëœ LKG ë¦¬íŠ¸ë¼ì´ë²„ + HippoRAG2Retriever í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ ë¡œë“œ"""
    print("ğŸš€ load_enhanced_rag_system() í•¨ìˆ˜ ì‹œì‘")
    
    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    import os
    print(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f" __file__ ê²½ë¡œ: {__file__}")
    print(f"ğŸ“ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {os.path.dirname(os.path.abspath(__file__))}")
    
    try:
        from atlas_rag.retriever import HippoRAG2Retriever
        from atlas_rag.retriever.lkg_retriever.enhanced_lkgr import EnhancedLargeKGRetriever
        from atlas_rag.llm_generator import LLMGenerator
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        print("ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì¤‘...")
        from openai import OpenAI
        client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        )
        print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ")
        
        print("ğŸ§  LLMGenerator ìƒì„± ì¤‘...")
        llm_generator = LLMGenerator(
            client=client, 
            model_name=os.getenv('DEFAULT_MODEL', "gpt-4.1-2025-04-14"),
        )
        print("âœ… LLMGenerator ìƒì„± ì™„ë£Œ")
        
        # Neo4j ì—°ê²° ì„¤ì •
        from neo4j import GraphDatabase
        neo4j_uri = os.getenv('NEO4J_URI', 'neo4j://127.0.0.1:7687')
        neo4j_user = os.getenv('NEO4J_USER', 'neo4j')
        neo4j_password = os.getenv('NEO4J_PASSWORD')
        neo4j_database = os.getenv('NEO4J_DATABASE', 'neo4j')
        keyword = os.getenv('KEYWORD', 'contract_v5')
        
        print(f"ğŸ”§ í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
        print(f"   - NEO4J_URI: {neo4j_uri}")
        print(f"   - NEO4J_DATABASE: {neo4j_database}")
        print(f"   - KEYWORD: {keyword}")
        
        try:
            neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
            print("âœ… Neo4j ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ Neo4j ì—°ê²° ì‹¤íŒ¨: {e}")
            neo4j_driver = None
        
        # ì €ì¥ëœ RAG ë°ì´í„° ë¡œë“œ
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        import_dir = os.getenv('IMPORT_DIRECTORY', 'import')
        precompute_dir = os.getenv('PRECOMPUTE_DIRECTORY', 'precompute')
        
        print(f"ğŸ“ ê²½ë¡œ ì •ë³´:")
        print(f"   - script_dir: {script_dir}")
        print(f"   - project_root: {project_root}")
        print(f"   - import_dir: {import_dir}")
        print(f"   - precompute_dir: {precompute_dir}")
        
        data_path = os.path.join(project_root, import_dir, keyword, precompute_dir, f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_node_faiss.index")
        print(f"   - data_path: {data_path}")
        print(f"   - data_path ì¡´ì¬: {os.path.exists(data_path)}")
        
        if not os.path.exists(data_path):
            print("âŒ ì €ì¥ëœ RAG ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € experiment_multihop_qa.pyë¥¼ ì‹¤í–‰í•´ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”.")
            return None, None, None, None
        
        # RAG ì‹œìŠ¤í…œ ìƒì„±
        from atlas_rag.vectorstore import create_embeddings_and_index
        from sentence_transformers import SentenceTransformer
        from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        sentence_encoder = SentenceEmbedding(sentence_model)
        
        # RAG ë°ì´í„° ìƒì„±
        data = create_embeddings_and_index(
            sentence_encoder=sentence_encoder,
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            working_directory=os.path.join(project_root, "import", keyword),
            keyword=keyword,
            include_concept=True,
            include_events=True,
            normalize_embeddings=True,
            text_batch_size=32,
            node_and_edge_batch_size=64,
        )
        
        # ë°ì´í„° êµ¬ì¡° í™•ì¸
        print(f"ğŸ“Š ë°ì´í„° êµ¬ì¡°:")
        # print(f"   - data í‚¤ë“¤: {list(data.keys())}")  # ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œ
        
        # FAISS ì¸ë±ìŠ¤ ì§ì ‘ ë¡œë“œ (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)
        enhanced_lkg_retriever = None
        hippo_retriever = None
        
        if neo4j_driver is not None:
            try:
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                import faiss
                node_index_path = os.path.join(project_root, import_dir, keyword, precompute_dir, f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_node_faiss.index")
                passage_index_path = os.path.join(project_root, import_dir, keyword, precompute_dir, f"{keyword}_text_faiss.index")
                
                print(f" FAISS ì¸ë±ìŠ¤ ê²½ë¡œ:")
                print(f"   - node_index_path: {node_index_path}")
                print(f"   - passage_index_path: {passage_index_path}")
                print(f"   - node_index_path ì¡´ì¬: {os.path.exists(node_index_path)}")
                print(f"   - passage_index_path ì¡´ì¬: {os.path.exists(passage_index_path)}")
                
                if os.path.exists(node_index_path) and os.path.exists(passage_index_path):
                    node_index = faiss.read_index(node_index_path)
                    passage_index = faiss.read_index(passage_index_path)
                    print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
                    
                    # node_list ë¡œë“œ
                    import pickle
                    node_list_path = os.path.join(project_root, import_dir, keyword, precompute_dir, f"{keyword}_eventTrue_conceptTrue_node_list.pkl")
                    print(f"   - node_list_path: {node_list_path}")
                    print(f"   - node_list_path ì¡´ì¬: {os.path.exists(node_list_path)}")
                    
                    if os.path.exists(node_list_path):
                        with open(node_list_path, "rb") as f:
                            node_list = pickle.load(f)
                        print(f"âœ… node_list ë¡œë“œ ì„±ê³µ: {len(node_list)}ê°œ ë…¸ë“œ")
                        print(f"   - node_list ì²« 3ê°œ: {node_list[:3] if len(node_list) >= 3 else node_list}")
                    else:
                        print("âŒ node_list íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                        node_list = []
                    
                    # print í•¨ìˆ˜ë¥¼ ë˜í•‘í•œ ê°„ë‹¨í•œ ë¡œê±° í´ë˜ìŠ¤ ìƒì„±
                    class PrintLogger:
                        def info(self, message):
                            print(f"[INFO] {message}")
                        def debug(self, message):
                            print(f"[DEBUG] {message}")
                        def warning(self, message):
                            print(f"[WARNING] {message}")
                        def error(self, message):
                            print(f"[ERROR] {message}")
                    
                    print_logger = PrintLogger()
                    
                    # EnhancedLargeKGRetriever ìƒì„±
                    enhanced_lkg_retriever = EnhancedLargeKGRetriever(
                        keyword=keyword,
                        neo4j_driver=neo4j_driver,
                        llm_generator=llm_generator,
                        sentence_encoder=sentence_encoder,
                        node_index=node_index,
                        passage_index=passage_index,
                        topN=5,
                        number_of_source_nodes_per_ner=10,
                        sampling_area=250,
                        database=neo4j_database,
                        verbose=True,
                        logger=print_logger
                    )
                    
                    # node_listì™€ GraphML ê·¸ë˜í”„ ì¶”ê°€ (ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ)
                    enhanced_lkg_retriever.node_list = node_list
                    
                    # GraphML ê·¸ë˜í”„ ë¡œë“œ (ë…¸ë“œ íƒ€ì… ì •ë³´ìš©)
                    import networkx as nx
                    graphml_path = os.path.join(project_root, "import", keyword, "kg_graphml", f"{keyword}_graph_with_numeric_id.graphml")
                    print(f"   - graphml_path: {graphml_path}")
                    print(f"   - graphml_path ì¡´ì¬: {os.path.exists(graphml_path)}")
                    
                    if os.path.exists(graphml_path):
                        with open(graphml_path, "rb") as f:
                            enhanced_lkg_retriever.kg_graph = nx.read_graphml(f)
                        print(f"âœ… GraphML ê·¸ë˜í”„ ë¡œë“œ ì„±ê³µ: {len(enhanced_lkg_retriever.kg_graph.nodes)}ê°œ ë…¸ë“œ")
                    else:
                        print("âŒ GraphML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                        enhanced_lkg_retriever.kg_graph = None
                    
                    print("âœ… EnhancedLargeKGRetriever ìƒì„± ì™„ë£Œ")
                    
                    # HippoRAG2Retriever ìƒì„±
                    hippo_retriever = HippoRAG2Retriever(
                        llm_generator=llm_generator,
                        sentence_encoder=sentence_encoder,
                        data=data,
                    )
                    print("âœ… HippoRAG2Retriever ìƒì„± ì™„ë£Œ")
                    
                else:
                    print("âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                    print(f"   node_index_path: {node_index_path}")
                    print(f"   passage_index_path: {passage_index_path}")
                    
            except Exception as e:
                print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("âš ï¸ Neo4j ì—°ê²°ì´ ì—†ì–´ì„œ ë¦¬íŠ¸ë¼ì´ë²„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return enhanced_lkg_retriever, hippo_retriever, llm_generator, neo4j_driver
        
    except Exception as e:
        print(f"âŒ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

def extract_concepts_from_question(question, llm_generator):
    """
    ì§ˆë¬¸ì—ì„œ concept í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not question or not llm_generator:
        return []
    
    print("ï¿½ï¿½ ì§ˆë¬¸ì—ì„œ concept í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    try:
        # Concept ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        concept_extraction_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê³„ì•½ì„œì™€ ê´€ë ¨ëœ í•µì‹¬ conceptë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
1. ê³„ì•½ ê´€ë ¨ concept (ì˜ˆ: ê³„ì•½ë‹¹ì‚¬ì, ì˜ë¬´, ê¶Œë¦¬, ì¡°ê±´, ì¡°í•­ ë“±)
2. ë²•ì  concept (ì˜ˆ: ì†í•´ë°°ìƒ, ê³„ì•½í•´ì§€, ë¹„ë°€ìœ ì§€, ì§€ì ì¬ì‚°ê¶Œ ë“±)
3. ë¹„ì¦ˆë‹ˆìŠ¤ concept (ì˜ˆ: ë§¤ë„ì¸, ë§¤ìˆ˜ì¸, ê°€ê²©, ì¡°ì •, ìš´ì „ìë³¸ ë“±)

ê° conceptì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
"""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì—ì„œ í•µì‹¬ conceptë“¤ì„ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."}, 
            {"role": "user", "content": concept_extraction_prompt}
        ]
        
        response = llm_generator.generate_response(
            messages, 
            max_new_tokens=256, 
            temperature=0.3
        )
        
        # ì‘ë‹µì—ì„œ concept ì¶”ì¶œ
        concepts = []
        if response:
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
            raw_concepts = [concept.strip() for concept in response.split(',') if concept.strip()]
            concepts = [concept for concept in raw_concepts if len(concept) > 1]
        
        print(f"âœ… ì¶”ì¶œëœ concept: {concepts}")
        return concepts
        
    except Exception as e:
        print(f"âš ï¸ concept ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def search_by_concept_matching(question, concepts, enhanced_lkg_retriever, neo4j_driver, topN=20):
    """
    Concept ë§¤ì¹­ì„ í†µí•œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not concepts or not enhanced_lkg_retriever or not neo4j_driver:
        return [], []
    
    print("ğŸ” Concept ë§¤ì¹­ ê²€ìƒ‰ ì‹œì‘...")
    
    try:
        # Neo4jì—ì„œ concept_listê°€ ìˆëŠ” ë…¸ë“œë“¤ì„ ê²€ìƒ‰
        with neo4j_driver.session() as session:
            # Concept ë§¤ì¹­ ì¿¼ë¦¬
            concept_query = """
            MATCH (n:Node)
            WHERE n.concept_list IS NOT NULL
            RETURN n.id as node_id, n.name as node_name, n.concept_list as concept_list
            """
            
            result = session.run(concept_query)
            nodes_with_concepts = []
            
            for record in result:
                node_id = record["node_id"]
                node_name = record["node_name"]
                concept_list = record["concept_list"]
                
                if concept_list:
                    # concept_listê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                    if isinstance(concept_list, str):
                        try:
                            concept_list = json.loads(concept_list)
                        except:
                            concept_list = []
                    
                    # Concept ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    concept_score = 0
                    matched_concepts = []
                    
                    for concept in concepts:
                        for node_concept in concept_list:
                            if concept.lower() in node_concept.lower() or node_concept.lower() in concept.lower():
                                concept_score += 1
                                matched_concepts.append(node_concept)
                    
                    if concept_score > 0:
                        nodes_with_concepts.append({
                            'node_id': node_id,
                            'node_name': node_name,
                            'concept_list': concept_list,
                            'concept_score': concept_score,
                            'matched_concepts': matched_concepts
                        })
            
            # Concept ì ìˆ˜ë¡œ ì •ë ¬
            nodes_with_concepts.sort(key=lambda x: x['concept_score'], reverse=True)
            
            print(f"âœ… Concept ë§¤ì¹­ ê²°ê³¼: {len(nodes_with_concepts)}ê°œ ë…¸ë“œ")
            
            # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            top_nodes = nodes_with_concepts[:topN]
            
            content = []
            context_ids = []
            
            for node in top_nodes:
                content.append(node['node_name'])
                context_ids.append(node['node_id'])
                print(f"   - {node['node_name'][:50]}... (ì ìˆ˜: {node['concept_score']}, ë§¤ì¹­: {node['matched_concepts']})")
            
            return content, context_ids
            
    except Exception as e:
        print(f"âš ï¸ Concept ë§¤ì¹­ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return [], []

def search_text_nodes_by_content(question, concepts, neo4j_driver, topN=15):
    """
    Text ë…¸ë“œì˜ ì‹¤ì œ ë‚´ìš©ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    if not concepts or not neo4j_driver:
        return [], []
    
    print("ğŸ” Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰ ì‹œì‘...")
    
    try:
        with neo4j_driver.session() as session:
            all_matched_texts = []
            
            # ê° conceptì— ëŒ€í•´ Text ë…¸ë“œì—ì„œ ê²€ìƒ‰
            for concept in concepts:
                # conceptì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                words = concept.split()
                for word in words:
                    if len(word) > 1:  # 1ê¸€ì ë‹¨ì–´ ì œì™¸
                        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ë„ ê²€ìƒ‰
                        similar_words = get_similar_words(word)
                        
                        for search_word in similar_words:
                            # Text ë…¸ë“œì—ì„œ ê²€ìƒ‰
                            text_query = """
                            MATCH (t:Text)
                            WHERE t.text CONTAINS $word
                            RETURN t.id as text_id, t.text as text_content
                            LIMIT 10
                            """
                            
                            result = session.run(text_query, word=search_word)
                            for record in result:
                                all_matched_texts.append({
                                    'text_id': record["text_id"],
                                    'text_content': record["text_content"],
                                    'search_word': search_word,
                                    'original_word': word
                                })
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê³„ì‚°
            text_scores = {}
            for text in all_matched_texts:
                text_id = text['text_id']
                if text_id not in text_scores:
                    text_scores[text_id] = {
                        'text_id': text_id,
                        'text_content': text['text_content'],
                        'score': 0,
                        'matched_words': []
                    }
                
                # ë§¤ì¹­ëœ ë‹¨ì–´ ìˆ˜ë¡œ ì ìˆ˜ ê³„ì‚°
                matched_words = []
                for concept in concepts:
                    for word in concept.split():
                        if len(word) > 1 and word in text['text_content']:
                            matched_words.append(word)
                
                text_scores[text_id]['score'] += len(matched_words)
                text_scores[text_id]['matched_words'].extend(matched_words)
            
            # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_texts = sorted(text_scores.values(), key=lambda x: x['score'], reverse=True)
            
            # ìƒìœ„ Nê°œ ì„ íƒ
            selected_texts = sorted_texts[:topN]
            
            content = [text['text_content'] for text in selected_texts]
            context_ids = [text['text_id'] for text in selected_texts]
            
            print(f"âœ… Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼: {len(selected_texts)}ê°œ")
            for i, text in enumerate(selected_texts[:5]):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                print(f"   - {text['text_content'][:50]}... (ì ìˆ˜: {text['score']}, ë§¤ì¹­: {text['matched_words'][:3]})")
            
            return content, context_ids
            
    except Exception as e:
        print(f"âš ï¸ Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return [], []

def get_similar_words(word):
    """í•œêµ­ì–´ ìœ ì‚¬ì–´ ë°˜í™˜"""
    similar_dict = {
        'ì¤‘ëŒ€í•œ': ['ì¤‘ìš”í•œ', 'ì£¼ìš”í•œ', 'í•µì‹¬ì ì¸'],
        'ë¶€ì •ì ': ['ë‚˜ìœ', 'ì•…í™”ëœ', 'ë¶ˆë¦¬í•œ'],
        'ë³€ê²½': ['ë³€ë™', 'ë³€í™”', 'ìˆ˜ì •'],
        'MAE': ['ì¤‘ìš”í•œ ë¶€ì •ì  ë³€ë™', 'ì¤‘ëŒ€í•œ ë¶€ì •ì  ë³€ê²½'],
        'ê±°ë˜ì¢…ê²°': ['ê±°ë˜ ì¢…ê²°', 'ê±°ë˜ì™„ë£Œ', 'ê±°ë˜ë§ˆê°'],
        'ê³„ì•½': ['ê³„ì•½ì„œ', 'í˜‘ì•½', 'ì•½ì •']
    }
    
    # ì •í™•í•œ ë§¤ì¹­
    if word in similar_dict:
        return [word] + similar_dict[word]
    
    # ë¶€ë¶„ ë§¤ì¹­
    similar_words = [word]
    for key, values in similar_dict.items():
        if word in key or key in word:
            similar_words.extend(values)
    
    return list(set(similar_words))  # ì¤‘ë³µ ì œê±°

def enhance_search_with_concept_expansion(question, concepts, enhanced_lkg_retriever, topN=10):
    """
    Conceptì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.
    """
    if not concepts or not enhanced_lkg_retriever:
        return [], []
    
    print("ğŸ” Concept í™•ì¥ ê²€ìƒ‰ ì‹œì‘...")
    
    try:
        # ì›ë³¸ ì§ˆë¬¸ + conceptë“¤ì„ ê²°í•©í•œ í™•ì¥ ì¿¼ë¦¬ ìƒì„±
        expanded_query = f"{question} {' '.join(concepts)}"
        
        # ì¡°í•­ ê²€ìƒ‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰ (ì¤‘ë³µ ë°©ì§€)
        if not enhanced_lkg_retriever.is_clause_question(expanded_query):
            # EnhancedLargeKGRetrieverë¡œ ê²€ìƒ‰
            result = enhanced_lkg_retriever.retrieve(expanded_query, topN=topN)
            
            # ê²°ê³¼ê°€ 2ê°œì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì–¸íŒ¨í‚¹
            if result and len(result) == 2:
                content, context_ids = result
            else:
                # ê²°ê³¼ê°€ 2ê°œê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                print(f"âš ï¸ retrieve ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ: {type(result)}, ê¸¸ì´: {len(result) if result else 'None'}")
                return [], []
        else:
            # ì¡°í•­ ì§ˆë¬¸ì¸ ê²½ìš° ì¡°í•­ ê²€ìƒ‰ë§Œ ì‹¤í–‰
            clause_results = enhanced_lkg_retriever.search_clause_directly(expanded_query, topN=topN)
            if clause_results:
                content = [result['text'] for result in clause_results]
                context_ids = [result['textId'] for result in clause_results]
            else:
                content, context_ids = [], []
        
        if content and context_ids:
            print(f"âœ… Concept í™•ì¥ ê²€ìƒ‰: {len(content)}ê°œ ê²°ê³¼")
            return content, context_ids
        else:
            print("âš ï¸ Concept í™•ì¥ ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
            return [], []
            
    except Exception as e:
        print(f"âš ï¸ Concept í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return [], []

def rerank_results_by_concept_similarity(content, context_ids, concepts, neo4j_driver):
    """
    Concept ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ìˆœìœ„í™”í•©ë‹ˆë‹¤.
    """
    if not content or not context_ids or not concepts or not neo4j_driver:
        return content, context_ids
    
    print("ï¿½ï¿½ Concept ìœ ì‚¬ë„ ê¸°ë°˜ ì¬ìˆœìœ„í™” ì‹œì‘...")
    
    try:
        # Neo4jì—ì„œ ê° ë…¸ë“œì˜ concept_list ê°€ì ¸ì˜¤ê¸°
        with neo4j_driver.session() as session:
            node_concepts = {}
            
            for context_id in context_ids:
                query = """
                MATCH (n:Node {id: $node_id})
                WHERE n.concept_list IS NOT NULL
                RETURN n.concept_list as concept_list
                """
                
                result = session.run(query, node_id=context_id)
                record = result.single()
                
                if record and record["concept_list"]:
                    concept_list = record["concept_list"]
                    if isinstance(concept_list, str):
                        try:
                            concept_list = json.loads(concept_list)
                        except:
                            concept_list = []
                    node_concepts[context_id] = concept_list
            
            # Concept ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
            scored_results = []
            
            for i, (content_item, context_id) in enumerate(zip(content, context_ids)):
                if context_id in node_concepts:
                    concept_list = node_concepts[context_id]
                    
                    # Concept ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    concept_score = 0
                    for concept in concepts:
                        for node_concept in concept_list:
                            if concept.lower() in node_concept.lower() or node_concept.lower() in concept.lower():
                                concept_score += 1
                    
                    scored_results.append({
                        'content': content_item,
                        'context_id': context_id,
                        'concept_score': concept_score,
                        'original_rank': i
                    })
                else:
                    scored_results.append({
                        'content': content_item,
                        'context_id': context_id,
                        'concept_score': 0,
                        'original_rank': i
                    })
            
            # Concept ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
            scored_results.sort(key=lambda x: x['concept_score'], reverse=True)
            
            # ì¬ì •ë ¬ëœ ê²°ê³¼ ë°˜í™˜
            reranked_content = [item['content'] for item in scored_results]
            reranked_ids = [item['context_id'] for item in scored_results]
            
            print(f"âœ… Concept ì¬ìˆœìœ„í™” ì™„ë£Œ: {len(reranked_content)}ê°œ ê²°ê³¼")
            
            # ìƒìœ„ 5ê°œ ê²°ê³¼ì˜ ì ìˆ˜ ì¶œë ¥
            for i, item in enumerate(scored_results[:5]):
                print(f"   {i+1}. ì ìˆ˜: {item['concept_score']}, ì›ë˜ ìˆœìœ„: {item['original_rank']+1}")
            
            return reranked_content, reranked_ids
            
    except Exception as e:
        print(f"âš ï¸ Concept ì¬ìˆœìœ„í™” ì‹¤íŒ¨: {e}")
        return content, context_ids

def is_clause_question(question):
    """
    ì§ˆë¬¸ì´ ì¡°í•­ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒë‹¨
    """
    import re
    clause_patterns = [
        r'ì œ\d+ì¡°',
        r'\d+ì¡°',
        r'ì¡°í•­\s*\d+',
        r'ì œ\d+ì¡°\s*\d+í•­',
        r'\d+ì¡°\s*\d+í•­',
        r'ë¹„ë°€ìœ ì§€',
        r'ê³„ì•½í•´ì§€',
        r'ì†í•´ë°°ìƒ',
        r'ì§€ì ì¬ì‚°ê¶Œ',
        r'ìœ ì§€ë³´ìˆ˜',
        r'ëŒ€ê°€',
        r'ì±…ì„',
        r'íš¨ë ¥',
        r'ë¶„ìŸ',
        r'ê±°ë˜ì¢…ê²°',
        r'ë°°ë‹¹',
        r'ì°¨ì…',
        r'ë§¤ë„ì¸',
        r'ë§¤ìˆ˜ì¸',
        r'ê³„ì•½',
        r'ê°€ê²©',
        r'ì¡°ì •',
        r'ìš´ì „ìë³¸',
        r'ë¶€ì±„'
    ]
    
    for pattern in clause_patterns:
        if re.search(pattern, question):
            return True
    return False

def extract_key_terms_from_hippo_results(hippo_content, llm_generator):
    """
    HiPPO-RAG2 ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ì¡°í•­ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not hippo_content:
        return []
    
    print("ğŸ” HiPPO-RAG2 ê²°ê³¼ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    try:
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
        combined_text = "\n".join(hippo_content[:10])  # ìƒìœ„ 10ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
        
        # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        keyword_extraction_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê³„ì•½ì„œ ì¡°í•­ê³¼ ê´€ë ¨ëœ í•µì‹¬ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{combined_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
1. ì¡°í•­ ë²ˆí˜¸ (ì˜ˆ: ì œ6ì¡°, ì œ8ì¡° 3í•­, ì œ12ì¡° 2í•­ ë“±)
2. í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: ë¹„ë°€ìœ ì§€, ì†í•´ë°°ìƒ, ê³„ì•½í•´ì§€, ì§€ì ì¬ì‚°ê¶Œ ë“±)
3. ê´€ë ¨ ê°œë… (ì˜ˆ: ê³„ì•½ë‹¹ì‚¬ì, ì˜ë¬´, ê¶Œë¦¬, ì¡°ê±´ ë“±)

ê° í•­ëª©ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
"""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ì¡°í•­ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."}, 
            {"role": "user", "content": keyword_extraction_prompt}
        ]
        
        response = llm_generator.generate_response(
            messages, 
            max_new_tokens=512, 
            temperature=0.3
        )
        
        # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        if response:
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
            raw_keywords = [kw.strip() for kw in response.split(',') if kw.strip()]
            keywords = [kw for kw in raw_keywords if len(kw) > 1]
        
        print(f"âœ… ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        return keywords
        
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def search_multiple_clauses(extracted_keywords, enhanced_lkg_retriever, topN=10):
    """
    ì¶”ì¶œëœ í‚¤ì›Œë“œì—ì„œ ì—¬ëŸ¬ ì¡°í•­ì„ ì°¾ì•„ì„œ ê°ê° ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì—°ê²°ëœ ë…¸ë“œë„ í•¨ê»˜ í¬í•¨í•˜ì—¬ ë” í¬ê´„ì ì¸ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    if not extracted_keywords or not enhanced_lkg_retriever:
        return [], []
    
    print("ğŸ” ì—¬ëŸ¬ ì¡°í•­ ê²€ìƒ‰ ì‹œì‘...")
    
    # ì¡°í•­ ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
    import re
    clause_pattern = r'ì œ?(\d+)ì¡°'
    found_clauses = set()
    
    for keyword in extracted_keywords:
        matches = re.findall(clause_pattern, keyword)
        for match in matches:
            found_clauses.add(int(match))
    
    print(f"ğŸ” ë°œê²¬ëœ ì¡°í•­ë“¤: {sorted(found_clauses)}")
    
    if not found_clauses:
        print("âš ï¸ ì¡°í•­ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return [], []
    
    all_content = []
    all_ids = []
    
    # ê° ì¡°í•­ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ê²€ìƒ‰
    for clause_num in sorted(found_clauses):
        try:
            print(f"ğŸ” ì œ{clause_num}ì¡° ê²€ìƒ‰ ì¤‘...")
            clause_query = f"ì œ{clause_num}ì¡°"
            
            # EnhancedLargeKGRetrieverì˜ ì¡°í•­ ê²€ìƒ‰ ì‚¬ìš© (ì—°ê²°ëœ ë…¸ë“œ í¬í•¨)
            clause_content, clause_ids = enhanced_lkg_retriever.retrieve(clause_query, topN=topN//len(found_clauses))
            
            if clause_content and clause_ids:
                all_content.extend(clause_content)
                all_ids.extend(clause_ids)
                print(f"âœ… ì œ{clause_num}ì¡°: {len(clause_content)}ê°œ ê²°ê³¼ (ì—°ê²°ëœ ë…¸ë“œ í¬í•¨)")
                # ë””ë²„ê¹…: ê° ê²°ê³¼ì˜ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
                for i, (content_item, content_id) in enumerate(zip(clause_content, clause_ids)):
                    print(f"   ê²°ê³¼ {i+1} (ID: {content_id}): {content_item[:100]}...")
            else:
                print(f"âš ï¸ ì œ{clause_num}ì¡°: ê²°ê³¼ ì—†ìŒ")
                
        except Exception as e:
            print(f"âš ï¸ ì œ{clause_num}ì¡° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    # ì¤‘ë³µ ì œê±°
    unique_content = []
    unique_ids = []
    seen_ids = set()
    
    for content_item, context_id in zip(all_content, all_ids):
        if content_item and context_id and context_id not in seen_ids:
            seen_ids.add(context_id)
            unique_content.append(content_item)
            unique_ids.append(context_id)
    
    print(f"âœ… ì´ ì¡°í•­ ê²€ìƒ‰ ê²°ê³¼: {len(unique_content)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
    return unique_content, unique_ids

def concept_enhanced_hybrid_retrieve(question, enhanced_lkg_retriever, hippo_retriever, llm_generator, neo4j_driver, topN=50):
    """
    Conceptì„ í™œìš©í•œ í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ìˆ˜ì • ë²„ì „)
    0. ì¡°í•­ ê²€ìƒ‰ ì‹œë„ (ì¡°í•­ ì§ˆë¬¸ì¸ ê²½ìš°)
    1. ì§ˆë¬¸ì—ì„œ concept ì¶”ì¶œ
    2. Concept ë§¤ì¹­ ê²€ìƒ‰
    3. Concept í™•ì¥ ê²€ìƒ‰
    4. HiPPO-RAG2 ê²€ìƒ‰
    5. ê²°ê³¼ ì¬ìˆœìœ„í™”
    """
    print(f"ï¿½ï¿½ Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: {question} (ìµœëŒ€ {topN}ê°œ)")
    
    content = []
    context_ids = []
    
    # 0ë‹¨ê³„: Neo4j ì§ì ‘ ê²€ìƒ‰ (ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´)
    print(f"ğŸ” 0ë‹¨ê³„ - Neo4j ì§ì ‘ ê²€ìƒ‰ ì‹œë„")
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = enhanced_lkg_retriever._extract_keywords_from_query(question)
        print(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        
        if keywords:
            # í‚¤ì›Œë“œë¡œ Neo4jì—ì„œ ì§ì ‘ ê²€ìƒ‰
            keyword_results = enhanced_lkg_retriever._search_by_keywords(keywords, topN=15)
            if keyword_results:
                keyword_content = [result['text'] for result in keyword_results]
                keyword_ids = [result['textId'] for result in keyword_results]
                content.extend(keyword_content)
                context_ids.extend(keyword_ids)
                print(f"âœ… Neo4j ì§ì ‘ ê²€ìƒ‰: {len(keyword_content)}ê°œ ê²°ê³¼")
            else:
                print("âš ï¸ Neo4j ì§ì ‘ ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
        else:
            print("âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
    except Exception as e:
        print(f"âš ï¸ Neo4j ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 0.5ë‹¨ê³„: ì¡°í•­ ê²€ìƒ‰ ì‹œë„ (ì¡°í•­ ì§ˆë¬¸ì¸ ê²½ìš°)
    if enhanced_lkg_retriever.is_clause_question(question):
        print(f"ğŸ” 0.5ë‹¨ê³„ - ì¡°í•­ ê²€ìƒ‰ ì‹œë„")
        try:
            # ì¡°í•­ ê²€ìƒ‰ë§Œ ì§ì ‘ ì‹¤í–‰ (ì¤‘ë³µ ë°©ì§€)
            clause_results = enhanced_lkg_retriever.search_clause_directly(question, topN=10)
            if clause_results:
                clause_content = [result['text'] for result in clause_results]
                clause_ids = [result['textId'] for result in clause_results]
                content.extend(clause_content)
                context_ids.extend(clause_ids)
                print(f"âœ… ì¡°í•­ ê²€ìƒ‰: {len(clause_content)}ê°œ ê²°ê³¼")
            else:
                print("âš ï¸ ì¡°í•­ ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"âš ï¸ ì¡°í•­ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ concept ì¶”ì¶œ (ì¡°í•­ ì§ˆë¬¸ì´ì–´ë„ ì‹¤í–‰)
    concepts = extract_concepts_from_question(question, llm_generator)
    
    # 2ë‹¨ê³„: Concept ë§¤ì¹­ ê²€ìƒ‰ (ì „ì²´ì˜ 30%)
    # 2.5ë‹¨ê³„: Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰ (ì „ì²´ì˜ 20%)
    text_content_topN = max(1, int(topN * 0.2))
    print(f"ğŸ” 1.5ë‹¨ê³„ - Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰: {text_content_topN}ê°œ")

    if concepts and neo4j_driver:
        try:
            text_content, text_ids = search_text_nodes_by_content(
                question, concepts, neo4j_driver, text_content_topN
            )
            
            if text_content and text_ids:
                content.extend(text_content)
                context_ids.extend(text_ids)
                print(f"âœ… Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰: {len(text_content)}ê°œ ê²°ê³¼")
            else:
                print("âš ï¸ Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"âš ï¸ Text ë…¸ë“œ ë‚´ìš© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
    concept_matching_topN = max(1, int(topN * 0.3))
    print(f"ï¿½ï¿½ 1ë‹¨ê³„ - Concept ë§¤ì¹­ ê²€ìƒ‰: {concept_matching_topN}ê°œ")
    
    if concepts and neo4j_driver:
        try:
            concept_content, concept_ids = search_by_concept_matching(
                question, concepts, enhanced_lkg_retriever, neo4j_driver, concept_matching_topN
            )
            
            if concept_content and concept_ids:
                content.extend(concept_content)
                context_ids.extend(concept_ids)
                print(f"âœ… Concept ë§¤ì¹­ ê²€ìƒ‰: {len(concept_content)}ê°œ ê²°ê³¼")
            else:
                print("âš ï¸ Concept ë§¤ì¹­ ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"âš ï¸ Concept ë§¤ì¹­ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 3ë‹¨ê³„: Concept í™•ì¥ ê²€ìƒ‰ (ì „ì²´ì˜ 40%)
    concept_expansion_topN = max(1, int(topN * 0.3))
    print(f"ï¿½ï¿½ 2ë‹¨ê³„ - Concept í™•ì¥ ê²€ìƒ‰: {concept_expansion_topN}ê°œ")
    
    if concepts and enhanced_lkg_retriever:
        try:
            expansion_content, expansion_ids = enhance_search_with_concept_expansion(
                question, concepts, enhanced_lkg_retriever, concept_expansion_topN
            )
            
            if expansion_content and expansion_ids:
                content.extend(expansion_content)
                context_ids.extend(expansion_ids)
                print(f"âœ… Concept í™•ì¥ ê²€ìƒ‰: {len(expansion_content)}ê°œ ê²°ê³¼")
            else:
                print("âš ï¸ Concept í™•ì¥ ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"âš ï¸ Concept í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 4ë‹¨ê³„: HiPPO-RAG2 ê²€ìƒ‰ (ì „ì²´ì˜ 50%)
    hippo_topN = max(1, int(topN * 0.5))
    print(f"ï¿½ï¿½ 3ë‹¨ê³„ - HiPPO-RAG2 ê²€ìƒ‰: {hippo_topN}ê°œ")
    
    if hippo_retriever:
        try:
            hippo_content, hippo_ids = hippo_retriever.retrieve(question, topN=hippo_topN)
            
            if hippo_content and hippo_ids:
                content.extend(hippo_content)
                context_ids.extend(hippo_ids)
                print(f"âœ… HiPPO-RAG2 ê²€ìƒ‰: {len(hippo_content)}ê°œ ê²°ê³¼")
            else:
                print("âš ï¸ HiPPO-RAG2 ê²€ìƒ‰: ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"âš ï¸ HiPPO-RAG2 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 5ë‹¨ê³„: ì¤‘ë³µ ì œê±°
    unique_content = []
    unique_ids = []
    seen_ids = set()
    
    if content and context_ids and len(content) == len(context_ids):
        for content_item, context_id in zip(content, context_ids):
            if content_item and context_id and context_id not in seen_ids:
                seen_ids.add(context_id)
                unique_content.append(content_item)
                unique_ids.append(context_id)
    
    # 6ë‹¨ê³„: Concept ìœ ì‚¬ë„ ê¸°ë°˜ ì¬ìˆœìœ„í™”
    if concepts and neo4j_driver and unique_content and unique_ids:
        try:
            unique_content, unique_ids = rerank_results_by_concept_similarity(
                unique_content, unique_ids, concepts, neo4j_driver
            )
        except Exception as e:
            print(f"âš ï¸ Concept ì¬ìˆœìœ„í™” ì‹¤íŒ¨: {e}")
    
    if not unique_content or not unique_ids:
        print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return [], []
    
    print(f"âœ… ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(unique_content)}ê°œ")
    return unique_content[:topN], unique_ids[:topN]

def save_qa_to_file(question, answer, qa_file_path):
    """ì§ˆë¬¸ê³¼ ë‹µë³€ì„ JSON íŒŒì¼ì— ì €ì¥"""
    try:
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        qa_data = []
        if os.path.exists(qa_file_path):
            with open(qa_file_path, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
        
        # ìƒˆë¡œìš´ Q&A ì¶”ê°€
        qa_entry = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer
        }
        qa_data.append(qa_entry)
        
        # íŒŒì¼ì— ì €ì¥
        with open(qa_file_path, 'w', encoding='utf-8') as f:
            json.dump(qa_data, f, ensure_ascii=False, indent=2)
        
        print(f"ï¿½ï¿½ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {qa_file_path}")
        
    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_qa_history(qa_file_path, max_entries=5):
    """ì´ì „ ì§ˆë¬¸ê³¼ ë‹µë³€ ê¸°ë¡ì„ ë¡œë“œí•˜ì—¬ í‘œì‹œ"""
    try:
        if not os.path.exists(qa_file_path):
            return
        
        with open(qa_file_path, 'r', encoding='utf-8') as f:
            qa_data = json.load(f)
        
        if not qa_data:
            return
        
        print(f"\nï¿½ï¿½ ìµœê·¼ ì§ˆë¬¸ê³¼ ë‹µë³€ ê¸°ë¡ (ìµœëŒ€ {max_entries}ê°œ):")
        print("=" * 60)
        
        # ìµœê·¼ í•­ëª©ë“¤ì„ ì—­ìˆœìœ¼ë¡œ í‘œì‹œ
        recent_entries = qa_data[-max_entries:]
        for i, entry in enumerate(reversed(recent_entries), 1):
            timestamp = entry.get('timestamp', 'ì•Œ ìˆ˜ ì—†ìŒ')
            question = entry.get('question', '')
            answer = entry.get('answer', '')
            
            print(f"\n[{i}] {timestamp}")
            print(f"ì§ˆë¬¸: {question}")
            print(f"ë‹µë³€: {answer[:200]}{'...' if len(answer) > 200 else ''}")
            print("-" * 40)
        
    except Exception as e:
        print(f"âš ï¸ ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")

def run_single_question(question, llm_generator, enhanced_lkg_retriever, hippo_retriever, neo4j_driver, qa_file_path=None):
    """ë‹¨ì¼ ì§ˆë¬¸ ì‹¤í–‰ (Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"""
    print(f"\n ì§ˆë¬¸: {question}")
    print("-" * 50)
    
    try:
        # Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        result = concept_enhanced_hybrid_retrieve(
            question, 
            enhanced_lkg_retriever, 
            hippo_retriever,
            llm_generator,
            neo4j_driver,
            topN=50
        )
        
        # ê²°ê³¼ ê²€ì¦ ë° ì•ˆì „í•œ ì–¸íŒ¨í‚¹
        if result and len(result) == 2:
            content, context_ids = result
            print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ - content: {type(content)}, context_ids: {type(context_ids)}")
            print(f"ğŸ” content ê¸¸ì´: {len(content) if content else 'None'}, context_ids ê¸¸ì´: {len(context_ids) if context_ids else 'None'}")
            
            # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
            if content:
                print(f"ğŸ“‹ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ (ì²˜ìŒ 3ê°œ):")
                for i, ctx in enumerate(content[:3], 1):
                    print(f"   {i}. {ctx[:100]}...")
            else:
                print("âŒ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")
        else:
            print("âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"ğŸ” result íƒ€ì…: {type(result)}, ê¸¸ì´: {len(result) if result else 'None'}")
            content, context_ids = [], []
        
        if content and context_ids:
            print(f"âœ… {len(content)}ê°œì˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
            print("ğŸ¤– LLMì„ ì‚¬ìš©í•´ì„œ ë‹µë³€ì„ ìƒì„± ì¤‘...")
            sorted_context = "\n".join(content)
            
            try:
                # í•œêµ­ì–´ ë‹µë³€ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                korean_system_instruction = (
                    "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ê³ ê¸‰ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                    "ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ìì‹ ì˜ ì§€ì‹ì„ í™œìš©í•´ì„œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    "ë‹µë³€ì€ 'Thought: 'ë¡œ ì‹œì‘í•˜ì—¬ ì¶”ë¡  ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³ , "
                    "'Answer: 'ë¡œ ëë‚˜ë©° ê°„ê²°í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "
                    "ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."
                )
                messages = [
                    {"role": "system", "content": korean_system_instruction},
                    {"role": "user", "content": f"{sorted_context}\n\n{question}\nThought:"},
                ]
                
                print(f" LLM í˜¸ì¶œ ì‹œì‘ - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sorted_context)}")
                answer = llm_generator.generate_response(
                    messages, 
                    max_new_tokens=2048, 
                    temperature=0.5,
                    validate_function=None
                )
                print(f" ë‹µë³€: {answer}")
                
                # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ íŒŒì¼ì— ì €ì¥
                if qa_file_path:
                    save_qa_to_file(question, answer, qa_file_path)
                
                # ë¹ˆ ë‹µë³€ì¸ ê²½ìš° ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                if not answer or answer == "[]" or len(str(answer)) < 5:
                    print(" ë¹ˆ ë‹µë³€ ê°ì§€, ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„...")
                    # í•œêµ­ì–´ ë‹µë³€ì„ ìœ„í•œ KG ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    korean_kg_system_instruction = (
                        "ë‹¹ì‹ ì€ ê³ ê¸‰ ë…í•´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¶”ì¶œëœ ì •ë³´ì™€ ì§ˆë¬¸ì„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                        "ì§€ì‹ ê·¸ë˜í”„ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ìì‹ ì˜ ì§€ì‹ì„ í™œìš©í•´ì„œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                        "ë‹µë³€ì€ 'Thought: 'ë¡œ ì‹œì‘í•˜ì—¬ ì¶”ë¡  ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³ , "
                        "'Answer: 'ë¡œ ëë‚˜ë©° ê°„ê²°í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "
                        "ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."
                    )
                    kg_messages = [
                        {"role": "system", "content": korean_kg_system_instruction},
                        {"role": "user", "content": f"{sorted_context}\n\n{question}"},
                    ]
                    answer = llm_generator.generate_response(
                        kg_messages, 
                        max_new_tokens=2048, 
                        temperature=0.5,
                        validate_function=None
                    )
                    print(f" ë°±ì—… ë‹µë³€: {answer}")
                    
                    # ë°±ì—… ë‹µë³€ë„ íŒŒì¼ì— ì €ì¥
                    if qa_file_path:
                        save_qa_to_file(question, answer, qa_file_path)
                    
            except Exception as e:
                print(f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                import traceback
                print(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
                answer = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
        else:
            print("âŒ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            answer = "ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì˜¤ë¥˜ ìƒí™©ë„ íŒŒì¼ì— ì €ì¥
            if qa_file_path:
                save_qa_to_file(question, answer, qa_file_path)
            
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
        answer = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        # ì˜¤ë¥˜ ìƒí™©ë„ íŒŒì¼ì— ì €ì¥
        if qa_file_path:
            save_qa_to_file(question, answer, qa_file_path)
    
    print("-" * 50)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Concept í™œìš© LKG + HiPPO-RAG2 í•˜ì´ë¸Œë¦¬ë“œ RAG ì§ˆë¬¸ ëª¨ë“œ ì‹œì‘!")
    print("="*80)
    print("ï¿½ï¿½ ì›í•˜ëŠ” ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ ì…ë ¥í•˜ì„¸ìš”!")
    print("ğŸ’¡ ê³„ì•½ì„œ ê´€ë ¨ ì§ˆë¬¸ ì˜ˆì‹œ:")
    print("   - 'ì œ6ì¡°ëŠ” ë¬´ì—‡ì— ê´€í•œ ì¡°í•­ì¸ê°€ìš”?'")
    print("   - 'ì œ8ì¡° 3í•­ì˜ ë‚´ìš©ì„ ì•Œë ¤ì£¼ì„¸ìš”'")
    print("   - 'ë¹„ë°€ìœ ì§€ ì¡°í•­ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”'")
    print("   - 'ê³„ì•½ í•´ì§€ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?'")
    print("   - 'ì†í•´ë°°ìƒì— ëŒ€í•œ ê·œì •ì„ ì•Œë ¤ì£¼ì„¸ìš”'")
    print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ï¿½ï¿½ 'history' ë˜ëŠ” 'ê¸°ë¡'ì„ ì…ë ¥í•˜ë©´ ì´ì „ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("="*80)
    
    # Q&A ì €ì¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    qa_file_path = os.path.join(script_dir, "qa_history_concept_enhanced.json")
    print(f"ğŸ“ ì§ˆë¬¸ê³¼ ë‹µë³€ì€ ë‹¤ìŒ íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤: {qa_file_path}")
    
    # ì´ì „ ê¸°ë¡ì´ ìˆìœ¼ë©´ ë³´ì—¬ì£¼ê¸°
    load_qa_history(qa_file_path, max_entries=3)
    
    # RAG ì‹œìŠ¤í…œ ë¡œë“œ
    enhanced_lkg_retriever, hippo_retriever, llm_generator, neo4j_driver = load_enhanced_rag_system()
    if llm_generator is None:
        return
    
    # retriever í™•ì¸
    if enhanced_lkg_retriever is None and hippo_retriever is None:
        print("âŒ retrieverê°€ ëª¨ë‘ Noneì…ë‹ˆë‹¤. RAG ì‹œìŠ¤í…œ ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ëŒ€í™”í˜• ì§ˆë¬¸ ì…ë ¥
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            question = input("\nğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("ï¿½ï¿½ Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not question:
                print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ê¸°ë¡ ë³´ê¸° ëª…ë ¹ ì²˜ë¦¬
            if question.lower() in ['history', 'ê¸°ë¡', 'h']:
                load_qa_history(qa_file_path, max_entries=10)
                continue
            
            # ì§ˆë¬¸ ì‹¤í–‰ (Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            run_single_question(question, llm_generator, enhanced_lkg_retriever, hippo_retriever, neo4j_driver, qa_file_path)
            
        except KeyboardInterrupt:
            print("\nï¿½ï¿½ Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("\nï¿½ï¿½ Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ëª¨ë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
