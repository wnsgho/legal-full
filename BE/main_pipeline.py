#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ATLAS ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
READMEì˜ ì˜ˆì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹ê·¸ë˜í”„ êµ¬ì¶•ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

=============================================================================
ì„œë²„ ë°°í¬ ì‹œ ìˆ˜ì • í•„ìš” ì‚¬í•­:
=============================================================================
1. ìƒëŒ€ import â†’ ì ˆëŒ€ import ë³€ê²½ (ì™„ë£Œ)
   - from .atlas_rag... â†’ from atlas_rag...

2. subprocess.run cwd ì„¤ì • ìˆ˜ì • (ì™„ë£Œ)
   - cwd="BE" â†’ cwd="." (BE ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•  ë•Œ)
   - ì„œë²„ì—ì„œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ë¯€ë¡œ cwd="BE"ë¡œ ë˜ëŒë ¤ì•¼ í•¨

3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸
   - .env íŒŒì¼ ê²½ë¡œ: load_dotenv('.env') â†’ load_dotenv('../.env')
   - DATA_DIRECTORY: "example_data" â†’ "BE/example_data"

4. Neo4j ì—°ê²° ì„¤ì •
   - NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE í™•ì¸

5. OpenRouter API ì„¤ì •
   - OPENAI_API_KEY, OPENAI_BASE_URL, OPENAI_MODEL_NAME í™•ì¸
=============================================================================
"""

import os
import subprocess
import sys
import glob
import logging
import io

# Windowsì—ì„œ UTF-8 ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
if sys.platform.startswith('win'):
    # stdoutê³¼ stderrì„ UTF-8ë¡œ ì„¤ì •
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ì„œë²„ ë°°í¬ ì‹œ Python ê²½ë¡œ ì„¤ì • (ëª¨ë“ˆë¡œ importë  ë•Œë„ ì‹¤í–‰)
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from dotenv import load_dotenv

# UTF-8 ë¡œê¹… ì„¤ì • (atlas_rag import ì „ì— ë¨¼ì € ì„¤ì •)
from atlas_rag.utils.utf8_logging import setup_utf8_logging
setup_utf8_logging()

# atlas_rag ëª¨ë“ˆë“¤ì„ ë‚˜ì¤‘ì— import
from atlas_rag.kg_construction.triple_extraction import KnowledgeGraphExtractor
from atlas_rag.kg_construction.triple_config import ProcessingConfig
from atlas_rag.llm_generator import LLMGenerator
from openai import OpenAI
from transformers import pipeline

# OpenAI í´ë¼ì´ì–¸íŠ¸ì˜ ë¡œê¹… ë¹„í™œì„±í™”
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

def check_files_exist(file_patterns, directory):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ íŒ¨í„´ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸: {directory}")
    if not os.path.exists(directory):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
        return False
    
    for pattern in file_patterns:
        full_pattern = os.path.join(directory, pattern)
        matches = glob.glob(full_pattern)
        print(f"ğŸ” íŒ¨í„´ ê²€ìƒ‰: {full_pattern} -> {matches}")
        if not matches:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_pattern}")
            return False
    print(f"âœ… ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {file_patterns}")
    return True

def check_triple_extraction_is_empty(keyword, output_directory):
    """íŠ¸ë¦¬í”Œ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    import json
    kg_extraction_dir = f"{output_directory}/kg_extraction"
    
    # ì‹¤ì œ íŒŒì¼ ì´ë¦„ íŒ¨í„´: {model_name}_{keyword}_output_{timestamp}_{shard}_in_{total}.json
    # ë˜ëŠ” {keyword}_kg_extraction*.json
    pattern1 = f"{keyword}_kg_extraction*.json"
    pattern2 = f"*_{keyword}_output_*.json"  # ëª¨ë¸ ì´ë¦„ì´ ì•ì— ë¶™ëŠ” ê²½ìš°
    
    full_pattern1 = os.path.join(kg_extraction_dir, pattern1)
    full_pattern2 = os.path.join(kg_extraction_dir, pattern2)
    matches1 = glob.glob(full_pattern1)
    matches2 = glob.glob(full_pattern2)
    matches = matches1 + matches2
    
    if not matches:
        print(f"ğŸ” íŠ¸ë¦¬í”Œ ì¶”ì¶œ ê²°ê³¼ í™•ì¸: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (íŒ¨í„´1: {pattern1}, íŒ¨í„´2: {pattern2})")
        return True  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²ƒìœ¼ë¡œ ê°„ì£¼
    
    # ê°€ì¥ ìµœê·¼ íŒŒì¼ í™•ì¸ (ì—¬ëŸ¬ íŒŒì¼ì´ ìˆì„ ìˆ˜ ìˆìŒ)
    if matches:
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœê·¼ íŒŒì¼ í™•ì¸
        matches_with_time = [(f, os.path.getmtime(f)) for f in matches]
        matches_with_time.sort(key=lambda x: x[1], reverse=True)
        latest_file = matches_with_time[0][0]
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                total_triples = 0
                line_count = 0
                for line in f:
                    line_count += 1
                    if line.strip():
                        try:
                            data = json.loads(line)
                            entity_relations = data.get("entity_relation_dict", [])
                            event_entities = data.get("event_entity_relation_dict", [])
                            event_relations = data.get("event_relation_dict", [])
                            total_triples += len(entity_relations) + len(event_entities) + len(event_relations)
                        except json.JSONDecodeError:
                            continue
                
                print(f"ğŸ” íŠ¸ë¦¬í”Œ ì¶”ì¶œ ê²°ê³¼ í™•ì¸: {line_count}ì¤„, {total_triples}ê°œ íŠ¸ë¦¬í”Œ (íŒŒì¼: {os.path.basename(latest_file)})")
                return total_triples == 0
        except Exception as e:
            print(f"âš ï¸ íŠ¸ë¦¬í”Œ ì¶”ì¶œ ê²°ê³¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì¬ì¶”ì¶œ
    
    return True

def convert_md_to_json(keyword):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    print("ğŸ“ ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    
    data_directory = os.getenv('DATA_DIRECTORY', 'BE/example_data')
    target_json = f"{data_directory}/{keyword}.json"
    if os.path.exists(target_json):
        print(f"âœ… {keyword}.json íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
    
    try:
        # markdown_to_json ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (md_data ë””ë ‰í† ë¦¬ ì „ì²´ ë³€í™˜)
        # í˜„ì¬ í´ë”ì—ì„œ ì‹¤í–‰í•˜ë¯€ë¡œ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
        relative_data_dir = data_directory
        cmd = [
            sys.executable, "-m", 
            "atlas_rag.kg_construction.utils.md_processing.markdown_to_json",
            "--input", f"{relative_data_dir}/md_data",
            "--output", relative_data_dir
        ]
        
        # í˜„ì¬ í´ë”ì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, cwd=".")
        print("âœ… ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ!")
        print(f"ì¶œë ¥: {result.stdout}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

def test_atlas_pipeline(start_step=1, keyword=None):
    """ATLAS ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    print(f"ğŸš€ ATLAS íŒŒì´í”„ë¼ì¸ ì‹œì‘! (ë‹¨ê³„ {start_step}ë¶€í„°)")
    print(f"ğŸ“ ì „ë‹¬ë°›ì€ keyword: {keyword}")
    print(f"ğŸ“‚ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    # kg_extractor ì´ˆê¸°í™”
    kg_extractor = None
    
    # .env íŒŒì¼ ë¡œë“œ (BE í´ë”ì˜ .env íŒŒì¼ ìš°ì„ )
    # API ì„œë²„ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„¤ì •
    env_path = 'BE/.env'  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ë  ë•Œ
    if not os.path.exists(env_path):
        env_path = '.env'  # BE ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ì‹¤í–‰ë  ë•Œ
    
    print(f"ğŸ” .env íŒŒì¼ ê²½ë¡œ í™•ì¸: {env_path}")
    print(f"ğŸ“„ .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(env_path)}")
    
    try:
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    # keywordê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°
    if keyword is None:
        keyword = os.getenv('KEYWORD')
    
    print(f"ğŸ”‘ ì‚¬ìš©í•  keyword: {keyword}")
    
    import_dir = os.getenv('IMPORT_DIRECTORY', 'import')
    output_directory = f'{import_dir}/{keyword}'
    
    print(f"ğŸ“ import_directory: {import_dir}")
    print(f"ğŸ“ output_directory: {output_directory}")
    
    # ì£¼ìš” í™˜ê²½ë³€ìˆ˜ í™•ì¸
    print(f"ğŸ” OPENAI_API_KEY ì¡´ì¬: {'ìˆìŒ' if os.getenv('OPENAI_API_KEY') else 'ì—†ìŒ'}")
    print(f"ğŸŒ OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'ê¸°ë³¸ê°’')}")
    print(f"ğŸ¤– DEFAULT_MODEL: {os.getenv('DEFAULT_MODEL', 'ê¸°ë³¸ê°’')}")
    print(f"ğŸ—„ï¸ NEO4J_URI: {os.getenv('NEO4J_URI', 'ê¸°ë³¸ê°’')}")
    print(f"ğŸ“Š DATA_DIRECTORY: {os.getenv('DATA_DIRECTORY', 'ê¸°ë³¸ê°’')}")
    
    if start_step <= 1:
        # 1. ëª¨ë¸ ì„¤ì •
        print("\nğŸ“‹ 1ë‹¨ê³„: ëª¨ë¸ ì„¤ì •")
        
        try:
            client = OpenAI(
                api_key=os.getenv('OPENAI_API_KEY'),
                base_url=os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
            )
            model_name = os.getenv('DEFAULT_MODEL', 'gpt-4.1-2025-04-14')
            triple_generator = LLMGenerator(client=client, model_name=model_name, verbose=False)
            print(f"âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ: {model_name}")
            
        except Exception as e:
            print(f"âŒ OpenAI API ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    if start_step <= 2:
        # 2. ì„¤ì • êµ¬ì„±
        print("\nğŸ“‹ 2ë‹¨ê³„: ì²˜ë¦¬ ì„¤ì • êµ¬ì„±")
        print(f"ğŸ”‘ ì‚¬ìš©í•  ê³„ì•½ì„œ: {keyword}")
    
    if start_step <= 0:
        # 0. ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ (keyword ì‚¬ìš©)
        print("\nğŸ“‹ 0ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜")
        if not convert_md_to_json(keyword):
            print("âŒ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return False
    
    if start_step <= 3:
        # model_name ê°€ì ¸ì˜¤ê¸° (start_step > 1ì¸ ê²½ìš°)
        if start_step > 1:
            model_name = os.getenv('DEFAULT_MODEL', 'gpt-4.1-2025-04-14')
        
        kg_extraction_config = ProcessingConfig(
            model_path=model_name,
            data_directory=os.getenv('DATA_DIRECTORY', "BE/example_data"),
            filename_pattern=keyword,
            remove_doc_spaces=True,
            output_directory=output_directory,
        )
        
        print(f"âœ… ì²˜ë¦¬ ì„¤ì • ì™„ë£Œ: {output_directory}")
        
        # 3. ì§€ì‹ê·¸ë˜í”„ ì¶”ì¶œê¸° ìƒì„±
        print("\nğŸ“‹ 3ë‹¨ê³„: ì§€ì‹ê·¸ë˜í”„ ì¶”ì¶œê¸° ìƒì„±")
        kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)
        print("âœ… ì§€ì‹ê·¸ë˜í”„ ì¶”ì¶œê¸° ìƒì„± ì™„ë£Œ")
    
    if start_step <= 4:
        # 4. íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì‹¤í–‰
        print("\nğŸš€ 4ë‹¨ê³„: íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì‹¤í–‰")
        triple_files = [
            f"{keyword}_kg_extraction.json",
            f"{keyword}_kg_extraction_processed.json"
        ]
        
        files_exist = check_files_exist(triple_files, f"{output_directory}/kg_extraction")
        is_empty = check_triple_extraction_is_empty(keyword, output_directory)
        
        if files_exist and not is_empty:
            print("âœ… íŠ¸ë¦¬í”Œ ì¶”ì¶œ íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  íŠ¸ë¦¬í”Œì´ ìˆìŠµë‹ˆë‹¤. ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            if files_exist and is_empty:
                print("âš ï¸ íŠ¸ë¦¬í”Œ ì¶”ì¶œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ë§Œ ë¹ˆ íŠ¸ë¦¬í”Œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì¶”ì¶œí•©ë‹ˆë‹¤.")
            else:
                print("ğŸ”„ íŠ¸ë¦¬í”Œ ì¶”ì¶œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            try:
                print(f"ğŸ”„ kg_extractor.run_extraction() ì‹¤í–‰ ì¤‘...")
                if kg_extractor is None:
                    print("âŒ kg_extractorê°€ Noneì…ë‹ˆë‹¤. 2-3ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                    return False
                kg_extractor.run_extraction()
                print("âœ… íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì™„ë£Œ!")
                
                # ì¶”ì¶œ í›„ ë‹¤ì‹œ í™•ì¸
                is_still_empty = check_triple_extraction_is_empty(keyword, output_directory)
                if is_still_empty:
                    print("âš ï¸ ê²½ê³ : íŠ¸ë¦¬í”Œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì—¬ì „íˆ ë¹ˆ íŠ¸ë¦¬í”Œì…ë‹ˆë‹¤.")
                    print("âš ï¸ LLM ì‘ë‹µì„ í™•ì¸í•˜ê±°ë‚˜ ëª¨ë¸ ì„¤ì •ì„ ì ê²€í•˜ì„¸ìš”.")
            except Exception as e:
                import traceback
                print(f"âŒ íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                return False
    
    # kg_extractorê°€ í•„ìš”í•œ ê²½ìš° ìƒì„±
    if kg_extractor is None:
        print("ğŸ”„ kg_extractor ìƒì„± ì¤‘...")
        # ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ kg_extractor ìƒì„±
        kg_extraction_config = ProcessingConfig(
            model_path="",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì • (LLM ì‚¬ìš© ì‹œ)
            data_directory=os.getenv('DATA_DIRECTORY', "BE/example_data"),
            filename_pattern=keyword,
            remove_doc_spaces=True,
            output_directory=output_directory,
        )
        kg_extractor = KnowledgeGraphExtractor(model=None, config=kg_extraction_config)
        print("âœ… kg_extractor ìƒì„± ì™„ë£Œ")
    
    # 5. JSONì„ CSVë¡œ ë³€í™˜
    print("\nğŸ”„ 5ë‹¨ê³„: JSONì„ CSVë¡œ ë³€í™˜")
    
    # CSV íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    csv_files = [
        f"{keyword}_triples.csv",
        f"{keyword}_entities.csv",
        f"{keyword}_relations.csv"
    ]
    if check_files_exist(csv_files, output_directory):
        print("âœ… CSV íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 5ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.convert_json_to_csv()
            print("âœ… CSV ë³€í™˜ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    # 6. ê°œë… ìƒì„±
    print("\nğŸ§  6ë‹¨ê³„: ê°œë… ìƒì„±")
    concept_files = [f"concept_shard_0.csv"]
    if check_files_exist(concept_files, f"{output_directory}/concepts"):
        print("âœ… ê°œë… ìƒì„± íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 6ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.generate_concept_csv_temp()
            print("âœ… ê°œë… ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ê°œë… ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    # 7. ê°œë… CSV ìƒì„±
    print("\nğŸ“Š 7ë‹¨ê³„: ê°œë… CSV ìƒì„±")
    
    # ê°œë… CSV íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    concept_csv_files = [
        f"concept_nodes_{keyword}_from_json_with_concept.csv",
        f"concept_edges_{keyword}_from_json_with_concept.csv"
    ]
    if check_files_exist(concept_csv_files, f"{output_directory}/concept_csv"):
        print("âœ… ê°œë… CSV íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 7ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.create_concept_csv()
            print("âœ… ê°œë… CSV ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ê°œë… CSV ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    # 8. GraphML ìƒì„±
    print("\nğŸ•¸ï¸ 8ë‹¨ê³„: GraphML ìƒì„±")
    
    # GraphML íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    graphml_files = [f"{keyword}_graph.graphml"]
    graphml_exists = check_files_exist(graphml_files, f"{output_directory}/kg_graphml")
    
    # GraphML íŒŒì¼ì— ì—£ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
    need_regenerate = False
    if graphml_exists:
        try:
            import networkx as nx
            graphml_path = f"{output_directory}/kg_graphml/{keyword}_graph.graphml"
            with open(graphml_path, "rb") as f:
                KG = nx.read_graphml(f)
            edge_count = len(KG.edges)
            node_count = len(KG.nodes)
            print(f"ğŸ” ê¸°ì¡´ GraphML í™•ì¸: {node_count}ê°œ ë…¸ë“œ, {edge_count}ê°œ ì—£ì§€")
            if edge_count == 0:
                print("âš ï¸ GraphMLì— ì—£ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë¦¬í”Œì´ ìƒˆë¡œ ì¶”ì¶œë˜ì—ˆìœ¼ë¯€ë¡œ GraphMLì„ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
                need_regenerate = True
        except Exception as e:
            print(f"âš ï¸ GraphML í™•ì¸ ì‹¤íŒ¨: {e}. GraphMLì„ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
            need_regenerate = True
    
    if not graphml_exists or need_regenerate:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 8ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            if need_regenerate:
                # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
                graphml_path = f"{output_directory}/kg_graphml/{keyword}_graph.graphml"
                if os.path.exists(graphml_path):
                    os.remove(graphml_path)
                    print(f"ğŸ—‘ï¸ ê¸°ì¡´ GraphML íŒŒì¼ ì‚­ì œ: {graphml_path}")
            kg_extractor.convert_to_graphml()
            print("âœ… GraphML ìƒì„± ì™„ë£Œ!")
            
            # ìƒì„± í›„ ì—£ì§€ ìˆ˜ í™•ì¸
            try:
                import networkx as nx
                graphml_path = f"{output_directory}/kg_graphml/{keyword}_graph.graphml"
                with open(graphml_path, "rb") as f:
                    KG = nx.read_graphml(f)
                edge_count = len(KG.edges)
                node_count = len(KG.nodes)
                print(f"âœ… GraphML ìƒì„± í™•ì¸: {node_count}ê°œ ë…¸ë“œ, {edge_count}ê°œ ì—£ì§€")
            except Exception as e:
                print(f"âš ï¸ GraphML í™•ì¸ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"âŒ GraphML ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    else:
        print("âœ… GraphML íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  ì—£ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # 9. ìˆ«ì ID ì¶”ê°€
    print("\nğŸ”¢ 9ë‹¨ê³„: ìˆ«ì ID ì¶”ê°€")
    
    # ì›ë³¸ GraphMLê³¼ ìˆ«ì IDê°€ ì¶”ê°€ëœ GraphML ë¹„êµ
    need_regenerate_numeric_id = False
    original_graphml = f"{output_directory}/kg_graphml/{keyword}_graph.graphml"
    numeric_id_graphml = f"{output_directory}/kg_graphml/{keyword}_graph_with_numeric_id.graphml"
    
    if os.path.exists(original_graphml) and os.path.exists(numeric_id_graphml):
        try:
            import networkx as nx
            # ì›ë³¸ GraphML í™•ì¸
            with open(original_graphml, "rb") as f:
                KG_original = nx.read_graphml(f)
            original_edges = len(KG_original.edges)
            original_nodes = len(KG_original.nodes)
            
            # ìˆ«ì ID GraphML í™•ì¸
            with open(numeric_id_graphml, "rb") as f:
                KG_numeric = nx.read_graphml(f)
            numeric_edges = len(KG_numeric.edges)
            numeric_nodes = len(KG_numeric.nodes)
            
            print(f"ğŸ” GraphML ë¹„êµ: ì›ë³¸({original_nodes}ë…¸ë“œ, {original_edges}ì—£ì§€) vs ìˆ«ìID({numeric_nodes}ë…¸ë“œ, {numeric_edges}ì—£ì§€)")
            
            # ì—£ì§€ ìˆ˜ê°€ ë‹¤ë¥´ë©´ ìˆ«ì IDë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì•¼ í•¨
            if original_edges != numeric_edges or original_nodes != numeric_nodes:
                print("âš ï¸ GraphMLì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ«ì IDë¥¼ ë‹¤ì‹œ ì¶”ê°€í•©ë‹ˆë‹¤.")
                need_regenerate_numeric_id = True
        except Exception as e:
            print(f"âš ï¸ GraphML ë¹„êµ ì‹¤íŒ¨: {e}. ìˆ«ì IDë¥¼ ë‹¤ì‹œ ì¶”ê°€í•©ë‹ˆë‹¤.")
            need_regenerate_numeric_id = True
    elif os.path.exists(original_graphml) and not os.path.exists(numeric_id_graphml):
        print("âš ï¸ ìˆ«ì ID GraphML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒì„±í•©ë‹ˆë‹¤.")
        need_regenerate_numeric_id = True
    
    # ìˆ«ì ID íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    numeric_id_files = [
        f"triple_nodes_{keyword}_from_json_without_emb_with_numeric_id.csv",
        f"triple_edges_{keyword}_from_json_without_emb_with_numeric_id.csv",
        f"text_nodes_{keyword}_from_json_with_numeric_id.csv"
    ]
    files_exist = check_files_exist(numeric_id_files, f"{output_directory}/triples_csv")
    
    if files_exist and not need_regenerate_numeric_id:
        print("âœ… ìˆ«ì ID íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  GraphMLê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        if need_regenerate_numeric_id:
            # ìˆ«ì ID GraphML íŒŒì¼ ì‚­ì œ
            if os.path.exists(numeric_id_graphml):
                os.remove(numeric_id_graphml)
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ìˆ«ì ID GraphML íŒŒì¼ ì‚­ì œ: {numeric_id_graphml}")
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 9ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.add_numeric_id()
            print("âœ… ìˆ«ì ID ì¶”ê°€ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ìˆ«ì ID ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    # GraphML íŒŒì¼ ë³µì‚¬ (ì„ë² ë”© ìƒì„±ìš©)
    import shutil
    source_graphml = f"{output_directory}/kg_graphml/{keyword}_graph.graphml"
    target_graphml = f"{output_directory}/kg_graphml/{keyword}_graph_with_numeric_id.graphml"
    
    # ì›ë³¸ GraphMLê³¼ ìˆ«ì ID GraphML ë¹„êµ
    if os.path.exists(source_graphml):
        try:
            import networkx as nx
            with open(source_graphml, "rb") as f:
                KG_source = nx.read_graphml(f)
            source_edges = len(KG_source.edges)
            source_nodes = len(KG_source.nodes)
            
            if os.path.exists(target_graphml):
                with open(target_graphml, "rb") as f:
                    KG_target = nx.read_graphml(f)
                target_edges = len(KG_target.edges)
                target_nodes = len(KG_target.nodes)
                
                # ì—£ì§€ ìˆ˜ê°€ ë‹¤ë¥´ë©´ ìˆ«ì ID GraphMLì´ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ
                if source_edges != target_edges or source_nodes != target_nodes:
                    print(f"âš ï¸ GraphML ë¶ˆì¼ì¹˜ ê°ì§€: ì›ë³¸({source_nodes}ë…¸ë“œ, {source_edges}ì—£ì§€) vs ìˆ«ìID({target_nodes}ë…¸ë“œ, {target_edges}ì—£ì§€)")
                    print("âš ï¸ ìˆ«ì ID GraphMLì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                    shutil.copy2(source_graphml, target_graphml)
                    print("âœ… GraphML íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                else:
                    print("âœ… GraphML íŒŒì¼ì´ ì¼ì¹˜í•©ë‹ˆë‹¤.")
            else:
                shutil.copy2(source_graphml, target_graphml)
                print("âœ… GraphML íŒŒì¼ ë³µì‚¬ ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ GraphML ë¹„êµ ì‹¤íŒ¨: {e}")
            if not os.path.exists(target_graphml):
                shutil.copy2(source_graphml, target_graphml)
                print("âœ… GraphML íŒŒì¼ ë³µì‚¬ ì™„ë£Œ!")

    # 10. ì„ë² ë”© ìƒì„±
    print("\nğŸ§® 10ë‹¨ê³„: ì„ë² ë”© ìƒì„±")
    
    # GraphML íŒŒì¼ í™•ì¸
    graphml_path = f"{output_directory}/kg_graphml/{keyword}_graph_with_numeric_id.graphml"
    if not os.path.exists(graphml_path):
        print(f"âŒ GraphML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {graphml_path}")
        print("âš ï¸ GraphML íŒŒì¼ì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. (ì„ë² ë”©ì€ ìƒì„±ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)")
    else:
        # GraphML íŒŒì¼ì—ì„œ ë…¸ë“œ/ì—£ì§€ ìˆ˜ í™•ì¸
        try:
            import networkx as nx
            with open(graphml_path, "rb") as f:
                KG = nx.read_graphml(f)
            print(f"ğŸ“Š GraphML íŒŒì¼ í™•ì¸: {len(KG.nodes)}ê°œ ë…¸ë“œ, {len(KG.edges)}ê°œ ì—£ì§€")
            
            # ë…¸ë“œ íƒ€ì… í™•ì¸
            node_types = {}
            for node in KG.nodes:
                node_type = KG.nodes[node].get("type", "unknown")
                node_types[node_type] = node_types.get(node_type, 0) + 1
            print(f"ğŸ“Š ë…¸ë“œ íƒ€ì… ë¶„í¬: {node_types}")
            
            if len(KG.nodes) == 0:
                print("âš ï¸ ê²½ê³ : GraphML íŒŒì¼ì— ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤!")
            if len(KG.edges) == 0:
                print("âš ï¸ ê²½ê³ : GraphML íŒŒì¼ì— ì—£ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        except Exception as e:
            print(f"âš ï¸ GraphML íŒŒì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    # ì„ë² ë”© íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    embedding_files = [
        f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_node_faiss.index",
        f"{keyword}_eventTrue_conceptTrue_node_list.pkl",
        f"{keyword}_text_faiss.index"
    ]
    
    # ì—£ì§€ ì„ë² ë”© íŒŒì¼ë„ í™•ì¸
    encoder_model_name = os.getenv('DEFAULT_EMBEDDING_MODEL', "sentence-transformers/all-MiniLM-L6-v2")
    encoder_model_short = encoder_model_name.split('/')[-1]
    edge_embedding_file = f"{keyword}_eventTrue_conceptTrue_{encoder_model_short}_edge_faiss.index"
    
    # GraphMLì— ì—£ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
    has_edges = False
    if os.path.exists(graphml_path):
        try:
            import networkx as nx
            with open(graphml_path, "rb") as f:
                KG_check = nx.read_graphml(f)
            has_edges = len(KG_check.edges) > 0
        except:
            pass
    
    # ì—£ì§€ ì„ë² ë”© íŒŒì¼ ì¡´ì¬ í™•ì¸
    edge_embedding_exists = os.path.exists(f"{output_directory}/precompute/{edge_embedding_file}")
    
    # ê¸°ë³¸ ì„ë² ë”© íŒŒì¼ ì¡´ì¬ í™•ì¸
    basic_files_exist = check_files_exist(embedding_files, f"{output_directory}/precompute")
    
    # GraphMLì— ì—£ì§€ê°€ ìˆëŠ”ë° ì—£ì§€ ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ë‹¤ì‹œ ìƒì„±
    need_regenerate = False
    if has_edges and not edge_embedding_exists:
        print(f"âš ï¸ GraphMLì— ì—£ì§€ê°€ ìˆì§€ë§Œ ì—£ì§€ ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ({edge_embedding_file})")
        print("âš ï¸ ì„ë² ë”©ì„ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
        need_regenerate = True
    elif not basic_files_exist:
        need_regenerate = True
    
    if basic_files_exist and not need_regenerate:
        print("âœ… ì„ë² ë”© íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        if need_regenerate:
            # ê¸°ì¡´ ì„ë² ë”© íŒŒì¼ ì‚­ì œ (ì„ íƒì )
            import glob
            precompute_dir = f"{output_directory}/precompute"
            if os.path.exists(precompute_dir):
                for pattern in [f"{keyword}_*_faiss.index", f"{keyword}_*.pkl"]:
                    for file in glob.glob(os.path.join(precompute_dir, pattern)):
                        try:
                            os.remove(file)
                            print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì„ë² ë”© íŒŒì¼ ì‚­ì œ: {os.path.basename(file)}")
                        except:
                            pass
        try:
            from sentence_transformers import SentenceTransformer
            from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
            from atlas_rag.vectorstore.create_graph_index import create_embeddings_and_index
            
            # Sentence Transformer ëª¨ë¸ ë¡œë“œ
            encoder_model_name = os.getenv('DEFAULT_EMBEDDING_MODEL', "sentence-transformers/all-MiniLM-L6-v2")
            print(f"ğŸ”„ {encoder_model_name} ëª¨ë¸ì„ ë¡œë”© ì¤‘...")
            
            sentence_model = SentenceTransformer(
                encoder_model_name, 
                trust_remote_code=True, 
                model_kwargs={'device_map': "auto"}
            )
            sentence_encoder = SentenceEmbedding(sentence_model)
            
            # create_embeddings_and_index ì‚¬ìš©
            print("ğŸ”„ create_embeddings_and_index ì‹¤í–‰ ì¤‘...")
            create_embeddings_and_index(
                sentence_encoder=sentence_encoder,
                model_name=encoder_model_name,
                working_directory=output_directory,
                keyword=keyword,
                include_events=True,
                include_concept=True,
                normalize_embeddings=True,
                text_batch_size=40,
                node_and_edge_batch_size=256
            )
            print("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            import traceback
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            # ì„ë² ë”© ìƒì„± ì‹¤íŒ¨í•´ë„ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰ (ê²½ê³ ë§Œ)
            print("âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨í–ˆì§€ë§Œ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

    
    # 11. ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ ìƒì„±
    print("\nğŸ” 11ë‹¨ê³„: ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ ìƒì„±")
    emb_csv_files = [
        f"triples_csv/triple_nodes_{keyword}_from_json_with_emb.csv",
        f"triples_csv/text_nodes_{keyword}_from_json_with_emb.csv",
        f"triples_csv/triple_edges_{keyword}_from_json_with_concept_with_emb.csv"
    ]
    if check_files_exist(emb_csv_files, output_directory):
        print("âœ… ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            from sentence_transformers import SentenceTransformer
            from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
            
            # Sentence Transformer ëª¨ë¸ ë¡œë“œ
            encoder_model_name = os.getenv('DEFAULT_EMBEDDING_MODEL', "sentence-transformers/all-MiniLM-L6-v2")
            print(f"ğŸ”„ {encoder_model_name} ëª¨ë¸ì„ ë¡œë”© ì¤‘...")
            
            sentence_model = SentenceTransformer(
                encoder_model_name, 
                trust_remote_code=True, 
                model_kwargs={'device_map': "auto"}
            )
            sentence_encoder = SentenceEmbedding(sentence_model)
            
            # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
            node_csv_without_emb = f"{output_directory}/triples_csv/triple_nodes_{keyword}_from_json_without_emb.csv"
            node_csv_file = f"{output_directory}/triples_csv/triple_nodes_{keyword}_from_json_with_emb.csv"
            edge_csv_without_emb = f"{output_directory}/concept_csv/triple_edges_{keyword}_from_json_with_concept.csv"
            edge_csv_file = f"{output_directory}/triples_csv/triple_edges_{keyword}_from_json_with_concept_with_emb.csv"
            text_node_csv_without_emb = f"{output_directory}/triples_csv/text_nodes_{keyword}_from_json.csv"
            text_node_csv = f"{output_directory}/triples_csv/text_nodes_{keyword}_from_json_with_emb.csv"
            
            # ì„ë² ë”©ì„ CSV íŒŒì¼ì— ì¶”ê°€
            sentence_encoder.compute_kg_embedding(
                node_csv_without_emb=node_csv_without_emb,
                node_csv_file=node_csv_file,
                edge_csv_without_emb=edge_csv_without_emb,
                edge_csv_file=edge_csv_file,
                text_node_csv_without_emb=text_node_csv_without_emb,
                text_node_csv=text_node_csv,
                batch_size=2048
            )
            print("âœ… ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    # 12. FAISS ì¸ë±ìŠ¤ ìƒì„±
    print("\nğŸ” 12ë‹¨ê³„: FAISS ì¸ë±ìŠ¤ ìƒì„±")
    # precompute í´ë”ì—ì„œ FAISS ì¸ë±ìŠ¤ íŒŒì¼ í™•ì¸
    precompute_dir = f"{output_directory}/precompute"
    
    # GraphMLì— ì—£ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
    has_edges = False
    edge_count = 0
    graphml_path = f"{output_directory}/kg_graphml/{keyword}_graph_with_numeric_id.graphml"
    if os.path.exists(graphml_path):
        try:
            import networkx as nx
            with open(graphml_path, "rb") as f:
                KG_check = nx.read_graphml(f)
            edge_count = len(KG_check.edges)
            has_edges = edge_count > 0
            print(f"ğŸ“Š GraphML í™•ì¸: {len(KG_check.nodes)}ê°œ ë…¸ë“œ, {edge_count}ê°œ ì—£ì§€")
        except Exception as e:
            print(f"âš ï¸ GraphML í™•ì¸ ì‹¤íŒ¨: {e}")
    
    # í•„ìˆ˜ FAISS íŒŒì¼ ëª©ë¡
    required_faiss_files = [
        f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_node_faiss.index",
        f"{keyword}_text_faiss.index"
    ]
    
    # ì—£ì§€ê°€ ìˆìœ¼ë©´ ì—£ì§€ ì¸ë±ìŠ¤ë„ í•„ìˆ˜
    edge_faiss_file = f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_edge_faiss.index"
    if has_edges:
        required_faiss_files.append(edge_faiss_file)
        print(f"ğŸ“Š GraphMLì— {edge_count}ê°œ ì—£ì§€ê°€ ìˆìœ¼ë¯€ë¡œ ì—£ì§€ FAISS ì¸ë±ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print(f"âš ï¸ GraphMLì— ì—£ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì—£ì§€ FAISS ì¸ë±ìŠ¤ëŠ” ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # precompute í´ë”ì˜ íŒŒì¼ë“¤ í™•ì¸
    existing_files = []
    missing_files = []
    for file in required_faiss_files:
        file_path = f"{precompute_dir}/{file}"
        if os.path.exists(file_path):
            existing_files.append(file)
        else:
            missing_files.append(file)
    
    print(f"ğŸ“Š FAISS ì¸ë±ìŠ¤ íŒŒì¼ ìƒíƒœ: {len(existing_files)}/{len(required_faiss_files)} ì¡´ì¬")
    if missing_files:
        print(f"âŒ ëˆ„ë½ëœ FAISS ì¸ë±ìŠ¤ íŒŒì¼:")
        for file in missing_files:
            print(f"   - {file}")
    
    # ì—£ì§€ê°€ ìˆëŠ”ë° ì—£ì§€ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
    if has_edges and edge_faiss_file in missing_files:
        print(f"âŒ ì˜¤ë¥˜: GraphMLì— {edge_count}ê°œ ì—£ì§€ê°€ ìˆì§€ë§Œ ì—£ì§€ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        print(f"âŒ ì´ëŠ” ì„ë² ë”© ìƒì„± ë‹¨ê³„ì—ì„œ ì—£ì§€ í•„í„°ë§ ë¬¸ì œê°€ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        print(f"âŒ ë””ë²„ê¹…ì„ ìœ„í•´ 10ë‹¨ê³„(ì„ë² ë”© ìƒì„±)ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ê±°ë‚˜ ì—£ì§€ í•„í„°ë§ ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.")
        return False
    
    if len(existing_files) == len(required_faiss_files):
        print(f"âœ… ëª¨ë“  í•„ìˆ˜ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        print(f"âš ï¸ ì¼ë¶€ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        try:
            from atlas_rag.vectorstore.create_neo4j_index import create_faiss_index
            
            create_faiss_index(
                output_directory=output_directory,
                filename_pattern=keyword,
                index_type="HNSW,Flat",
                faiss_gpu=False
            )
            print("âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    # 13. Neo4j ì„í¬íŠ¸ (í•´ì‹œ ID + conceptì„ ì†ì„±ìœ¼ë¡œ ì €ì¥)
    print("\nğŸ—„ï¸ 13ë‹¨ê³„: Neo4j ì„í¬íŠ¸ (í•´ì‹œ ID + conceptì„ ì†ì„±ìœ¼ë¡œ ì €ì¥)")
    
    try:
        import subprocess
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['LANG'] = 'ko_KR.UTF-8'
        env['LC_ALL'] = 'ko_KR.UTF-8'
        env['NEO4J_DATABASE'] = os.getenv('NEO4J_DATABASE', 'neo4j')
        env['KEYWORD'] = keyword
        
        # API ì„œë²„ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„¤ì •
        script_path = "neo4j_with_hash_ids_and_concept_attributes.py"
        if not os.path.exists(script_path):
            script_path = "BE/neo4j_with_hash_ids_and_concept_attributes.py"
        
        result = subprocess.run([
            sys.executable, script_path, "--keyword", keyword
        ], capture_output=True, text=True, encoding='utf-8', errors='ignore', env=env, check=True, cwd=".")
        print("âœ… Neo4j ì„í¬íŠ¸ ì™„ë£Œ!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Neo4j ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False
    except Exception as e:
        print(f"âŒ Neo4j ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    # 14. GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜
    print("\nğŸ•¸ï¸ 14ë‹¨ê³„: GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜")
    
    try:
        import subprocess
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['LANG'] = 'ko_KR.UTF-8'
        env['LC_ALL'] = 'ko_KR.UTF-8'
        env['NEO4J_DATABASE'] = os.getenv('NEO4J_DATABASE', 'neo4j')
        env['KEYWORD'] = keyword
        
        # API ì„œë²„ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„¤ì •
        script_path = "experiment/create_gds_graph.py"
        if not os.path.exists(script_path):
            script_path = "BE/experiment/create_gds_graph.py"
        
        result = subprocess.run([
            sys.executable, script_path
        ], capture_output=True, text=True, encoding='utf-8', errors='ignore', env=env, check=True, cwd=".")
        print("âœ… GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜ ì™„ë£Œ!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False
    except Exception as e:
        print(f"âŒ GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜ ì‹¤íŒ¨: {e}")
        return False
    
    print("\nğŸ‰ ATLAS ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ë¬¼ ìœ„ì¹˜: {output_directory}")
    print("ğŸ’¡ ì´ì œ conceptì´ ë…¸ë“œ ì†ì„±ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ’¡ 'python experiment/run_questions_v2.py'ë¥¼ ì‹¤í–‰í•´ì„œ í•˜ì´ë¸Œë¦¬ë“œ RAGë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    return True

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì‹œì‘ ë‹¨ê³„ì™€ í‚¤ì›Œë“œ ë°›ê¸°
    start_step = 0
    keyword = None
    
    if len(sys.argv) > 1:
        try:
            start_step = int(sys.argv[1])
            print(f"ğŸ“‹ ì‹œì‘ ë‹¨ê³„: {start_step}")
        except ValueError:
            # ìˆ«ìê°€ ì•„ë‹ˆë©´ í‚¤ì›Œë“œë¡œ ê°„ì£¼
            keyword = sys.argv[1]
            print(f"ğŸ“‹ ì‚¬ìš©í•  í‚¤ì›Œë“œ: {keyword}")
    
    if len(sys.argv) > 2:
        keyword = sys.argv[2]
        print(f"ğŸ“‹ ì‚¬ìš©í•  í‚¤ì›Œë“œ: {keyword}")
    
    success = test_atlas_pipeline(start_step, keyword)
    if success:
        print("\nâœ… ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì¼ë¶€ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
