#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ATLAS ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
READMEì˜ ì˜ˆì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹ê·¸ë˜í”„ êµ¬ì¶•ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

=============================================================================
ì„œë²„ ë°°í¬ ì‹œ ìˆ˜ì • í•„ìš” ì‚¬í•­:
=============================================================================
1. ìƒëŒ€ import â†’ ì ˆëŒ€ import ë³€ê²½ (ì™„ë£Œ)
   - from .atlas_rag... â†’ from atlas_rag...

2. subprocess.run cwd ì„¤ì • ìˆ˜ì • (ì™„ë£Œ)
   - cwd="BE" â†’ cwd="." (BE ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•  ë•Œ)
   - ì„œë²„ì—ì„œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ë¯€ë¡œ cwd="BE"ë¡œ ë˜ëŒë ¤ì•¼ í•¨

3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸
   - .env íŒŒì¼ ê²½ë¡œ: load_dotenv('.env') â†’ load_dotenv('../.env')
   - DATA_DIRECTORY: "example_data" â†’ "BE/example_data"

4. Neo4j ì—°ê²° ì„¤ì •
   - NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE í™•ì¸

5. OpenRouter API ì„¤ì •
   - OPENAI_API_KEY, OPENAI_BASE_URL, OPENAI_MODEL_NAME í™•ì¸
=============================================================================
"""

import os
import subprocess
import sys
import glob
import logging
import io

# Windowsì—ì„œ UTF-8 ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
if sys.platform.startswith('win'):
    # stdoutê³¼ stderrì„ UTF-8ë¡œ ì„¤ì •
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ì„œë²„ ë°°í¬ ì‹œ Python ê²½ë¡œ ì„¤ì • (ëª¨ë“ˆë¡œ importë  ë•Œë„ ì‹¤í–‰)
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from dotenv import load_dotenv
from atlas_rag.kg_construction.triple_extraction import KnowledgeGraphExtractor
from atlas_rag.kg_construction.triple_config import ProcessingConfig
from atlas_rag.llm_generator import LLMGenerator
from openai import OpenAI
from transformers import pipeline

# UTF-8 ë¡œê¹… ì„¤ì •
from atlas_rag.utils.utf8_logging import setup_utf8_logging

# UTF-8 ë¡œê¹… ì´ˆê¸°í™”
setup_utf8_logging()

# OpenAI í´ë¼ì´ì–¸íŠ¸ì˜ ë¡œê¹… ë¹„í™œì„±í™”
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

def check_files_exist(file_patterns, directory):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ íŒ¨í„´ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸: {directory}")
    if not os.path.exists(directory):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
        return False
    
    for pattern in file_patterns:
        full_pattern = os.path.join(directory, pattern)
        matches = glob.glob(full_pattern)
        print(f"ğŸ” íŒ¨í„´ ê²€ìƒ‰: {full_pattern} -> {matches}")
        if not matches:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_pattern}")
            return False
    print(f"âœ… ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {file_patterns}")
    return True

def convert_md_to_json(keyword):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    print("ğŸ“ ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    
    data_directory = os.getenv('DATA_DIRECTORY', 'BE/example_data')
    target_json = f"{data_directory}/{keyword}.json"
    if os.path.exists(target_json):
        print(f"âœ… {keyword}.json íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
    
    try:
        # markdown_to_json ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (md_data ë””ë ‰í† ë¦¬ ì „ì²´ ë³€í™˜)
        # í˜„ì¬ í´ë”ì—ì„œ ì‹¤í–‰í•˜ë¯€ë¡œ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
        relative_data_dir = data_directory
        cmd = [
            sys.executable, "-m", 
            "atlas_rag.kg_construction.utils.md_processing.markdown_to_json",
            "--input", f"{relative_data_dir}/md_data",
            "--output", relative_data_dir
        ]
        
        # í˜„ì¬ í´ë”ì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, cwd=".")
        print("âœ… ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ!")
        print(f"ì¶œë ¥: {result.stdout}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

def test_atlas_pipeline(start_step=1, keyword=None):
    """ATLAS ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    print(f"ğŸš€ ATLAS íŒŒì´í”„ë¼ì¸ ì‹œì‘! (ë‹¨ê³„ {start_step}ë¶€í„°)")
    print(f"ğŸ“ ì „ë‹¬ë°›ì€ keyword: {keyword}")
    print(f"ğŸ“‚ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    # kg_extractor ì´ˆê¸°í™”
    kg_extractor = None
    
    # .env íŒŒì¼ ë¡œë“œ (BE í´ë”ì˜ .env íŒŒì¼ ìš°ì„ )
    # API ì„œë²„ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„¤ì •
    env_path = 'BE/.env'  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ë  ë•Œ
    if not os.path.exists(env_path):
        env_path = '.env'  # BE ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ì‹¤í–‰ë  ë•Œ
    
    print(f"ğŸ” .env íŒŒì¼ ê²½ë¡œ í™•ì¸: {env_path}")
    print(f"ğŸ“„ .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(env_path)}")
    
    try:
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    # keywordê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°
    if keyword is None:
        keyword = os.getenv('KEYWORD', 'contract_v5')
    
    print(f"ğŸ”‘ ì‚¬ìš©í•  keyword: {keyword}")
    
    import_dir = os.getenv('IMPORT_DIRECTORY', 'import')
    output_directory = f'{import_dir}/{keyword}'
    
    print(f"ğŸ“ import_directory: {import_dir}")
    print(f"ğŸ“ output_directory: {output_directory}")
    
    # ì£¼ìš” í™˜ê²½ë³€ìˆ˜ í™•ì¸
    print(f"ğŸ” OPENAI_API_KEY ì¡´ì¬: {'ìˆìŒ' if os.getenv('OPENAI_API_KEY') else 'ì—†ìŒ'}")
    print(f"ğŸŒ OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'ê¸°ë³¸ê°’')}")
    print(f"ğŸ¤– DEFAULT_MODEL: {os.getenv('DEFAULT_MODEL', 'ê¸°ë³¸ê°’')}")
    print(f"ğŸ—„ï¸ NEO4J_URI: {os.getenv('NEO4J_URI', 'ê¸°ë³¸ê°’')}")
    print(f"ğŸ“Š DATA_DIRECTORY: {os.getenv('DATA_DIRECTORY', 'ê¸°ë³¸ê°’')}")
    
    if start_step <= 1:
        # 1. ëª¨ë¸ ì„¤ì •
        print("\nğŸ“‹ 1ë‹¨ê³„: ëª¨ë¸ ì„¤ì •")
        
        try:
            client = OpenAI(
                api_key=os.getenv('OPENAI_API_KEY'),
                base_url=os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
            )
            model_name = os.getenv('DEFAULT_MODEL', 'gpt-4.1-mini')
            triple_generator = LLMGenerator(client=client, model_name=model_name, verbose=False)
            print(f"âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ: {model_name}")
            
        except Exception as e:
            print(f"âŒ OpenAI API ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    if start_step <= 2:
        # 2. ì„¤ì • êµ¬ì„±
        print("\nğŸ“‹ 2ë‹¨ê³„: ì²˜ë¦¬ ì„¤ì • êµ¬ì„±")
        print(f"ğŸ”‘ ì‚¬ìš©í•  ê³„ì•½ì„œ: {keyword}")
    
    if start_step <= 0:
        # 0. ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ (keyword ì‚¬ìš©)
        print("\nğŸ“‹ 0ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜")
        if not convert_md_to_json(keyword):
            print("âŒ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return False
    
    if start_step <= 3:
        # model_name ê°€ì ¸ì˜¤ê¸° (start_step > 1ì¸ ê²½ìš°)
        if start_step > 1:
            model_name = os.getenv('DEFAULT_MODEL', 'gpt-4.1-mini')
        
        kg_extraction_config = ProcessingConfig(
            model_path=model_name,
            data_directory=os.getenv('DATA_DIRECTORY', "BE/example_data"),
            filename_pattern=keyword,
            remove_doc_spaces=True,
            output_directory=output_directory,
        )
        
        print(f"âœ… ì²˜ë¦¬ ì„¤ì • ì™„ë£Œ: {output_directory}")
        
        # 3. ì§€ì‹ê·¸ë˜í”„ ì¶”ì¶œê¸° ìƒì„±
        print("\nğŸ“‹ 3ë‹¨ê³„: ì§€ì‹ê·¸ë˜í”„ ì¶”ì¶œê¸° ìƒì„±")
        kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)
        print("âœ… ì§€ì‹ê·¸ë˜í”„ ì¶”ì¶œê¸° ìƒì„± ì™„ë£Œ")
    
    if start_step <= 4:
        # 4. íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì‹¤í–‰
        print("\nğŸš€ 4ë‹¨ê³„: íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì‹¤í–‰")
        triple_files = [
            f"{keyword}_kg_extraction.json",
            f"{keyword}_kg_extraction_processed.json"
        ]
        
        if check_files_exist(triple_files, f"{output_directory}/kg_extraction"):
            print("âœ… íŠ¸ë¦¬í”Œ ì¶”ì¶œ íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            try:
                kg_extractor.run_extraction()
                print("âœ… íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì™„ë£Œ!")
            except Exception as e:
                print(f"âŒ íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                return False
    
    # kg_extractorê°€ í•„ìš”í•œ ê²½ìš° ìƒì„±
    if kg_extractor is None:
        print("ğŸ”„ kg_extractor ìƒì„± ì¤‘...")
        # ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ kg_extractor ìƒì„±
        kg_extraction_config = ProcessingConfig(
            model_path="",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì • (LLM ì‚¬ìš© ì‹œ)
            data_directory=os.getenv('DATA_DIRECTORY', "BE/example_data"),
            filename_pattern=keyword,
            remove_doc_spaces=True,
            output_directory=output_directory,
        )
        kg_extractor = KnowledgeGraphExtractor(model=None, config=kg_extraction_config)
        print("âœ… kg_extractor ìƒì„± ì™„ë£Œ")
    
    # 5. JSONì„ CSVë¡œ ë³€í™˜
    print("\nğŸ”„ 5ë‹¨ê³„: JSONì„ CSVë¡œ ë³€í™˜")
    
    # CSV íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    csv_files = [
        f"{keyword}_triples.csv",
        f"{keyword}_entities.csv",
        f"{keyword}_relations.csv"
    ]
    if check_files_exist(csv_files, output_directory):
        print("âœ… CSV íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 5ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.convert_json_to_csv()
            print("âœ… CSV ë³€í™˜ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    # 6. ê°œë… ìƒì„±
    print("\nğŸ§  6ë‹¨ê³„: ê°œë… ìƒì„±")
    concept_files = [f"concept_shard_0.csv"]
    if check_files_exist(concept_files, f"{output_directory}/concepts"):
        print("âœ… ê°œë… ìƒì„± íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 6ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.generate_concept_csv_temp()
            print("âœ… ê°œë… ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ê°œë… ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    # 7. ê°œë… CSV ìƒì„±
    print("\nğŸ“Š 7ë‹¨ê³„: ê°œë… CSV ìƒì„±")
    
    # ê°œë… CSV íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    concept_csv_files = [
        f"concept_nodes_{keyword}_from_json_with_concept.csv",
        f"concept_edges_{keyword}_from_json_with_concept.csv"
    ]
    if check_files_exist(concept_csv_files, f"{output_directory}/concept_csv"):
        print("âœ… ê°œë… CSV íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 7ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.create_concept_csv()
            print("âœ… ê°œë… CSV ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ê°œë… CSV ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    # 8. GraphML ìƒì„±
    print("\nğŸ•¸ï¸ 8ë‹¨ê³„: GraphML ìƒì„±")
    
    # GraphML íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    graphml_files = [f"{keyword}_graph.graphml"]
    if check_files_exist(graphml_files, f"{output_directory}/kg_graphml"):
        print("âœ… GraphML íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 8ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.convert_to_graphml()
            print("âœ… GraphML ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ GraphML ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    # 9. ìˆ«ì ID ì¶”ê°€
    print("\nğŸ”¢ 9ë‹¨ê³„: ìˆ«ì ID ì¶”ê°€")
    
    # ìˆ«ì ID íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    numeric_id_files = [
        f"triple_nodes_{keyword}_from_json_without_emb_with_numeric_id.csv",
        f"triple_edges_{keyword}_from_json_without_emb_with_numeric_id.csv",
        f"text_nodes_{keyword}_from_json_with_numeric_id.csv"
    ]
    if check_files_exist(numeric_id_files, f"{output_directory}/triples_csv"):
        print("âœ… ìˆ«ì ID íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            if kg_extractor is None:
                print("âŒ kg_extractorê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 9ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ë ¤ë©´ ì´ì „ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            kg_extractor.add_numeric_id()
            print("âœ… ìˆ«ì ID ì¶”ê°€ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ìˆ«ì ID ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    # GraphML íŒŒì¼ ë³µì‚¬ (ì„ë² ë”© ìƒì„±ìš©)
    import shutil
    source_graphml = f"{output_directory}/kg_graphml/{keyword}_graph.graphml"
    target_graphml = f"{output_directory}/kg_graphml/{keyword}_graph_with_numeric_id.graphml"
    if os.path.exists(source_graphml) and not os.path.exists(target_graphml):
        shutil.copy2(source_graphml, target_graphml)
        print("âœ… GraphML íŒŒì¼ ë³µì‚¬ ì™„ë£Œ!")

    # 10. ì„ë² ë”© ìƒì„±
    print("\nğŸ§® 10ë‹¨ê³„: ì„ë² ë”© ìƒì„±")
    
    # ì„ë² ë”© íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    embedding_files = [
        f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_node_faiss.index",
        f"{keyword}_eventTrue_conceptTrue_node_list.pkl",
        f"{keyword}_text_faiss.index"
    ]
    if check_files_exist(embedding_files, f"{output_directory}/precompute"):
        print("âœ… ì„ë² ë”© íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            from sentence_transformers import SentenceTransformer
            from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
            from atlas_rag.vectorstore.create_graph_index import create_embeddings_and_index
            
            # Sentence Transformer ëª¨ë¸ ë¡œë“œ
            encoder_model_name = os.getenv('DEFAULT_EMBEDDING_MODEL', "sentence-transformers/all-MiniLM-L6-v2")
            print(f"ğŸ”„ {encoder_model_name} ëª¨ë¸ì„ ë¡œë”© ì¤‘...")
            
            sentence_model = SentenceTransformer(
                encoder_model_name, 
                trust_remote_code=True, 
                model_kwargs={'device_map': "auto"}
            )
            sentence_encoder = SentenceEmbedding(sentence_model)
            
            # create_embeddings_and_index ì‚¬ìš©
            print("ğŸ”„ create_embeddings_and_index ì‹¤í–‰ ì¤‘...")
            create_embeddings_and_index(
                sentence_encoder=sentence_encoder,
                model_name=encoder_model_name,
                working_directory=output_directory,
                keyword=keyword,
                include_events=True,
                include_concept=True,
                normalize_embeddings=True,
                text_batch_size=40,
                node_and_edge_batch_size=256
            )
            print("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    
    # 11. ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ ìƒì„±
    print("\nğŸ” 11ë‹¨ê³„: ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ ìƒì„±")
    emb_csv_files = [
        f"triples_csv/triple_nodes_{keyword}_from_json_with_emb.csv",
        f"triples_csv/text_nodes_{keyword}_from_json_with_emb.csv",
        f"triples_csv/triple_edges_{keyword}_from_json_with_concept_with_emb.csv"
    ]
    if check_files_exist(emb_csv_files, output_directory):
        print("âœ… ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            from sentence_transformers import SentenceTransformer
            from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
            
            # Sentence Transformer ëª¨ë¸ ë¡œë“œ
            encoder_model_name = os.getenv('DEFAULT_EMBEDDING_MODEL', "sentence-transformers/all-MiniLM-L6-v2")
            print(f"ğŸ”„ {encoder_model_name} ëª¨ë¸ì„ ë¡œë”© ì¤‘...")
            
            sentence_model = SentenceTransformer(
                encoder_model_name, 
                trust_remote_code=True, 
                model_kwargs={'device_map': "auto"}
            )
            sentence_encoder = SentenceEmbedding(sentence_model)
            
            # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
            node_csv_without_emb = f"{output_directory}/triples_csv/triple_nodes_{keyword}_from_json_without_emb.csv"
            node_csv_file = f"{output_directory}/triples_csv/triple_nodes_{keyword}_from_json_with_emb.csv"
            edge_csv_without_emb = f"{output_directory}/concept_csv/triple_edges_{keyword}_from_json_with_concept.csv"
            edge_csv_file = f"{output_directory}/triples_csv/triple_edges_{keyword}_from_json_with_concept_with_emb.csv"
            text_node_csv_without_emb = f"{output_directory}/triples_csv/text_nodes_{keyword}_from_json.csv"
            text_node_csv = f"{output_directory}/triples_csv/text_nodes_{keyword}_from_json_with_emb.csv"
            
            # ì„ë² ë”©ì„ CSV íŒŒì¼ì— ì¶”ê°€
            sentence_encoder.compute_kg_embedding(
                node_csv_without_emb=node_csv_without_emb,
                node_csv_file=node_csv_file,
                edge_csv_without_emb=edge_csv_without_emb,
                edge_csv_file=edge_csv_file,
                text_node_csv_without_emb=text_node_csv_without_emb,
                text_node_csv=text_node_csv,
                batch_size=2048
            )
            print("âœ… ì„ë² ë”©ì´ í¬í•¨ëœ CSV íŒŒì¼ ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    # 12. FAISS ì¸ë±ìŠ¤ ìƒì„±
    print("\nğŸ” 12ë‹¨ê³„: FAISS ì¸ë±ìŠ¤ ìƒì„±")
    # precompute í´ë”ì—ì„œ FAISS ì¸ë±ìŠ¤ íŒŒì¼ í™•ì¸
    precompute_dir = f"{output_directory}/precompute"
    faiss_files = [
        f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_node_faiss.index",
        f"{keyword}_eventTrue_conceptTrue_all-MiniLM-L6-v2_edge_faiss.index",
        f"{keyword}_text_faiss.index"
    ]
    
    # precompute í´ë”ì˜ íŒŒì¼ë“¤ í™•ì¸
    existing_files = []
    for file in faiss_files:
        if os.path.exists(f"{precompute_dir}/{file}"):
            existing_files.append(file)
    
    if len(existing_files) == len(faiss_files):
        print("âœ… FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        try:
            from atlas_rag.vectorstore.create_neo4j_index import create_faiss_index
            
            create_faiss_index(
                output_directory=output_directory,
                filename_pattern=keyword,
                index_type="HNSW,Flat",
                faiss_gpu=False
            )
            print("âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    # 13. Neo4j ì„í¬íŠ¸ (í•´ì‹œ ID + conceptì„ ì†ì„±ìœ¼ë¡œ ì €ì¥)
    print("\nğŸ—„ï¸ 13ë‹¨ê³„: Neo4j ì„í¬íŠ¸ (í•´ì‹œ ID + conceptì„ ì†ì„±ìœ¼ë¡œ ì €ì¥)")
    
    try:
        import subprocess
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['LANG'] = 'ko_KR.UTF-8'
        env['LC_ALL'] = 'ko_KR.UTF-8'
        env['NEO4J_DATABASE'] = os.getenv('NEO4J_DATABASE', 'neo4j')
        env['KEYWORD'] = keyword
        
        # API ì„œë²„ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„¤ì •
        script_path = "neo4j_with_hash_ids_and_concept_attributes.py"
        if not os.path.exists(script_path):
            script_path = "BE/neo4j_with_hash_ids_and_concept_attributes.py"
        
        result = subprocess.run([
            sys.executable, script_path, "--keyword", keyword
        ], capture_output=True, text=True, encoding='utf-8', errors='ignore', env=env, check=True, cwd=".")
        print("âœ… Neo4j ì„í¬íŠ¸ ì™„ë£Œ!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Neo4j ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False
    except Exception as e:
        print(f"âŒ Neo4j ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    # 14. GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜
    print("\nğŸ•¸ï¸ 14ë‹¨ê³„: GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜")
    
    try:
        import subprocess
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['LANG'] = 'ko_KR.UTF-8'
        env['LC_ALL'] = 'ko_KR.UTF-8'
        env['NEO4J_DATABASE'] = os.getenv('NEO4J_DATABASE', 'neo4j')
        env['KEYWORD'] = keyword
        
        # API ì„œë²„ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„¤ì •
        script_path = "experiment/create_gds_graph.py"
        if not os.path.exists(script_path):
            script_path = "BE/experiment/create_gds_graph.py"
        
        result = subprocess.run([
            sys.executable, script_path
        ], capture_output=True, text=True, encoding='utf-8', errors='ignore', env=env, check=True, cwd=".")
        print("âœ… GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜ ì™„ë£Œ!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False
    except Exception as e:
        print(f"âŒ GDS ê·¸ë˜í”„ í”„ë¡œì ì…˜ ì‹¤íŒ¨: {e}")
        return False
    
    print("\nğŸ‰ ATLAS ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ë¬¼ ìœ„ì¹˜: {output_directory}")
    print("ğŸ’¡ ì´ì œ conceptì´ ë…¸ë“œ ì†ì„±ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ’¡ 'python experiment/run_questions_v2.py'ë¥¼ ì‹¤í–‰í•´ì„œ í•˜ì´ë¸Œë¦¬ë“œ RAGë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    return True

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì‹œì‘ ë‹¨ê³„ì™€ í‚¤ì›Œë“œ ë°›ê¸°
    start_step = 0
    keyword = None
    
    if len(sys.argv) > 1:
        try:
            start_step = int(sys.argv[1])
            print(f"ğŸ“‹ ì‹œì‘ ë‹¨ê³„: {start_step}")
        except ValueError:
            # ìˆ«ìê°€ ì•„ë‹ˆë©´ í‚¤ì›Œë“œë¡œ ê°„ì£¼
            keyword = sys.argv[1]
            print(f"ğŸ“‹ ì‚¬ìš©í•  í‚¤ì›Œë“œ: {keyword}")
    
    if len(sys.argv) > 2:
        keyword = sys.argv[2]
        print(f"ğŸ“‹ ì‚¬ìš©í•  í‚¤ì›Œë“œ: {keyword}")
    
    success = test_atlas_pipeline(start_step, keyword)
    if success:
        print("\nâœ… ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì¼ë¶€ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
