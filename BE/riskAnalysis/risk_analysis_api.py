"""
ìœ„í—˜ ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
ì§ë ¬ ì²˜ë¦¬ ë° ì ì§„ì  ë¶„ì„ì„ ìœ„í•œ REST API
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import json
import asyncio
from datetime import datetime
import logging

# from .part_risk_analyzer import SequentialRiskAnalyzer, PartRiskAnalyzer
# from atlas_rag.retriever.vector_retriever import VectorRetriever
# from atlas_rag.llm_generator.llm_generator import LLMGenerator

# API ë¼ìš°í„° ì„¤ì • (prefixëŠ” api.pyì—ì„œ ì„¤ì •ë¨)
router = APIRouter(tags=["risk-analysis"])

# ì „ì—­ ë³€ìˆ˜ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” Redis ë“± ì‚¬ìš©)
analysis_sessions = {}
risk_check_data = None

class AnalysisRequest(BaseModel):
    contract_id: str
    contract_text: str
    contract_name: Optional[str] = "ê³„ì•½ì„œ"
    selected_parts: Optional[List[int]] = None  # íŠ¹ì • íŒŒíŠ¸ë§Œ ë¶„ì„í•  ê²½ìš°

class AnalysisResponse(BaseModel):
    analysis_id: str
    status: str
    message: str
    estimated_time: Optional[int] = None

class PartAnalysisResponse(BaseModel):
    part_number: int
    part_title: str
    risk_score: float
    risk_level: str
    checklist_results: List[Dict[str, Any]]
    relevant_clauses: List[str]
    recommendations: List[str]
    analysis_time: float

class FullAnalysisResponse(BaseModel):
    contract_name: str
    analysis_date: str
    total_analysis_time: float
    overall_risk_score: float
    overall_risk_level: str
    part_results: List[PartAnalysisResponse]
    summary: Dict[str, Any]

def load_risk_check_data():
    """ìœ„í—˜ ì²´í¬ ë°ì´í„° ë¡œë“œ"""
    global risk_check_data
    try:
        with open("riskAnalysis/checkList/riskCheck.json", "r", encoding="utf-8") as f:
            risk_check_data = json.load(f)
        logging.info("Risk check data loaded successfully")
    except Exception as e:
        logging.error(f"Failed to load risk check data: {e}")
        raise HTTPException(status_code=500, detail="ìœ„í—˜ ì²´í¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")

@router.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ìœ„í—˜ ì²´í¬ ë°ì´í„° ë¡œë“œ"""
    load_risk_check_data()

@router.post("/start", response_model=AnalysisResponse)
async def start_risk_analysis(request: AnalysisRequest, background_tasks: BackgroundTasks):
    """ìœ„í—˜ ë¶„ì„ ì‹œì‘"""
    if not risk_check_data:
        raise HTTPException(status_code=500, detail="ìœ„í—˜ ì²´í¬ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    # ë¶„ì„ ì„¸ì…˜ ID ìƒì„±
    analysis_id = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{request.contract_id}"
    
    # ë¶„ì„í•  íŒŒíŠ¸ ê²°ì •
    if request.selected_parts:
        parts_to_analyze = request.selected_parts
    else:
        parts_to_analyze = [part["partNumber"] for part in risk_check_data["analysisParts"]]
    
    # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚° (íŒŒíŠ¸ë‹¹ í‰ê·  30ì´ˆ)
    estimated_time = len(parts_to_analyze) * 30
    
    # ë¶„ì„ ì„¸ì…˜ ì´ˆê¸°í™”
    analysis_sessions[analysis_id] = {
        "status": "STARTING",
        "contract_id": request.contract_id,
        "contract_text": request.contract_text,
        "contract_name": request.contract_name,
        "selected_parts": parts_to_analyze,
        "start_time": datetime.now(),
        "results": {},
        "current_part": 0,
        "total_parts": len(parts_to_analyze)
    }
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹œì‘
    background_tasks.add_task(
        run_sequential_analysis, 
        analysis_id, 
        request.contract_text, 
        request.contract_name,
        parts_to_analyze
    )
    
    return AnalysisResponse(
        analysis_id=analysis_id,
        status="STARTED",
        message="ìœ„í—˜ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        estimated_time=estimated_time
    )

@router.get("/{analysis_id}/status")
async def get_analysis_status(analysis_id: str):
    """ë¶„ì„ ìƒíƒœ ì¡°íšŒ"""
    if analysis_id not in analysis_sessions:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    session = analysis_sessions[analysis_id]
    
    return {
        "analysis_id": analysis_id,
        "status": session["status"],
        "progress": {
            "current_part": session["current_part"],
            "total_parts": session["total_parts"],
            "percentage": (session["current_part"] / session["total_parts"]) * 100
        },
        "elapsed_time": (datetime.now() - session["start_time"]).total_seconds(),
        "current_part_title": session.get("current_part_title", "")
    }

@router.get("/{analysis_id}/part/{part_number}", response_model=PartAnalysisResponse)
async def get_part_analysis(analysis_id: str, part_number: int):
    """íŠ¹ì • íŒŒíŠ¸ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    if analysis_id not in analysis_sessions:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    session = analysis_sessions[analysis_id]
    
    if part_number not in session["results"]:
        raise HTTPException(status_code=404, detail=f"Part {part_number} ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    result = session["results"][part_number]
    
    return PartAnalysisResponse(
        part_number=result["part_number"],
        part_title=result["part_title"],
        risk_score=result["risk_score"],
        risk_level=result["risk_level"],
        checklist_results=result["checklist_results"],
        relevant_clauses=result.get("relevant_clauses", []),
        recommendations=result["recommendations"],
        analysis_time=result["analysis_time"]
    )

@router.get("/{analysis_id}/report", response_model=FullAnalysisResponse)
async def get_full_analysis_report(analysis_id: str):
    """ì „ì²´ ë¶„ì„ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
    if analysis_id not in analysis_sessions:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    session = analysis_sessions[analysis_id]
    
    if session["status"] != "COMPLETED":
        raise HTTPException(status_code=400, detail="ë¶„ì„ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ê²°ê³¼ êµ¬ì„±
    part_results = []
    total_risk_score = 0.0
    
    for part_number, result in session["results"].items():
        part_results.append(PartAnalysisResponse(
            part_number=result["part_number"],
            part_title=result["part_title"],
            risk_score=result["risk_score"],
            risk_level=result["risk_level"],
            checklist_results=result["checklist_results"],
            relevant_clauses=result.get("relevant_clauses", []),
            recommendations=result["recommendations"],
            analysis_time=result["analysis_time"]
        ))
        total_risk_score += result["risk_score"]
    
    overall_risk_score = total_risk_score / len(part_results) if part_results else 0.0
    overall_risk_level = determine_risk_level(overall_risk_score)
    
    # ìš”ì•½ ì •ë³´ ìƒì„±
    summary = {
        "total_parts_analyzed": len(part_results),
        "overall_risk_score": overall_risk_score,
        "overall_risk_level": overall_risk_level,
        "high_risk_parts": [r.part_title for r in part_results if r.risk_level in ["HIGH", "CRITICAL"]],
        "total_analysis_time": session.get("total_analysis_time", 0)
    }
    
    return FullAnalysisResponse(
        contract_name=session["contract_name"],
        analysis_date=session["start_time"].isoformat(),
        total_analysis_time=session.get("total_analysis_time", 0),
        overall_risk_score=overall_risk_score,
        overall_risk_level=overall_risk_level,
        part_results=part_results,
        summary=summary
    )

@router.delete("/{analysis_id}")
async def delete_analysis_session(analysis_id: str):
    """ë¶„ì„ ì„¸ì…˜ ì‚­ì œ"""
    if analysis_id not in analysis_sessions:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    del analysis_sessions[analysis_id]
    return {"message": "ë¶„ì„ ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}

async def run_sequential_analysis(analysis_id: str, contract_text: str, contract_name: str, parts_to_analyze: List[int]):
    """ì§ë ¬ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰"""
    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê¸°ì¡´ ì„œë²„ì˜ RAG ì‹œìŠ¤í…œ ì‚¬ìš©)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì „ì—­ RAG ì‹œìŠ¤í…œ ì‚¬ìš©
        from server import rag_system, neo4j_driver
        
        if not rag_system:
            raise Exception("RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìˆœì°¨ ë¶„ì„ê¸° ì´ˆê¸°í™”
        from .hybrid_risk_analyzer import HybridSequentialRiskAnalyzer
        
        analyzer = HybridSequentialRiskAnalyzer(
            risk_check_data, 
            rag_system["enhanced_lkg_retriever"], 
            rag_system["hippo_retriever"],
            rag_system["llm_generator"],
            neo4j_driver
        )
        
        # ë¶„ì„ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        analysis_sessions[analysis_id]["status"] = "RUNNING"
        
        # íŒŒíŠ¸ë³„ ìˆœì°¨ ë¶„ì„
        for i, part_number in enumerate(parts_to_analyze):
            try:
                # í˜„ì¬ íŒŒíŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸
                analysis_sessions[analysis_id]["current_part"] = i + 1
                part_data = next(p for p in risk_check_data["analysisParts"] if p["partNumber"] == part_number)
                analysis_sessions[analysis_id]["current_part_title"] = part_data["partTitle"]
                
                logging.info(f"Part {part_number} ë¶„ì„ ì‹œì‘: {part_data['partTitle']}")
                
                # íŒŒíŠ¸ë³„ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ìˆ˜í–‰
                from .hybrid_risk_analyzer import HybridRiskAnalyzer
                
                part_analyzer = HybridRiskAnalyzer(
                    risk_check_data, 
                    rag_system["enhanced_lkg_retriever"], 
                    rag_system["hippo_retriever"],
                    rag_system["llm_generator"],
                    neo4j_driver
                )
                result = await part_analyzer.analyze_part_with_hybrid_retrieval(part_number, contract_text)
                
                # í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥
                analysis_sessions[analysis_id]["results"][part_number] = {
                    "part_number": result.part_number,
                    "part_title": result.part_title,
                    "risk_score": result.risk_score,
                    "risk_level": result.risk_level,
                    "checklist_results": result.checklist_results,
                    "relevant_clauses": result.relevant_clauses,
                    "hybrid_search_results": result.hybrid_search_results,
                    "recommendations": result.recommendations,
                    "analysis_time": result.analysis_time
                }
                
                logging.info(f"Part {part_number} ë¶„ì„ ì™„ë£Œ - ìœ„í—˜ë„: {result.risk_level}")
                
            except Exception as e:
                logging.error(f"Part {part_number} ë¶„ì„ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ íŒŒíŠ¸ì— ëŒ€í•œ ê¸°ë³¸ ê²°ê³¼
                analysis_sessions[analysis_id]["results"][part_number] = {
                    "part_number": part_number,
                    "part_title": f"Part {part_number}",
                    "risk_score": 0.0,
                    "risk_level": "ERROR",
                    "checklist_results": [],
                    "relevant_clauses": [],
                    "hybrid_search_results": {"error": str(e)},
                    "recommendations": [f"í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"],
                    "analysis_time": 0.0
                }
        
        # ë¶„ì„ ì™„ë£Œ
        analysis_sessions[analysis_id]["status"] = "COMPLETED"
        analysis_sessions[analysis_id]["total_analysis_time"] = (datetime.now() - analysis_sessions[analysis_id]["start_time"]).total_seconds()
        
        logging.info(f"Analysis {analysis_id} completed successfully")
        
    except Exception as e:
        logging.error(f"Analysis {analysis_id} failed: {e}")
        analysis_sessions[analysis_id]["status"] = "FAILED"
        analysis_sessions[analysis_id]["error"] = str(e)

def determine_risk_level(risk_score: float) -> str:
    """ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •"""
    if risk_score >= 4.0:
        return "CRITICAL"
    elif risk_score >= 3.0:
        return "HIGH"
    elif risk_score >= 2.0:
        return "MEDIUM"
    else:
        return "LOW"


# ============================================
# í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
# ============================================

from .data_persistence import data_manager

@router.get("/rag-contracts")
async def get_rag_contracts():
    """RAG êµ¬ì¶•ëœ ê³„ì•½ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        from app.services.file_service import file_service
        files = file_service.list_files()
        
        return {
            "success": True,
            "data": [
                {
                    "file_id": f.file_id,
                    "filename": f.filename,
                    "uploaded_at": f.upload_time,
                    "file_size": f.file_size,
                    "file_type": "markdown" if f.filename.endswith(".md") else "text"
                }
                for f in files
            ],
            "total_count": len(files)
        }
    except Exception as e:
        logging.error(f"RAG contracts ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": True, "data": [], "total_count": 0}


@router.get("/saved")
async def get_saved_risk_analysis():
    """ì €ì¥ëœ ìœ„í—˜ ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        results = data_manager.get_analysis_list()
        return {
            "success": True,
            "data": {
                "results": results
            }
        }
    except Exception as e:
        logging.error(f"ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": True, "data": {"results": []}}


@router.get("/saved/{file_id}")
async def get_saved_risk_analysis_by_file(file_id: str):
    """íŠ¹ì • íŒŒì¼ì˜ ì €ì¥ëœ ìœ„í—˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        result = data_manager.load_analysis_result(file_id)
        if result:
            return {"success": True, "data": result}
        return {"success": False, "data": None, "message": "ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"íŒŒì¼ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": False, "data": None, "message": str(e)}


@router.get("/gpt-results")
async def get_gpt_analysis_results():
    """GPT ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        # GPT ë¶„ì„ ê²°ê³¼ëŠ” data_managerì—ì„œ ê°€ì ¸ì˜´ (gpt_ ì ‘ë‘ì‚¬ë¡œ í•„í„°ë§)
        all_results = data_manager.load_all_results()
        gpt_results = [r for k, r in all_results.items() if k.startswith("gpt_")]
        
        return {
            "success": True,
            "data": {
                "results": gpt_results
            }
        }
    except Exception as e:
        logging.error(f"GPT ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": True, "data": {"results": []}}


class GPTOnlyAnalysisRequest(BaseModel):
    file_id: str


@router.post("/analyze-gpt-only")
async def analyze_gpt_only(request: GPTOnlyAnalysisRequest):
    """GPT ì „ìš© ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ (RAG ì‹œìŠ¤í…œ ì—†ì´ OpenAI GPTë§Œ ì‚¬ìš©)"""
    try:
        from app.services.file_service import file_service
        from .simple_gpt_risk_analyzer import SimpleGPTRiskAnalyzer
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        file_info = file_service.get_file_info(request.file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        contract_text = file_service.get_file_content(request.file_id)
        contract_name = file_info.get("filename", "ê³„ì•½ì„œ")
        
        logging.info(f"GPT ì „ìš© ë¶„ì„ ì‹œì‘: {contract_name} (file_id: {request.file_id})")
        
        # GPT ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ë¶„ì„ ì‹¤í–‰
        analyzer = SimpleGPTRiskAnalyzer()
        analysis_result = analyzer.analyze_contract(contract_text, contract_name)
        
        # ë¶„ì„ ID ìƒì„± (gpt_ ì ‘ë‘ì‚¬ ì‚¬ìš©)
        analysis_id = f"gpt_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{request.file_id}"
        
        # ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result = {
            "analysis_id": analysis_id,
            "file_id": request.file_id,
            "contract_name": contract_name,
            "created_at": datetime.now().isoformat(),
            "analysis_type": "gpt_only",
            "analysis_result": {
                "overall_risk_score": 0.0,  # GPT ë¶„ì„ì€ ì ìˆ˜ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
                "part_results": [],  # GPT ë¶„ì„ì€ íŒŒíŠ¸ë³„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                "gpt_analysis": analysis_result.get("analysis_result", ""),
                "model_used": analysis_result.get("model_used", ""),
                "analysis_time": analysis_result.get("analysis_time", 0)
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        data_manager.save_analysis_result(analysis_id, result)
        
        logging.info(f"GPT ì „ìš© ë¶„ì„ ì™„ë£Œ: {analysis_id}")
        
        return {
            "success": True,
            "data": result
        }
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"GPT ì „ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


class UploadedFileAnalysisRequest(BaseModel):
    file_id: str
    selected_parts: Optional[str] = "all"


@router.post("/analyze-uploaded-file")
async def analyze_uploaded_file_risk(request: UploadedFileAnalysisRequest, background_tasks: BackgroundTasks):
    """ì—…ë¡œë“œëœ íŒŒì¼ì˜ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©)"""
    try:
        from app.services.file_service import file_service
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        file_info = file_service.get_file_info(request.file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        contract_text = file_service.get_file_content(request.file_id)
        contract_name = file_info.get("filename", "ê³„ì•½ì„œ")
        
        # ë¶„ì„ ID ìƒì„±
        analysis_id = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{request.file_id}"
        
        # ë¶„ì„í•  íŒŒíŠ¸ ê²°ì •
        if request.selected_parts == "all":
            parts_to_analyze = [part["partNumber"] for part in risk_check_data["analysisParts"]] if risk_check_data else []
        else:
            parts_to_analyze = [int(p) for p in request.selected_parts.split(",")]
        
        # ë¶„ì„ ì„¸ì…˜ ì´ˆê¸°í™”
        analysis_sessions[analysis_id] = {
            "status": "STARTING",
            "contract_id": request.file_id,
            "contract_text": contract_text,
            "contract_name": contract_name,
            "selected_parts": parts_to_analyze,
            "start_time": datetime.now(),
            "results": {},
            "current_part": 0,
            "total_parts": len(parts_to_analyze)
        }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
        background_tasks.add_task(
            run_uploaded_file_analysis,
            analysis_id,
            request.file_id,
            contract_text,
            contract_name,
            parts_to_analyze
        )
        
        return {
            "success": True,
            "message": "ìœ„í—˜ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "analysis_id": analysis_id,
                "analysis_result": {
                    "analysis_id": analysis_id,
                    "contract_name": contract_name,
                    "status": "RUNNING"
                }
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"ìœ„í—˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def run_uploaded_file_analysis(analysis_id: str, file_id: str, contract_text: str, contract_name: str, parts_to_analyze: List[int]):
    """ì—…ë¡œë“œëœ íŒŒì¼ì˜ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)"""
    try:
        from .hybrid_risk_analyzer import HybridRiskAnalyzer
        
        analysis_sessions[analysis_id]["status"] = "RUNNING"
        start_time = datetime.now()
        
        # RAG ì‹œìŠ¤í…œ í™•ì¸
        rag_system = None
        neo4j_driver = None
        
        try:
            from experiment.run_questions_v3_with_concept import load_enhanced_rag_system
            
            # í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ì„œ RAG ì‹œìŠ¤í…œ ë¡œë“œ
            enhanced_lkg_retriever, hippo_retriever, llm_generator, driver = load_enhanced_rag_system()
            
            if enhanced_lkg_retriever and llm_generator:
                rag_system = {
                    "enhanced_lkg_retriever": enhanced_lkg_retriever,
                    "hippo_retriever": hippo_retriever,
                    "llm_generator": llm_generator
                }
                neo4j_driver = driver
                logging.info("âœ… RAG ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ")
            else:
                logging.warning("âš ï¸ RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ê°€ Noneì…ë‹ˆë‹¤")
        except Exception as e:
            logging.warning(f"RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰: {e}")
        
        part_results = []
        total_risk_score = 0.0
        
        for i, part_number in enumerate(parts_to_analyze):
            try:
                analysis_sessions[analysis_id]["current_part"] = i + 1
                
                if risk_check_data:
                    part_data = next((p for p in risk_check_data["analysisParts"] if p["partNumber"] == part_number), None)
                    if part_data:
                        analysis_sessions[analysis_id]["current_part_title"] = part_data["partTitle"]
                
                # RAG ì‹œìŠ¤í…œì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ìˆ˜í–‰
                if rag_system and neo4j_driver and risk_check_data:
                    analyzer = HybridRiskAnalyzer(
                        risk_check_data,
                        rag_system["enhanced_lkg_retriever"],
                        rag_system.get("hippo_retriever"),
                        rag_system["llm_generator"],
                        neo4j_driver
                    )
                    result = await analyzer.analyze_part_with_hybrid_retrieval(part_number, contract_text)
                    
                    part_result = {
                        "part_number": result.part_number,
                        "part_title": result.part_title,
                        "risk_score": result.risk_score,
                        "risk_level": result.risk_level,
                        "checklist_results": result.checklist_results,
                        "relevant_clauses": result.relevant_clauses,
                        "risk_clauses": result.risk_clauses,
                        "recommendations": result.recommendations,
                        "analysis_time": result.analysis_time
                    }
                else:
                    # RAG ì‹œìŠ¤í…œ ì—†ì´ ê¸°ë³¸ ë¶„ì„
                    part_result = await _basic_risk_analysis(part_number, contract_text, risk_check_data)
                
                part_results.append(part_result)
                total_risk_score += part_result["risk_score"]
                analysis_sessions[analysis_id]["results"][part_number] = part_result
                
            except Exception as e:
                logging.error(f"Part {part_number} ë¶„ì„ ì‹¤íŒ¨: {e}")
                part_results.append({
                    "part_number": part_number,
                    "part_title": f"Part {part_number}",
                    "risk_score": 0.0,
                    "risk_level": "ERROR",
                    "checklist_results": [],
                    "relevant_clauses": [],
                    "risk_clauses": [],
                    "recommendations": [f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"],
                    "analysis_time": 0.0
                })
        
        # ì „ì²´ ê²°ê³¼ ê³„ì‚°
        overall_risk_score = total_risk_score / len(part_results) if part_results else 0.0
        overall_risk_level = determine_risk_level(overall_risk_score)
        total_analysis_time = (datetime.now() - start_time).total_seconds()
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            "analysis_id": analysis_id,
            "contract_name": contract_name,
            "created_at": start_time.isoformat(),
            "analysis_type": "hybrid" if rag_system else "basic",
            "analysis_result": {
                "overall_risk_score": overall_risk_score,
                "overall_risk_level": overall_risk_level,
                "part_results": part_results,
                "total_analysis_time": total_analysis_time,
                "summary": {
                    "total_parts_analyzed": len(part_results),
                    "high_risk_parts": len([p for p in part_results if p["risk_level"] in ["HIGH", "CRITICAL"]]),
                    "critical_issues": [p["part_title"] for p in part_results if p["risk_level"] == "CRITICAL"]
                }
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        data_manager.save_analysis_result(file_id, final_result)
        
        analysis_sessions[analysis_id]["status"] = "COMPLETED"
        analysis_sessions[analysis_id]["total_analysis_time"] = total_analysis_time
        analysis_sessions[analysis_id]["final_result"] = final_result
        
        logging.info(f"Analysis {analysis_id} completed: {overall_risk_level} ({overall_risk_score:.2f})")
        
    except Exception as e:
        logging.error(f"Analysis {analysis_id} failed: {e}")
        analysis_sessions[analysis_id]["status"] = "FAILED"
        analysis_sessions[analysis_id]["error"] = str(e)


async def _basic_risk_analysis(part_number: int, contract_text: str, risk_check_data: Dict) -> Dict:
    """RAG ì—†ì´ ê¸°ë³¸ ìœ„í—˜ ë¶„ì„ ìˆ˜í–‰"""
    import re
    
    part_data = None
    if risk_check_data:
        part_data = next((p for p in risk_check_data["analysisParts"] if p["partNumber"] == part_number), None)
    
    if not part_data:
        return {
            "part_number": part_number,
            "part_title": f"Part {part_number}",
            "risk_score": 0.0,
            "risk_level": "UNKNOWN",
            "checklist_results": [],
            "relevant_clauses": [],
            "risk_clauses": [],
            "recommendations": ["ìœ„í—˜ ì²´í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."],
            "analysis_time": 0.0
        }
    
    checklist_results = []
    total_score = 0.0
    relevant_clauses = []
    risk_clauses = []
    
    # ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë³„ ê¸°ë³¸ ë¶„ì„
    for item in part_data.get("checklistItems", []):
        item_text = item.get("item", "")
        keywords = item.get("keywords", [])
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        found_keywords = []
        for keyword in keywords:
            if keyword.lower() in contract_text.lower():
                found_keywords.append(keyword)
        
        # ê´€ë ¨ ì¡°í•­ ì°¾ê¸°
        clause_pattern = r'ì œ\d+ì¡°[^\n]*'
        clauses = re.findall(clause_pattern, contract_text)
        item_clauses = [c for c in clauses if any(kw.lower() in c.lower() for kw in keywords)]
        relevant_clauses.extend(item_clauses[:3])
        
        # ìœ„í—˜ë„ íŒë‹¨ (í‚¤ì›Œë“œ ë°œê²¬ ì—¬ë¶€ ê¸°ë°˜)
        if found_keywords:
            risk_score = min(len(found_keywords) * 1.5, 5.0)
            if risk_score >= 3:
                risk_clauses.extend(item_clauses[:2])
        else:
            risk_score = 1.0
        
        total_score += risk_score
        
        checklist_results.append({
            "item": item_text,
            "keywords_found": found_keywords,
            "risk_score": risk_score,
            "risk_level": determine_risk_level(risk_score),
            "related_clauses": item_clauses[:3],
            "analysis": f"í‚¤ì›Œë“œ {len(found_keywords)}ê°œ ë°œê²¬" if found_keywords else "ê´€ë ¨ í‚¤ì›Œë“œ ë¯¸ë°œê²¬"
        })
    
    avg_score = total_score / len(checklist_results) if checklist_results else 0.0
    
    return {
        "part_number": part_number,
        "part_title": part_data["partTitle"],
        "risk_score": avg_score,
        "risk_level": determine_risk_level(avg_score),
        "checklist_results": checklist_results,
        "relevant_clauses": list(set(relevant_clauses))[:10],
        "risk_clauses": list(set(risk_clauses))[:5],
        "recommendations": _generate_basic_recommendations(part_data, checklist_results),
        "analysis_time": 0.0
    }


def _generate_basic_recommendations(part_data: Dict, checklist_results: List[Dict]) -> List[str]:
    """ê¸°ë³¸ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    high_risk_items = [r for r in checklist_results if r.get("risk_score", 0) >= 3]
    
    if high_risk_items:
        recommendations.append(f"âš ï¸ {len(high_risk_items)}ê°œì˜ ê³ ìœ„í—˜ í•­ëª©ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for item in high_risk_items[:3]:
            recommendations.append(f"- {item.get('item', '')[:50]}... ê²€í†  í•„ìš”")
    else:
        recommendations.append("âœ… íŠ¹ë³„í•œ ê³ ìœ„í—˜ í•­ëª©ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    recommendations.append(f"ğŸ“‹ {part_data.get('partTitle', '')} ì˜ì—­ì˜ ìƒì„¸ ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    return recommendations
