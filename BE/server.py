#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AutoSchemaKG ë°±ì—”ë“œ ì„œë²„
FastAPIë¥¼ ì‚¬ìš©í•œ í˜„ëŒ€ì ì¸ REST API ì„œë²„
"""

import os
import sys
import json
import logging
import shutil
import uuid
import time
import requests
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# ìœ„í—˜ ë¶„ì„ ëª¨ë“ˆ import
from riskAnalysis.risk_analysis_api import router as risk_analysis_router

from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv

# UTF-8 ë¡œê¹… ì„¤ì •
from atlas_rag.utils.utf8_logging import setup_utf8_logging

# UTF-8 ë¡œê¹… ì´ˆê¸°í™”
setup_utf8_logging()
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ì „ì—­ ë³€ìˆ˜
rag_system = None
neo4j_driver = None
pipeline_status = {}  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬
uploaded_files = {}   # ì—…ë¡œë“œëœ íŒŒì¼ ê´€ë¦¬

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ì„¤ì •
UPLOAD_DIR = Path(__file__).parent.parent / "uploads"
UPLOAD_DIR.mkdir(exist_ok=True)

# Pydantic ëª¨ë¸ë“¤
class PipelineRequest(BaseModel):
    start_step: int = 0
    keyword: Optional[str] = None

class PipelineResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    question: str
    max_tokens: int = 8192
    temperature: float = 0.5

class ChatResponse(BaseModel):
    success: bool
    answer: str
    context_count: int
    processing_time: float

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    version: str

class FileUploadResponse(BaseModel):
    success: bool
    file_id: str
    filename: str
    message: str

class PipelineStatusResponse(BaseModel):
    success: bool
    status: str
    progress: int
    message: str
    data: Optional[Dict[str, Any]] = None

def restore_uploaded_files():
    """ì„œë²„ ì‹œì‘ ì‹œ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì„ ìŠ¤ìº”í•˜ì—¬ uploaded_files ë³µêµ¬"""
    global uploaded_files
    
    if not UPLOAD_DIR.exists():
        return
    
    # íŒŒì¼ë“¤ì„ ìˆ˜ì • ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœê·¼ íŒŒì¼ë§Œ ë³µêµ¬
    files_with_time = []
    for file_path in UPLOAD_DIR.iterdir():
        if file_path.is_file():
            files_with_time.append((file_path, file_path.stat().st_mtime))
    
    # ìˆ˜ì • ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    files_with_time.sort(key=lambda x: x[1], reverse=True)
    
    # ê°€ì¥ ìµœê·¼ íŒŒì¼ë§Œ ë³µêµ¬
    if files_with_time:
        file_path, _ = files_with_time[0]
        filename = file_path.name
        if '_' in filename:
            file_id = filename.split('_')[0]
            original_filename = '_'.join(filename.split('_')[1:])
            
            # íŒŒì¼ ì •ë³´ ë³µêµ¬
            uploaded_files[file_id] = {
                "filename": original_filename,
                "file_path": str(file_path),
                "upload_time": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                "file_size": file_path.stat().st_size
            }
            
            logger.info(f"âœ… ê°€ì¥ ìµœê·¼ ì—…ë¡œë“œ íŒŒì¼ ë³µêµ¬: {original_filename} (ID: {file_id})")

def load_risk_checklist():
    """ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
    try:
        # BE í´ë” ê¸°ì¤€ìœ¼ë¡œ ìœ„í—˜ì¡°í•­.txt íŒŒì¼ ì°¾ê¸°
        risk_file = Path(__file__).parent / "ìœ„í—˜ì¡°í•­.txt"
        
        if risk_file.exists():
            with open(risk_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    risk_items = [item.strip() for item in content.split('\n') if item.strip()]
                    return '\n'.join([f"{i+1}. {item}" for i, item in enumerate(risk_items)])
        else:
            logger.warning(f"âš ï¸ ìœ„í—˜ì¡°í•­.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {risk_file.absolute()}")
            return "ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"âŒ ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def find_existing_keyword():
    """ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ” í‚¤ì›Œë“œ ì°¾ê¸° (.env í‚¤ì›Œë“œ ìš°ì„ , ê·¸ ë‹¤ìŒ ìµœê·¼ ì—…ë¡œë“œ íŒŒì¼)"""
    import_dir = Path(os.getenv('IMPORT_DIRECTORY', 'BE/import'))
    logger.info(f"ğŸ” ì„ë² ë”© ë°ì´í„° íƒìƒ‰ ì¤‘... import_dir: {import_dir.absolute()}")
    
    if not import_dir.exists():
        logger.warning(f"âŒ import ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {import_dir.absolute()}")
        return None
    
    # 1. .envì— ì„¤ì •ëœ í‚¤ì›Œë“œ ìš°ì„  í™•ì¸
    env_keyword = os.getenv('KEYWORD')
    if env_keyword:
        keyword_dir = import_dir / env_keyword
        if keyword_dir.exists():
            precompute_dir = keyword_dir / os.getenv('PRECOMPUTE_DIRECTORY', 'precompute')
            if precompute_dir.exists():
                faiss_files = list(precompute_dir.glob("*_faiss.index"))
                if faiss_files:
                    logger.info(f"âœ… .envì— ì„¤ì •ëœ í‚¤ì›Œë“œì˜ ì„ë² ë”© ë°ì´í„° ë°œê²¬: {env_keyword}")
                    return env_keyword
                else:
                    logger.info(f"âš ï¸ .env í‚¤ì›Œë“œ {env_keyword}ì˜ precompute ë””ë ‰í† ë¦¬ëŠ” ìˆì§€ë§Œ FAISS íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.info(f"âš ï¸ .env í‚¤ì›Œë“œ {env_keyword}ì˜ precompute ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        else:
            logger.info(f"âš ï¸ .env í‚¤ì›Œë“œ {env_keyword}ì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
    else:
        logger.info("â„¹ï¸ .envì— KEYWORDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # 2. ê°€ì¥ ìµœê·¼ ì—…ë¡œë“œ íŒŒì¼ì˜ í‚¤ì›Œë“œ í™•ì¸
    if uploaded_files:
        # ì—…ë¡œë“œëœ íŒŒì¼ ì¤‘ ê°€ì¥ ìµœê·¼ íŒŒì¼ì˜ í‚¤ì›Œë“œ í™•ì¸
        latest_file_id = max(uploaded_files.keys(), key=lambda k: uploaded_files[k]['upload_time'])
        latest_file_keyword = f"contract_{latest_file_id}"
        
        # í•´ë‹¹ í‚¤ì›Œë“œì˜ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        keyword_dir = import_dir / latest_file_keyword
        if keyword_dir.exists():
            precompute_dir = keyword_dir / os.getenv('PRECOMPUTE_DIRECTORY', 'precompute')
            if precompute_dir.exists():
                faiss_files = list(precompute_dir.glob("*_faiss.index"))
                if faiss_files:
                    logger.info(f"âœ… ê°€ì¥ ìµœê·¼ ì—…ë¡œë“œ íŒŒì¼ì˜ ì„ë² ë”© ë°ì´í„° ë°œê²¬: {latest_file_keyword}")
                    return latest_file_keyword
                else:
                    logger.info(f"âš ï¸ ìµœê·¼ íŒŒì¼ {latest_file_keyword}ì˜ precompute ë””ë ‰í† ë¦¬ëŠ” ìˆì§€ë§Œ FAISS íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.info(f"âš ï¸ ìµœê·¼ íŒŒì¼ {latest_file_keyword}ì˜ precompute ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    # 3. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì°¾ê¸° (ìˆ˜ì • ì‹œê°„ìˆœ)
    logger.info(" ìµœê·¼ íŒŒì¼ì— ì„ë² ë”©ì´ ì—†ì–´ì„œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ íƒìƒ‰...")
    
    # import í´ë”ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸° (ìˆ˜ì • ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬)
    keyword_dirs = []
    for keyword_dir in import_dir.iterdir():
        if keyword_dir.is_dir():
            logger.info(f"ğŸ“ í‚¤ì›Œë“œ ë””ë ‰í† ë¦¬ ë°œê²¬: {keyword_dir.name}")
            precompute_dir = keyword_dir / os.getenv('PRECOMPUTE_DIRECTORY', 'precompute')
            if precompute_dir.exists():
                # FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                faiss_files = list(precompute_dir.glob("*_faiss.index"))
                if faiss_files:
                    # ìˆ˜ì • ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                    mtime = keyword_dir.stat().st_mtime
                    keyword_dirs.append((mtime, keyword_dir.name))
    
    if keyword_dirs:
        # ìˆ˜ì • ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        keyword_dirs.sort(key=lambda x: x[0], reverse=True)
        latest_keyword = keyword_dirs[0][1]
        logger.info(f"âœ… ê¸°ì¡´ ì„ë² ë”© ë°ì´í„° ë°œê²¬ (ìµœì‹ ìˆœ): {latest_keyword}")
        return latest_keyword
    
    logger.warning("âŒ ì„ë² ë”© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return None

def check_and_load_existing_data():
    """ì„œë²„ ì‹œì‘ ì‹œ ê¸°ì¡´ Neo4j ë°ì´í„° í™•ì¸ ë° ë¡œë“œ"""
    global rag_system, neo4j_driver
    
    try:
        logger.info("ğŸ” ê¸°ì¡´ Neo4j ë°ì´í„° í™•ì¸ ì¤‘...")
        
        # ë¨¼ì € Neo4j ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            from neo4j import GraphDatabase
            neo4j_uri = os.getenv('NEO4J_URI', 'neo4j://127.0.0.1:7687')
            neo4j_user = os.getenv('NEO4J_USER', 'neo4j')
            neo4j_password = os.getenv('NEO4J_PASSWORD', '')
            neo4j_database = os.getenv('NEO4J_DATABASE')
            
            driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
            
            # Neo4jì—ì„œ ë…¸ë“œ ìˆ˜ í™•ì¸
            with driver.session(database=neo4j_database) as session:
                result = session.run("MATCH (n) RETURN count(n) as node_count")
                node_count = result.single()["node_count"]
                
                if node_count > 0:
                    logger.info(f"âœ… ê¸°ì¡´ Neo4j ë°ì´í„° ë°œê²¬: {node_count}ê°œ ë…¸ë“œ")
                    
                    # ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
                    existing_keyword = find_existing_keyword()
                    if existing_keyword:
                        # í™˜ê²½ë³€ìˆ˜ì— í‚¤ì›Œë“œ ì„¤ì •
                        os.environ['KEYWORD'] = existing_keyword
                        logger.info(f"ğŸ”‘ í‚¤ì›Œë“œ ì„¤ì •: {existing_keyword}")
                        
                        # RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹œë„
                        try:
                            from experiment.run_questions_v3_with_concept import load_enhanced_rag_system
                            enhanced_lkg_retriever, hippo_retriever, llm_generator, neo4j_driver = load_enhanced_rag_system()
                            
                            # RAG ì‹œìŠ¤í…œ ì„¤ì •
                            rag_system = {
                                "enhanced_lkg_retriever": enhanced_lkg_retriever,
                                "hippo_retriever": hippo_retriever,
                                "llm_generator": llm_generator
                            }
                            
                            logger.info("âœ… ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
                            return True
                            
                        except Exception as rag_error:
                            logger.warning(f"âš ï¸ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {rag_error}")
                            return False
                    else:
                        logger.info("â„¹ï¸ ì„ë² ë”© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                        return False
                        
                else:
                    logger.info("â„¹ï¸ Neo4jì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                    return False
                    
        except Exception as neo4j_error:
            logger.warning(f"âš ï¸ Neo4j ì—°ê²° ì‹¤íŒ¨: {neo4j_error}")
            return False
            
    except Exception as e:
        logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜"""
    # ì‹œì‘ ì‹œ
    logger.info("ğŸš€ AutoSchemaKG ë°±ì—”ë“œ ì„œë²„ ì‹œì‘ ì¤‘...")
    restore_uploaded_files()
    
    # ê¸°ì¡´ Neo4j ë°ì´í„° í™•ì¸ ë° ë¡œë“œ
    data_loaded = check_and_load_existing_data()
    if not data_loaded:
        logger.info("â„¹ï¸ ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    yield
    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ›‘ AutoSchemaKG ë°±ì—”ë“œ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    
    # Neo4j ì—°ê²° ì •ë¦¬
    if neo4j_driver:
        neo4j_driver.close()
        logger.info("âœ… Neo4j ì—°ê²° ì •ë¦¬ ì™„ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AutoSchemaKG Backend API",
    description="ì§€ì‹ê·¸ë˜í”„ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ë°±ì—”ë“œ API",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìœ„í—˜ ë¶„ì„ ë¼ìš°í„° ì¶”ê°€
app.include_router(risk_analysis_router)

def load_rag_system():
    """RAG ì‹œìŠ¤í…œ ë¡œë“œ"""
    global rag_system, neo4j_driver
    
    try:
        from experiment.run_questions_v3_with_concept import load_enhanced_rag_system
        
        enhanced_lkg_retriever, hippo_retriever, llm_generator, neo4j_driver = load_enhanced_rag_system()
        
        rag_system = {
            "enhanced_lkg_retriever": enhanced_lkg_retriever,
            "hippo_retriever": hippo_retriever,
            "llm_generator": llm_generator
        }
        
        logger.info("âœ… RAG ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - ì„œë²„ ìƒíƒœ í™•ì¸"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.now().isoformat(),
        version="2.0.0"
    )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.now().isoformat(),
        version="2.0.0"
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±—ë´‡ ì§ˆë¬¸ ì²˜ë¦¬"""
    start_time = datetime.now()
    
    try:
        # RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ë¡œë“œ ì‹œë„
        if rag_system is None:
            if not load_rag_system():
                raise HTTPException(status_code=500, detail="RAG ì‹œìŠ¤í…œì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        from experiment.run_questions_v3_with_concept import concept_enhanced_hybrid_retrieve
        
        # Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        search_result = concept_enhanced_hybrid_retrieve(
            request.question, 
            rag_system["enhanced_lkg_retriever"], 
            rag_system["hippo_retriever"],
            rag_system["llm_generator"],
            neo4j_driver
        )
        
        # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        if search_result and len(search_result) == 2:
            sorted_context, context_ids = search_result
            context_count = len(context_ids) if context_ids else 0
        else:
            sorted_context = search_result if search_result else None
            context_count = 0
        
        if sorted_context:
            # ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
            risk_checklist = load_risk_checklist()
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            system_instruction = (
                "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ê³ ê¸‰ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¶”ì¶œëœ ì •ë³´ì™€ ì§ˆë¬¸ì„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                "ì§€ì‹ ê·¸ë˜í”„ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ìì‹ ì˜ ì§€ì‹ì„ í™œìš©í•´ì„œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                "ë‹µë³€ì€ 'Thought: 'ë¡œ ì‹œì‘í•˜ì—¬ ì¶”ë¡  ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³ , "
                "'Answer: 'ë¡œ ëë‚˜ë©° ê°„ê²°í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "
                "ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.\n\n"
                f"=== ê³„ì•½ì„œ ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ ===\n{risk_checklist}\n"
                "ìœ„ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ê³„ì•½ì„œì˜ ì ì¬ì  ìœ„í—˜ìš”ì†Œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
            )
            
            messages = [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": f"{sorted_context}\n\n{request.question}"},
            ]
            
            result = rag_system["llm_generator"].generate_response(
                messages, 
                max_new_tokens=request.max_tokens, 
                temperature=request.temperature,
                validate_function=None
            )
        else:
            result = "ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ChatResponse(
            success=True,
            answer=result if result else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            context_count=context_count,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ChatResponse(
            success=False,
            answer=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            context_count=0,
            processing_time=processing_time
        )

@app.post("/analyze-risks", response_model=ChatResponse)
async def analyze_contract_risks(request: ChatRequest):
    """ê³„ì•½ì„œ ìœ„í—˜ì¡°í•­ ì „ìš© ë¶„ì„"""
    start_time = datetime.now()
    
    try:
        # RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ë¡œë“œ ì‹œë„
        if rag_system is None:
            if not load_rag_system():
                raise HTTPException(status_code=500, detail="RAG ì‹œìŠ¤í…œì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Concept í™œìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        from experiment.run_questions_v3_with_concept import concept_enhanced_hybrid_retrieve
        
        sorted_context = concept_enhanced_hybrid_retrieve(
            request.question, 
            rag_system["enhanced_lkg_retriever"], 
            rag_system["hippo_retriever"],
            rag_system["llm_generator"],
            neo4j_driver,
            topN=50
        )
        
        if sorted_context:
            # ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
            risk_checklist = load_risk_checklist()
            
            # ìœ„í—˜ì¡°í•­ ë¶„ì„ ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_instruction = (
                "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ê³„ì•½ì„œ ìœ„í—˜ì¡°í•­ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                "ì œê³µëœ ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì•½ì„œë¥¼ ì² ì €íˆ ë¶„ì„í•˜ì„¸ìš”.\n\n"
                "ë¶„ì„ ì‹œ ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n"
                "1. **ë°œê²¬ëœ ìœ„í—˜ìš”ì†Œ**: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë°œê²¬ëœ ìœ„í—˜ì¡°í•­ë“¤\n"
                "2. **ìœ„í—˜ë„ í‰ê°€**: ê° ìœ„í—˜ìš”ì†Œì˜ ìœ„í—˜ë„ (ë§¤ìš° ë†’ìŒ/ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)\n"
                "3. **ì˜í–¥ ë‹¹ì‚¬ì**: ë§¤ìˆ˜ì¸/ë§¤ë„ì¸/ì–‘ ë‹¹ì‚¬ì\n"
                "4. **ê°œì„  ê¶Œê³ ì‚¬í•­**: êµ¬ì²´ì ì¸ ê³„ì•½ì„œ ìˆ˜ì • ì œì•ˆ\n"
                "5. **ì¢…í•© í‰ê°€**: ì „ì²´ì ì¸ ìœ„í—˜ ìˆ˜ì¤€ê³¼ ì£¼ìš” ìš°ë ¤ì‚¬í•­\n\n"
                f"=== ê³„ì•½ì„œ ìœ„í—˜ì¡°í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸ ===\n{risk_checklist}\n"
                "ìœ„ ì²´í¬ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì„ ê³„ì•½ì„œ ë‚´ìš©ê³¼ ëŒ€ì¡°í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”."
            )
            
            messages = [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": f"ê³„ì•½ì„œ ë‚´ìš©:\n{sorted_context}\n\në¶„ì„ ìš”ì²­: {request.question}"},
            ]
            
            result = rag_system["llm_generator"].generate_response(
                messages, 
                max_new_tokens=request.max_tokens, 
                temperature=request.temperature
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… ìœ„í—˜ì¡°í•­ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
            
            return ChatResponse(
                success=True,
                answer=result,
                context_count=len(sorted_context) if isinstance(sorted_context, list) else 0,
                processing_time=processing_time
            )
        else:
            logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ChatResponse(
                success=False,
                answer="ê³„ì•½ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.",
                context_count=0,
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
    except Exception as e:
        logger.error(f"âŒ ìœ„í—˜ì¡°í•­ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return ChatResponse(
            success=False,
            answer=f"ìœ„í—˜ì¡°í•­ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            context_count=0,
            processing_time=(datetime.now() - start_time).total_seconds()
        )

@app.post("/pipeline/run", response_model=PipelineResponse)
async def run_pipeline(request: PipelineRequest, background_tasks: BackgroundTasks):
    """ATLAS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        background_tasks.add_task(execute_pipeline, request.start_step, request.keyword)
        
        return PipelineResponse(
            success=True,
            message="íŒŒì´í”„ë¼ì¸ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            data={"start_step": request.start_step, "keyword": request.keyword}
        )
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def execute_pipeline(start_step: int, keyword: Optional[str], pipeline_id: str = None):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ (subprocess ë°©ì‹)"""
    global pipeline_status
    
    print(f"ğŸš€ subprocessë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ - keyword: {keyword}, start_step: {start_step}")
    logger.info(f"ğŸš€ subprocessë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ - keyword: {keyword}, start_step: {start_step}")
    
    if pipeline_id:
        pipeline_status[pipeline_id] = {
            "status": "running",
            "progress": 0,
            "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...",
            "start_time": datetime.now().isoformat()
        }
        print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ - ID: {pipeline_id}")
        logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ - ID: {pipeline_id}")
    
    try:
        import subprocess
        import sys
        
        # BE ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
        be_dir = Path(__file__).parent
        cmd = [sys.executable, "main_pipeline.py", str(start_step), keyword]
        
        print(f"ğŸ“‹ subprocess ëª…ë ¹ì–´: {' '.join(cmd)}")
        print(f"ğŸ“‚ ì‹¤í–‰ ë””ë ‰í† ë¦¬: {be_dir}")
        logger.info(f"ğŸ“‹ subprocess ëª…ë ¹ì–´: {' '.join(cmd)}")
        logger.info(f"ğŸ“‚ ì‹¤í–‰ ë””ë ‰í† ë¦¬: {be_dir}")
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['LANG'] = 'ko_KR.UTF-8'
        env['LC_ALL'] = 'ko_KR.UTF-8'
        # env['KEYWORD'] = os.getenv('KEYWORD')
        env['KEYWORD'] = keyword  # keyword í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        
        if pipeline_id:
            pipeline_status[pipeline_id]["progress"] = 25
            pipeline_status[pipeline_id]["message"] = "íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì¤‘..."
            print("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: 25%")
            logger.info("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: 25%")
        
        # subprocessë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = subprocess.run(
            cmd,
            cwd=be_dir,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            env=env,
            timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        )
        
        print(f"ğŸ“‹ subprocess ê²°ê³¼ ì½”ë“œ: {result.returncode}")
        print(f"ğŸ“ stdout: {result.stdout}")
        print(f"ğŸ“ stderr: {result.stderr}")
        logger.info(f"ğŸ“‹ subprocess ê²°ê³¼ ì½”ë“œ: {result.returncode}")
        logger.info(f"ğŸ“ stdout: {result.stdout}")
        logger.info(f"ğŸ“ stderr: {result.stderr}")
        
        success = result.returncode == 0
        
        if success:
            print("âœ… subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
            logger.info("âœ… subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
            
            # íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ìë™ ë¡œë“œ
            print("ğŸ”„ RAG ì‹œìŠ¤í…œ ìë™ ë¡œë“œ ì¤‘...")
            logger.info("ğŸ”„ RAG ì‹œìŠ¤í…œ ìë™ ë¡œë“œ ì¤‘...")
            if check_and_load_existing_data():
                print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ")
                logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ")
            else:
                print("âš ï¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨")
                logger.warning("âš ï¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨")
            
            if pipeline_id:
                pipeline_status[pipeline_id] = {
                    "status": "completed",
                    "progress": 100,
                    "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ",
                    "end_time": datetime.now().isoformat()
                }
                print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ - ID: {pipeline_id}")
                logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ - ID: {pipeline_id}")
        else:
            print("âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            logger.error("âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            if pipeline_id:
                pipeline_status[pipeline_id] = {
                    "status": "failed",
                    "progress": 0,
                    "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨",
                    "end_time": datetime.now().isoformat()
                }
                print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹¤íŒ¨ - ID: {pipeline_id}")
                logger.error(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹¤íŒ¨ - ID: {pipeline_id}")
        
        return success
        
    except subprocess.TimeoutExpired:
        print("â° subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
        logger.error("â° subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
        
        if pipeline_id:
            pipeline_status[pipeline_id] = {
                "status": "failed",
                "progress": 0,
                "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ",
                "end_time": datetime.now().isoformat()
            }
            print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: íƒ€ì„ì•„ì›ƒ - ID: {pipeline_id}")
            logger.error(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: íƒ€ì„ì•„ì›ƒ - ID: {pipeline_id}")
        
        return False
        
    except Exception as e:
        print(f"âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        import traceback
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
        logger.error(f"subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
        
        if pipeline_id:
            pipeline_status[pipeline_id] = {
                "status": "failed",
                "progress": 0,
                "message": f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "end_time": datetime.now().isoformat()
            }
            print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì˜¤ë¥˜ - ID: {pipeline_id}")
            logger.error(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì˜¤ë¥˜ - ID: {pipeline_id}")
        
        return False

@app.post("/upload/contract", response_model=FileUploadResponse)
async def upload_contract(file: UploadFile = File(...)):
    """ê³„ì•½ì„œ íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ ID ìƒì„±
        file_id = str(uuid.uuid4())
        
        # íŒŒì¼ ì €ì¥
        file_path = UPLOAD_DIR / f"{file_id}_{file.filename}"
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì €ì¥
        uploaded_files[file_id] = {
            "filename": file.filename,
            "file_path": str(file_path),
            "upload_time": datetime.now().isoformat(),
            "file_size": len(content)
        }
        
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} (ID: {file_id})")
        
        return FileUploadResponse(
            success=True,
            file_id=file_id,
            filename=file.filename,
            message="íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/pipeline/run-with-file", response_model=PipelineResponse)
async def run_pipeline_with_file(
    file_id: str = Form(...),
    start_step: int = Form(1),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """ì—…ë¡œë“œëœ íŒŒì¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    try:
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìš”ì²­ - file_id: {file_id}")
        logger.info(f"í˜„ì¬ uploaded_files í‚¤ë“¤: {list(uploaded_files.keys())}")
        
        if file_id not in uploaded_files:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš”ì²­ëœ file_id: {file_id}")
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        file_info = uploaded_files[file_id]
        file_path = file_info["file_path"]
        
        # íŒŒì¼ì„ example_data ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬ (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
        example_data_dir = Path(os.getenv('DATA_DIRECTORY', 'BE/example_data'))
        example_data_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì²˜ë¦¬
        file_ext = Path(file_path).suffix.lower()
        if file_ext == '.json':
            # JSON íŒŒì¼ì¸ ê²½ìš° ë‚´ìš© í™•ì¸ í›„ ATLAS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # ATLASê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if isinstance(json_data, list) and len(json_data) > 0 and 'text' in json_data[0]:
                # ì´ë¯¸ ATLAS í˜•ì‹ì¸ ê²½ìš°
                atlas_data = json_data
            elif isinstance(json_data, dict) and 'content' in json_data:
                # ì—…ë¡œë“œëœ JSONì—ì„œ content ì¶”ì¶œ
                content = json_data['content']
                atlas_data = [
                    {
                        "id": "1",
                        "text": content,
                        "metadata": {
                            "lang": "ko",
                            "filename": file_info["filename"],
                            "upload_time": file_info["upload_time"]
                        }
                    }
                ]
            else:
                # ì§ì ‘ì ì¸ JSON ë‚´ìš©ì¸ ê²½ìš°
                content = json.dumps(json_data, ensure_ascii=False, indent=2)
                atlas_data = [
                    {
                        "id": "1",
                        "text": content,
                        "metadata": {
                            "lang": "ko",
                            "filename": file_info["filename"],
                            "upload_time": file_info["upload_time"]
                        }
                    }
                ]
            
            target_path = example_data_dir / f"contract_{file_id}.json"
            with open(target_path, 'w', encoding='utf-8') as f:
                json.dump(atlas_data, f, ensure_ascii=False, indent=2)
            
            keyword = f"contract_{file_id}"
        elif file_ext in ['.txt', '.md']:
            # í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì¸ ê²½ìš° md_data í´ë”ì— ì €ì¥ (íŒŒì´í”„ë¼ì¸ì—ì„œ ë³€í™˜)
            if file_ext == '.md':
                # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì€ md_data í´ë”ì— ì €ì¥
                md_data_dir = example_data_dir / "md_data"
                md_data_dir.mkdir(exist_ok=True)
                target_path = md_data_dir / f"contract_{file_id}{file_ext}"
            else:
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ example_dataì— ì§ì ‘ ì €ì¥
                target_path = example_data_dir / f"contract_{file_id}{file_ext}"
            
            shutil.copy2(file_path, target_path)
            keyword = f"contract_{file_id}"
        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # íŒŒì´í”„ë¼ì¸ ID ìƒì„±
        pipeline_id = str(uuid.uuid4())
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë§ˆí¬ë‹¤ìš´ ë³€í™˜ í¬í•¨)
        actual_start_step = 0 if start_step == 1 else start_step
        background_tasks.add_task(execute_pipeline_with_risk_analysis, actual_start_step, keyword, pipeline_id, file_id)
        
        return PipelineResponse(
            success=True,
            message="íŒŒì´í”„ë¼ì¸ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            data={
                "pipeline_id": pipeline_id,
                "keyword": keyword,
                "file_info": file_info
            }
        )
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/pipeline/status/{pipeline_id}", response_model=PipelineStatusResponse)
async def get_pipeline_status(pipeline_id: str):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ ì¡°íšŒ"""
    try:
        if pipeline_id not in pipeline_status:
            raise HTTPException(status_code=404, detail="íŒŒì´í”„ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        status_info = pipeline_status[pipeline_id]
        
        return PipelineStatusResponse(
            success=True,
            status=status_info["status"],
            progress=status_info["progress"],
            message=status_info["message"],
            data=status_info
        )
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def get_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        # Neo4j ì—°ê²° ì‹¤ì œ í…ŒìŠ¤íŠ¸
        neo4j_connected = False
        if neo4j_driver:
            try:
                with neo4j_driver.session() as session:
                    result = session.run("RETURN 1 as test")
                    neo4j_connected = True
            except Exception as e:
                logger.warning(f"Neo4j ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                neo4j_connected = False
        
        status = {
            "rag_system_loaded": rag_system is not None,
            "neo4j_connected": neo4j_connected,  # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸ ê²°ê³¼
            "timestamp": datetime.now().isoformat()
        }
        
        return {"success": True, "status": status}
        
    except Exception as e:
        logger.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/files")
async def list_uploaded_files():
    """ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        files = []
        for file_id, file_info in uploaded_files.items():
            files.append({
                "file_id": file_id,
                "filename": file_info["filename"],
                "upload_time": file_info["upload_time"],
                "file_size": file_info["file_size"]
            })
        
        return {"success": True, "files": files}
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/files/{file_id}")
async def delete_uploaded_file(file_id: str):
    """ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ"""
    try:
        if file_id not in uploaded_files:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        file_info = uploaded_files[file_id]
        file_path = Path(file_info["file_path"])
        
        # íŒŒì¼ ì‚­ì œ
        if file_path.exists():
            file_path.unlink()
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        del uploaded_files[file_id]
        
        logger.info(f"íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_info['filename']}")
        
        return {"success": True, "message": "íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/chat/history")
async def get_chat_history(limit: int = 10):
    """ì±—ë´‡ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    try:
        qa_file_path = "qa_history_api.json"
        
        if not os.path.exists(qa_file_path):
            return {"success": True, "history": []}
        
        with open(qa_file_path, 'r', encoding='utf-8') as f:
            qa_data = json.load(f)
        
        # ìµœê·¼ ê¸°ë¡ë§Œ ë°˜í™˜
        recent_history = qa_data[-limit:] if len(qa_data) > limit else qa_data
        
        return {"success": True, "history": recent_history}
        
    except Exception as e:
        logger.error(f"ëŒ€í™” ê¸°ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/chat/history")
async def clear_chat_history():
    """ì±—ë´‡ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
    try:
        qa_file_path = "qa_history_api.json"
        
        if os.path.exists(qa_file_path):
            os.remove(qa_file_path)
        
        return {"success": True, "message": "ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        
    except Exception as e:
        logger.error(f"ëŒ€í™” ê¸°ë¡ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload-and-run", response_model=PipelineResponse)
async def upload_and_run_pipeline(
    file: UploadFile = File(...),
    start_step: int = Form(1),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """íŒŒì¼ ì—…ë¡œë“œì™€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ í•œ ë²ˆì— ì²˜ë¦¬"""
    try:
        # 1. íŒŒì¼ ì—…ë¡œë“œ
        file_id = str(uuid.uuid4())
        file_path = UPLOAD_DIR / f"{file_id}_{file.filename}"
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì €ì¥
        uploaded_files[file_id] = {
            "filename": file.filename,
            "file_path": str(file_path),
            "upload_time": datetime.now().isoformat(),
            "file_size": len(content)
        }
        
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} (ID: {file_id})")
        
        # 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        file_info = uploaded_files[file_id]
        file_path = file_info["file_path"]
        
        # íŒŒì¼ì„ example_data ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬ (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
        example_data_dir = Path(os.getenv('DATA_DIRECTORY', 'BE/example_data'))
        example_data_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì²˜ë¦¬
        file_ext = Path(file_path).suffix.lower()
        if file_ext == '.json':
            # JSON íŒŒì¼ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë³µì‚¬
            target_path = example_data_dir / f"contract_{file_id}.json"
            shutil.copy2(file_path, target_path)
            keyword = f"contract_{file_id}"
        elif file_ext in ['.txt', '.md']:
            # í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì¸ ê²½ìš° md_data í´ë”ì— ì €ì¥ (íŒŒì´í”„ë¼ì¸ì—ì„œ ë³€í™˜)
            if file_ext == '.md':
                # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì€ md_data í´ë”ì— ì €ì¥
                md_data_dir = example_data_dir / "md_data"
                md_data_dir.mkdir(exist_ok=True)
                target_path = md_data_dir / f"contract_{file_id}{file_ext}"
            else:
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ example_dataì— ì§ì ‘ ì €ì¥
                target_path = example_data_dir / f"contract_{file_id}{file_ext}"
            
            shutil.copy2(file_path, target_path)
            keyword = f"contract_{file_id}"
        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # íŒŒì´í”„ë¼ì¸ ID ìƒì„±
        pipeline_id = str(uuid.uuid4())
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
        pipeline_status[pipeline_id] = {
            "status": "running",
            "start_time": datetime.now().isoformat(),
            "file_info": file_info,
            "keyword": keyword
        }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë§ˆí¬ë‹¤ìš´ ë³€í™˜ í¬í•¨)
        actual_start_step = 0 if start_step == 1 else start_step
        background_tasks.add_task(execute_pipeline, actual_start_step, keyword, pipeline_id)
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: {pipeline_id} (íŒŒì¼: {file.filename})")
        
        return PipelineResponse(
            success=True,
            message="íŒŒì´í”„ë¼ì¸ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            data={
                "pipeline_id": pipeline_id,
                "keyword": keyword,
                "file_info": file_info
            }
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/docs")
async def get_api_docs():
    """API ë¬¸ì„œ ì •ë³´ ë°˜í™˜"""
    return {
        "title": "AutoSchemaKG Backend API",
        "version": "2.0.0",
        "description": "ì§€ì‹ê·¸ë˜í”„ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ë°±ì—”ë“œ API",
        "endpoints": {
            "health": "GET /health - ì„œë²„ ìƒíƒœ í™•ì¸",
            "chat": "POST /chat - ì±—ë´‡ ì§ˆë¬¸ ì²˜ë¦¬",
            "analyze_risks": "POST /analyze-risks - ê³„ì•½ì„œ ìœ„í—˜ì¡°í•­ ë¶„ì„",
            "upload_contract": "POST /upload/contract - ê³„ì•½ì„œ íŒŒì¼ ì—…ë¡œë“œ",
            "run_pipeline": "POST /pipeline/run - ATLAS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰",
            "run_with_file": "POST /pipeline/run-with-file - ì—…ë¡œë“œëœ íŒŒì¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰",
            "upload_and_run": "POST /upload-and-run - íŒŒì¼ ì—…ë¡œë“œì™€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ í•œ ë²ˆì— ì²˜ë¦¬",
            "pipeline_status": "GET /pipeline/status/{pipeline_id} - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ ì¡°íšŒ",
            "system_status": "GET /status - ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ",
            "list_files": "GET /files - ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ",
            "delete_file": "DELETE /files/{file_id} - ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ",
            "chat_history": "GET /chat/history - ì±—ë´‡ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ",
            "clear_history": "DELETE /chat/history - ì±—ë´‡ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"
        },
        "swagger_ui": "/docs",
        "redoc": "/redoc"
    }

@app.post("/test-pipeline")
async def test_pipeline_direct():
    """íŒŒì´í”„ë¼ì¸ ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§ª íŒŒì´í”„ë¼ì¸ ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("ğŸ§ª íŒŒì´í”„ë¼ì¸ ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
        print(f"ğŸ“‚ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
        logger.info(f"ğŸ“‚ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        print(f"ğŸ”‘ KEYWORD: {os.getenv('KEYWORD', 'ì—†ìŒ')}")
        print(f"ğŸ“Š DATA_DIRECTORY: {os.getenv('DATA_DIRECTORY', 'ì—†ìŒ')}")
        print(f"ğŸ“¦ OPENAI_API_KEY: {'ìˆìŒ' if os.getenv('OPENAI_API_KEY') else 'ì—†ìŒ'}")
        
        # ë‹¨ê³„ë³„ import í…ŒìŠ¤íŠ¸
        print("ğŸ“¦ 1ë‹¨ê³„: dotenv import ì¤‘...")
        from dotenv import load_dotenv
        print("âœ… dotenv import ì™„ë£Œ")
        
        print("ğŸ“¦ 2ë‹¨ê³„: atlas_rag ëª¨ë“ˆë“¤ import ì¤‘...")
        from atlas_rag.kg_construction.triple_extraction import KnowledgeGraphExtractor
        print("âœ… KnowledgeGraphExtractor import ì™„ë£Œ")
        
        from atlas_rag.kg_construction.triple_config import ProcessingConfig
        print("âœ… ProcessingConfig import ì™„ë£Œ")
        
        from atlas_rag.llm_generator import LLMGenerator
        print("âœ… LLMGenerator import ì™„ë£Œ")
        
        print("ğŸ“¦ 3ë‹¨ê³„: main_pipeline ëª¨ë“ˆ import ì¤‘...")
        from main_pipeline import test_atlas_pipeline
        print("âœ… main_pipeline import ì™„ë£Œ")
        
        print("ğŸ“¦ 4ë‹¨ê³„: test_atlas_pipeline í•¨ìˆ˜ ì‹¤í–‰ ì¤‘...")
        success = test_atlas_pipeline(0, "test_contract")
        print(f"âœ… test_atlas_pipeline ì‹¤í–‰ ì™„ë£Œ: {success}")
        
        return {
            "success": success,
            "message": "íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ",
            "current_dir": os.getcwd(),
            "env_vars": {
                "KEYWORD": os.getenv('KEYWORD'),
                "DATA_DIRECTORY": os.getenv('DATA_DIRECTORY'),
                "OPENAI_API_KEY": "ìˆìŒ" if os.getenv('OPENAI_API_KEY') else "ì—†ìŒ"
            }
        }
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        logger.error(f"íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        
        return {
            "success": False,
            "message": f"íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}",
            "error": str(e)
        }

@app.post("/compare-answers")
async def compare_answers(
    question: str = Form(...),
    document_id: str = Form(...)
):
    """
    AutoSchemaKGì™€ OpenAIì˜ ë‹µë³€ì„ ë¹„êµí•©ë‹ˆë‹¤.
    """
    try:
        logger.info(f"ë‹µë³€ ë¹„êµ ì‹œì‘ - ì§ˆë¬¸: {question[:50]}..., ë¬¸ì„œ: {document_id}")
        
        # AutoSchemaKG ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        atlas_start_time = time.time()
        atlas_response = requests.post(
            f"http://localhost:8000/chat",
            json={
                "question": question,
                "document_id": document_id
            },
            timeout=60
        )
        atlas_time = time.time() - atlas_start_time
        
        atlas_result = {
            "success": False,
            "answer": "",
            "contexts": [],
            "processing_time": atlas_time
        }
        
        if atlas_response.status_code == 200:
            atlas_data = atlas_response.json()
            atlas_result = {
                "success": True,
                "answer": atlas_data.get("answer", ""),
                "contexts": atlas_data.get("contexts", []),
                "processing_time": atlas_time,
                "context_count": len(atlas_data.get("contexts", []))
            }
        
        # OpenAI ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        openai_result = await get_openai_answer(question, document_id)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = 0.0
        if atlas_result["success"] and openai_result["success"]:
            similarity = calculate_text_similarity(
                atlas_result["answer"], 
                openai_result["answer"]
            )
        
        # ê²°ê³¼ ë°˜í™˜
        comparison_result = {
            "question": question,
            "document_id": document_id,
            "atlas_result": atlas_result,
            "openai_result": openai_result,
            "similarity": similarity,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"ë‹µë³€ ë¹„êµ ì™„ë£Œ - ìœ ì‚¬ë„: {similarity:.3f}")
        return comparison_result
        
    except Exception as e:
        logger.error(f"ë‹µë³€ ë¹„êµ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ë¹„êµ ì‹¤íŒ¨: {str(e)}")

async def get_openai_answer(question: str, document_id: str) -> Dict[str, Any]:
    """
    OpenAI APIë¥¼ í†µí•´ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        import openai
        
        # ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        document_path = UPLOAD_DIR / f"{document_id}.md"
        if not document_path.exists():
            return {
                "success": False,
                "error": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "answer": "",
                "processing_time": 0
            }
        
        with open(document_path, 'r', encoding='utf-8') as f:
            document_content = f.read()
        
        start_time = time.time()
        
        # OpenAI API í˜¸ì¶œ
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        prompt = f"""
ë‹¤ìŒ ê³„ì•½ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ê³„ì•½ì„œ ë‚´ìš©:
{document_content}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ê³„ì•½ì„œì˜ êµ¬ì²´ì ì¸ ì¡°í•­ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€
2. ë²•ì  ê´€ì ì—ì„œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë¶„ì„ ì œê³µ
3. ë…ì†Œì¡°í•­ì´ë‚˜ ìœ„í—˜ ìš”ì†Œê°€ ìˆë‹¤ë©´ ëª…í™•íˆ ì§€ì 
4. ë‹µë³€ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­ ë²ˆí˜¸ë‚˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
"""
        
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê³„ì•½ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.1
        )
        
        processing_time = time.time() - start_time
        answer = response.choices[0].message.content.strip()
        
        return {
            "success": True,
            "answer": answer,
            "processing_time": processing_time,
            "model": "gpt-4o",
            "tokens_used": response.usage.total_tokens if response.usage else 0
        }
        
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "answer": "",
            "processing_time": 0
        }

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    try:
        # ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
            
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0

@app.post("/batch-compare")
async def batch_compare_answers(
    questions_file: str = Form(...),
    document_id: str = Form(...),
    max_questions: Optional[int] = Form(None)
):
    """
    ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•´ AutoSchemaKGì™€ OpenAIì˜ ë‹µë³€ì„ ì¼ê´„ ë¹„êµí•©ë‹ˆë‹¤.
    """
    try:
        logger.info(f"ë°°ì¹˜ ë¹„êµ ì‹œì‘ - ì§ˆë¬¸ íŒŒì¼: {questions_file}, ë¬¸ì„œ: {document_id}")
        
        # ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ
        questions_path = Path(questions_file)
        if not questions_path.exists():
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(questions_path, 'r', encoding='utf-8') as f:
            questions_data = json.load(f)
        
        questions = questions_data.get("questions", [])
        if max_questions:
            questions = questions[:max_questions]
        
        # ë¬¸ì„œ ë‚´ìš© ë¡œë“œ
        document_path = UPLOAD_DIR / f"{document_id}.md"
        if not document_path.exists():
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(document_path, 'r', encoding='utf-8') as f:
            document_content = f.read()
        
        # ë¹„êµ ê²°ê³¼ ì €ì¥
        comparison_results = []
        successful_comparisons = 0
        total_atlas_time = 0
        total_openai_time = 0
        total_similarity = 0
        
        for i, question_data in enumerate(questions, 1):
            question_id = question_data.get("question_id", i)
            question = question_data.get("question", "")
            expected_answer = question_data.get("answer", "")
            
            logger.info(f"ì§ˆë¬¸ {i}/{len(questions)} ì²˜ë¦¬ ì¤‘: {question[:50]}...")
            
            # AutoSchemaKG ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            atlas_start_time = time.time()
            atlas_response = requests.post(
                f"http://localhost:8000/chat",
                json={
                    "question": question,
                    "document_id": document_id
                },
                timeout=60
            )
            atlas_time = time.time() - atlas_start_time
            
            atlas_result = {
                "success": False,
                "answer": "",
                "contexts": [],
                "processing_time": atlas_time
            }
            
            if atlas_response.status_code == 200:
                atlas_data = atlas_response.json()
                atlas_result = {
                    "success": True,
                    "answer": atlas_data.get("answer", ""),
                    "contexts": atlas_data.get("contexts", []),
                    "processing_time": atlas_time,
                    "context_count": len(atlas_data.get("contexts", []))
                }
            
            # OpenAI ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            openai_result = await get_openai_answer_with_content(question, document_content)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = 0.0
            if atlas_result["success"] and openai_result["success"]:
                similarity = calculate_text_similarity(
                    atlas_result["answer"], 
                    openai_result["answer"]
                )
            
            # ê²°ê³¼ ì €ì¥
            comparison_result = {
                "question_id": question_id,
                "question": question,
                "expected_answer": expected_answer,
                "atlas_result": atlas_result,
                "openai_result": openai_result,
                "similarity": similarity,
                "processing_time": {
                    "atlas": atlas_result.get("processing_time", 0),
                    "openai": openai_result.get("processing_time", 0)
                }
            }
            
            comparison_results.append(comparison_result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if atlas_result["success"] and openai_result["success"]:
                successful_comparisons += 1
                total_atlas_time += atlas_result.get("processing_time", 0)
                total_openai_time += openai_result.get("processing_time", 0)
                total_similarity += similarity
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "questions_file": questions_file,
                "document_id": document_id,
                "total_questions": len(questions),
                "max_questions": max_questions
            },
            "comparison_results": comparison_results,
            "analysis": {
                "summary": {
                    "total_questions": len(questions),
                    "successful_comparisons": successful_comparisons,
                    "success_rate": (successful_comparisons / len(questions)) * 100 if questions else 0,
                    "average_similarity": total_similarity / successful_comparisons if successful_comparisons > 0 else 0,
                    "average_atlas_time": total_atlas_time / successful_comparisons if successful_comparisons > 0 else 0,
                    "average_openai_time": total_openai_time / successful_comparisons if successful_comparisons > 0 else 0,
                    "time_difference": (total_atlas_time - total_openai_time) / successful_comparisons if successful_comparisons > 0 else 0
                }
            }
        }
        
        logger.info(f"ë°°ì¹˜ ë¹„êµ ì™„ë£Œ - ì„±ê³µë¥ : {final_result['analysis']['summary']['success_rate']:.1f}%")
        return final_result
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ë¹„êµ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ë¹„êµ ì‹¤íŒ¨: {str(e)}")

async def get_openai_answer_with_content(question: str, document_content: str) -> Dict[str, Any]:
    """
    ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ë°›ì•„ì„œ OpenAI APIë¥¼ í†µí•´ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        import openai
        
        start_time = time.time()
        
        # OpenAI API í˜¸ì¶œ
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        prompt = f"""
ë‹¤ìŒ ê³„ì•½ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ê³„ì•½ì„œ ë‚´ìš©:
{document_content}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ê³„ì•½ì„œì˜ êµ¬ì²´ì ì¸ ì¡°í•­ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€
2. ë²•ì  ê´€ì ì—ì„œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë¶„ì„ ì œê³µ
3. ë…ì†Œì¡°í•­ì´ë‚˜ ìœ„í—˜ ìš”ì†Œê°€ ìˆë‹¤ë©´ ëª…í™•íˆ ì§€ì 
4. ë‹µë³€ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­ ë²ˆí˜¸ë‚˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
"""
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê³„ì•½ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.1
        )
        
        processing_time = time.time() - start_time
        answer = response.choices[0].message.content.strip()
        
        return {
            "success": True,
            "answer": answer,
            "processing_time": processing_time,
            "model": "gpt-4o",
            "tokens_used": response.usage.total_tokens if response.usage else 0
        }
        
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "answer": "",
            "processing_time": 0
        }

def execute_pipeline_with_risk_analysis(start_step: int, keyword: Optional[str], pipeline_id: str = None, file_id: str = None):
    """ìœ„í—˜ ë¶„ì„ì´ í¬í•¨ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    global pipeline_status
    
    print(f"ğŸš€ ìœ„í—˜ ë¶„ì„ í¬í•¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ - keyword: {keyword}, start_step: {start_step}, file_id: {file_id}")
    logger.info(f"ğŸš€ ìœ„í—˜ ë¶„ì„ í¬í•¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ - keyword: {keyword}, start_step: {start_step}, file_id: {file_id}")
    
    if pipeline_id:
        pipeline_status[pipeline_id] = {
            "status": "running",
            "progress": 0,
            "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...",
            "start_time": datetime.now().isoformat()
        }
        print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ - ID: {pipeline_id}")
        logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ - ID: {pipeline_id}")
    
    try:
        import subprocess
        import sys
        
        # BE ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
        be_dir = Path(__file__).parent
        cmd = [sys.executable, "main_pipeline.py", str(start_step), keyword]
        
        print(f"ğŸ“‹ subprocess ëª…ë ¹ì–´: {' '.join(cmd)}")
        print(f"ğŸ“‚ ì‹¤í–‰ ë””ë ‰í† ë¦¬: {be_dir}")
        logger.info(f"ğŸ“‹ subprocess ëª…ë ¹ì–´: {' '.join(cmd)}")
        logger.info(f"ğŸ“‚ ì‹¤í–‰ ë””ë ‰í† ë¦¬: {be_dir}")
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['LANG'] = 'ko_KR.UTF-8'
        env['LC_ALL'] = 'ko_KR.UTF-8'
        env['KEYWORD'] = keyword  # keyword í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        
        if pipeline_id:
            pipeline_status[pipeline_id]["progress"] = 25
            pipeline_status[pipeline_id]["message"] = "íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì¤‘..."
            print("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: 25%")
            logger.info("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: 25%")
        
        # subprocessë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = subprocess.run(
            cmd,
            cwd=be_dir,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            env=env,
            timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        )
        
        print(f"ğŸ“‹ subprocess ê²°ê³¼ ì½”ë“œ: {result.returncode}")
        print(f"ğŸ“ stdout: {result.stdout}")
        print(f"ğŸ“ stderr: {result.stderr}")
        logger.info(f"ğŸ“‹ subprocess ê²°ê³¼ ì½”ë“œ: {result.returncode}")
        logger.info(f"ğŸ“ stdout: {result.stdout}")
        logger.info(f"ğŸ“ stderr: {result.stderr}")
        
        success = result.returncode == 0
        
        if success:
            print("âœ… subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
            logger.info("âœ… subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
            
            # íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ìë™ ë¡œë“œ
            print("ğŸ”„ RAG ì‹œìŠ¤í…œ ìë™ ë¡œë“œ ì¤‘...")
            logger.info("ğŸ”„ RAG ì‹œìŠ¤í…œ ìë™ ë¡œë“œ ì¤‘...")
            if check_and_load_existing_data():
                print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ")
                logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ")
                
                # RAG ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ í›„ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰
                if file_id and file_id in uploaded_files:
                    print("ğŸ›¡ï¸ ìœ„í—˜ ë¶„ì„ ì‹œì‘...")
                    logger.info("ğŸ›¡ï¸ ìœ„í—˜ ë¶„ì„ ì‹œì‘...")
                    
                    try:
                        # ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ (ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
                        execute_risk_analysis_sync(file_id, pipeline_id)
                    except Exception as e:
                        print(f"âš ï¸ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                        logger.error(f"âš ï¸ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            else:
                print("âš ï¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨")
                logger.warning("âš ï¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨")
            
            if pipeline_id:
                pipeline_status[pipeline_id] = {
                    "status": "completed",
                    "progress": 100,
                    "message": "íŒŒì´í”„ë¼ì¸ ë° ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ ì™„ë£Œ",
                    "end_time": datetime.now().isoformat()
                }
                print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ - ID: {pipeline_id}")
                logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ - ID: {pipeline_id}")
        else:
            print("âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            logger.error("âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            if pipeline_id:
                pipeline_status[pipeline_id] = {
                    "status": "failed",
                    "progress": 0,
                    "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨",
                    "end_time": datetime.now().isoformat()
                }
                print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹¤íŒ¨ - ID: {pipeline_id}")
                logger.error(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹¤íŒ¨ - ID: {pipeline_id}")
        
        return success
        
    except subprocess.TimeoutExpired:
        print("â° subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
        logger.error("â° subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
        
        if pipeline_id:
            pipeline_status[pipeline_id] = {
                "status": "failed",
                "progress": 0,
                "message": "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ",
                "end_time": datetime.now().isoformat()
            }
            print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: íƒ€ì„ì•„ì›ƒ - ID: {pipeline_id}")
            logger.error(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: íƒ€ì„ì•„ì›ƒ - ID: {pipeline_id}")
        
        return False
        
    except Exception as e:
        print(f"âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        logger.error(f"âŒ subprocess íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        logger.error(traceback.format_exc())
        
        if pipeline_id:
            pipeline_status[pipeline_id] = {
                "status": "failed",
                "progress": 0,
                "message": f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "end_time": datetime.now().isoformat()
            }
            print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì˜ˆì™¸ ì‹¤íŒ¨ - ID: {pipeline_id}")
            logger.error(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸: ì˜ˆì™¸ ì‹¤íŒ¨ - ID: {pipeline_id}")
        
        return False

def execute_risk_analysis_sync(file_id: str, pipeline_id: str):
    """ìœ„í—˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë™ê¸°)"""
    try:
        print(f"ğŸ›¡ï¸ ìœ„í—˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - file_id: {file_id}, pipeline_id: {pipeline_id}")
        logger.info(f"ğŸ›¡ï¸ ìœ„í—˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - file_id: {file_id}, pipeline_id: {pipeline_id}")
        
        # íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        if file_id not in uploaded_files:
            raise Exception(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
        
        file_info = uploaded_files[file_id]
        file_path = file_info["file_path"]
        
        # ê³„ì•½ì„œ ë‚´ìš© ì½ê¸°
        contract_text = ""
        with open(file_path, 'r', encoding='utf-8') as f:
            if file_path.endswith('.json'):
                json_data = json.load(f)
                if isinstance(json_data, dict) and 'content' in json_data:
                    contract_text = json_data['content']
                else:
                    contract_text = json.dumps(json_data, ensure_ascii=False, indent=2)
            else:
                contract_text = f.read()
        
        # ìœ„í—˜ ë¶„ì„ ì‹œì‘
        from riskAnalysis.hybrid_risk_analyzer import HybridSequentialRiskAnalyzer
        
        # RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not rag_system:
            raise Exception("RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìœ„í—˜ ë¶„ì„ê¸° ì´ˆê¸°í™”
        risk_check_data = load_risk_checklist()
        analyzer = HybridSequentialRiskAnalyzer(
            risk_check_data,
            rag_system["enhanced_lkg_retriever"],
            rag_system["hippo_retriever"],
            rag_system["llm_generator"],
            neo4j_driver
        )
        
        # ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ (ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
        import asyncio
        analysis_result = asyncio.run(analyzer.analyze_all_parts_with_hybrid(
            contract_text, 
            file_info["filename"]
        ))
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_id = f"risk_analysis_{pipeline_id}"
        risk_analysis_results[analysis_id] = {
            "analysis_id": analysis_id,
            "pipeline_id": pipeline_id,
            "file_id": file_id,
            "contract_name": file_info["filename"],
            "analysis_result": analysis_result,
            "created_at": datetime.now().isoformat()
        }
        
        print(f"âœ… ìœ„í—˜ ë¶„ì„ ì™„ë£Œ - analysis_id: {analysis_id}")
        logger.info(f"âœ… ìœ„í—˜ ë¶„ì„ ì™„ë£Œ - analysis_id: {analysis_id}")
        
    except Exception as e:
        print(f"âŒ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(f"âŒ ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise e

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
risk_analysis_results = {}

@app.get("/risk-analysis/{pipeline_id}")
async def get_risk_analysis_result(pipeline_id: str):
    """íŒŒì´í”„ë¼ì¸ IDë¡œ ìœ„í—˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        analysis_id = f"risk_analysis_{pipeline_id}"
        
        if analysis_id not in risk_analysis_results:
            raise HTTPException(status_code=404, detail="ìœ„í—˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        result = risk_analysis_results[analysis_id]
        
        return {
            "success": True,
            "data": result
        }
        
    except Exception as e:
        logger.error(f"ìœ„í—˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/risk-analysis")
async def get_all_risk_analysis_results():
    """ëª¨ë“  ìœ„í—˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        return {
            "success": True,
            "data": list(risk_analysis_results.values())
        }
        
    except Exception as e:
        logger.error(f"ìœ„í—˜ ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/risk-analysis/analyze-contract")
async def analyze_contract_risk(
    contract_text: str = Form(...),
    contract_name: str = Form("ê³„ì•½ì„œ"),
    selected_parts: str = Form("all")  # "all" ë˜ëŠ” "1,2,3" í˜•íƒœ
):
    """ë…ë¦½ì ì¸ ê³„ì•½ì„œ ìœ„í—˜ ë¶„ì„"""
    try:
        print(f"ğŸ›¡ï¸ ë…ë¦½ì ì¸ ìœ„í—˜ ë¶„ì„ ì‹œì‘ - contract_name: {contract_name}")
        logger.info(f"ğŸ›¡ï¸ ë…ë¦½ì ì¸ ìœ„í—˜ ë¶„ì„ ì‹œì‘ - contract_name: {contract_name}")
        
        # RAG ì‹œìŠ¤í…œ í™•ì¸
        if not rag_system:
            raise HTTPException(status_code=500, detail="RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë¶„ì„í•  íŒŒíŠ¸ ê²°ì •
        if selected_parts == "all":
            parts_to_analyze = list(range(1, 11))  # 1-10 íŒŒíŠ¸
        else:
            parts_to_analyze = [int(p.strip()) for p in selected_parts.split(",")]
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìœ„í—˜ ë¶„ì„ê¸° ì´ˆê¸°í™”
        from riskAnalysis.hybrid_risk_analyzer import HybridSequentialRiskAnalyzer
        risk_check_data = load_risk_checklist()
        
        analyzer = HybridSequentialRiskAnalyzer(
            risk_check_data,
            rag_system["enhanced_lkg_retriever"],
            rag_system["hippo_retriever"],
            rag_system["llm_generator"],
            neo4j_driver
        )
        
        # ìœ„í—˜ ë¶„ì„ ì‹¤í–‰
        import asyncio
        analysis_result = asyncio.run(analyzer.analyze_all_parts_with_hybrid(
            contract_text, 
            contract_name
        ))
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_id = f"standalone_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        risk_analysis_results[analysis_id] = {
            "analysis_id": analysis_id,
            "pipeline_id": None,
            "file_id": None,
            "contract_name": contract_name,
            "analysis_result": analysis_result,
            "created_at": datetime.now().isoformat(),
            "analysis_type": "standalone"
        }
        
        print(f"âœ… ë…ë¦½ì ì¸ ìœ„í—˜ ë¶„ì„ ì™„ë£Œ - analysis_id: {analysis_id}")
        logger.info(f"âœ… ë…ë¦½ì ì¸ ìœ„í—˜ ë¶„ì„ ì™„ë£Œ - analysis_id: {analysis_id}")
        
        return {
            "success": True,
            "message": "ìœ„í—˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "analysis_id": analysis_id,
                "analysis_result": analysis_result
            }
        }
        
    except Exception as e:
        logger.error(f"ë…ë¦½ì ì¸ ìœ„í—˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/risk-analysis/analyze-uploaded-file")
async def analyze_uploaded_file_risk(
    file_id: str = Form(...),
    selected_parts: str = Form("all")
):
    """ì—…ë¡œë“œëœ íŒŒì¼ì— ëŒ€í•œ ë…ë¦½ì ì¸ ìœ„í—˜ ë¶„ì„"""
    try:
        print(f"ğŸ›¡ï¸ ì—…ë¡œë“œëœ íŒŒì¼ ìœ„í—˜ ë¶„ì„ ì‹œì‘ - file_id: {file_id}")
        logger.info(f"ğŸ›¡ï¸ ì—…ë¡œë“œëœ íŒŒì¼ ìœ„í—˜ ë¶„ì„ ì‹œì‘ - file_id: {file_id}")
        
        # íŒŒì¼ ì •ë³´ í™•ì¸
        if file_id not in uploaded_files:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        file_info = uploaded_files[file_id]
        file_path = file_info["file_path"]
        
        # ê³„ì•½ì„œ ë‚´ìš© ì½ê¸°
        contract_text = ""
        with open(file_path, 'r', encoding='utf-8') as f:
            if file_path.endswith('.json'):
                json_data = json.load(f)
                if isinstance(json_data, dict) and 'content' in json_data:
                    contract_text = json_data['content']
                else:
                    contract_text = json.dumps(json_data, ensure_ascii=False, indent=2)
            else:
                contract_text = f.read()
        
        # RAG ì‹œìŠ¤í…œ í™•ì¸
        if not rag_system:
            raise HTTPException(status_code=500, detail="RAG ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë¶„ì„í•  íŒŒíŠ¸ ê²°ì •
        if selected_parts == "all":
            parts_to_analyze = list(range(1, 11))  # 1-10 íŒŒíŠ¸
        else:
            parts_to_analyze = [int(p.strip()) for p in selected_parts.split(",")]
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìœ„í—˜ ë¶„ì„ê¸° ì´ˆê¸°í™”
        from riskAnalysis.hybrid_risk_analyzer import HybridSequentialRiskAnalyzer
        risk_check_data = load_risk_checklist()
        
        analyzer = HybridSequentialRiskAnalyzer(
            risk_check_data,
            rag_system["enhanced_lkg_retriever"],
            rag_system["hippo_retriever"],
            rag_system["llm_generator"],
            neo4j_driver
        )
        
        # ìœ„í—˜ ë¶„ì„ ì‹¤í–‰
        import asyncio
        analysis_result = asyncio.run(analyzer.analyze_all_parts_with_hybrid(
            contract_text, 
            file_info["filename"]
        ))
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_id = f"file_{file_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        risk_analysis_results[analysis_id] = {
            "analysis_id": analysis_id,
            "pipeline_id": None,
            "file_id": file_id,
            "contract_name": file_info["filename"],
            "analysis_result": analysis_result,
            "created_at": datetime.now().isoformat(),
            "analysis_type": "file_analysis"
        }
        
        print(f"âœ… ì—…ë¡œë“œëœ íŒŒì¼ ìœ„í—˜ ë¶„ì„ ì™„ë£Œ - analysis_id: {analysis_id}")
        logger.info(f"âœ… ì—…ë¡œë“œëœ íŒŒì¼ ìœ„í—˜ ë¶„ì„ ì™„ë£Œ - analysis_id: {analysis_id}")
        
        return {
            "success": True,
            "message": "ìœ„í—˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "analysis_id": analysis_id,
                "analysis_result": analysis_result
            }
        }
        
    except Exception as e:
        logger.error(f"ì—…ë¡œë“œëœ íŒŒì¼ ìœ„í—˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
